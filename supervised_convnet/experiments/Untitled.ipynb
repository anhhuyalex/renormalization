{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import supervised_convnet\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "uncorrelated_data = np.load(\"../ising81x81_temp1_uncorrelated9x9.npy\")[:10]\n",
    "correlated_data = np.load(\"../ising81x81_temp1.npy\")[:4000,:9,:9]\n",
    "data = np.vstack((uncorrelated_data, correlated_data))\n",
    "label = np.hstack((np.zeros(10), np.ones(4000)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25, random_state=42)\n",
    "# print(model(torch.Tensor(data[0]).unsqueeze(0).unsqueeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.X = data\n",
    "        self.y = label\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isingdataset = IsingDataset(X_train[:8000], y_train[:8000])\n",
    "isingdataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isingdataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = supervised_convnet.SupervisedConvNet(filter_size = 3, square_size = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 250.249963\n",
      "output tensor([0.9663, 0.9781, 0.7723, 0.7723, 0.7723, 0.7723, 0.9781],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 11 \tTraining Loss: 333.108764\n",
      "output tensor([0.9255, 0.9712, 0.7739, 0.9726, 0.9726, 0.7739, 0.7739],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([0., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 21 \tTraining Loss: 233.161747\n",
      "output tensor([0.7723, 0.9770, 0.9770, 0.9770, 0.9770, 0.7723, 0.9770],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 31 \tTraining Loss: 256.608702\n",
      "output tensor([0.8058, 0.7746, 0.7746, 0.9773, 0.7746, 0.9773, 0.7746],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 41 \tTraining Loss: 241.509635\n",
      "output tensor([0.7746, 0.9776, 0.9776, 0.9776, 0.7746, 0.9776, 0.7746],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 51 \tTraining Loss: 249.808807\n",
      "output tensor([0.7731, 0.9783, 0.7731, 0.9783, 0.7731, 0.9783, 0.7731],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 61 \tTraining Loss: 257.920835\n",
      "output tensor([0.7759, 0.7759, 0.7759, 0.7759, 0.9772, 0.9772, 0.7759],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 71 \tTraining Loss: 224.836130\n",
      "output tensor([0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781, 0.7737],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 81 \tTraining Loss: 241.463408\n",
      "output tensor([0.9782, 0.7735, 0.7735, 0.7735, 0.9782, 0.9782, 0.9782],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 91 \tTraining Loss: 258.144312\n",
      "output tensor([0.7734, 0.9777, 0.7734, 0.7734, 0.7734, 0.7734, 0.9764],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 101 \tTraining Loss: 241.471898\n",
      "output tensor([0.7717, 0.9774, 0.7717, 0.7717, 0.9774, 0.9774, 0.9774],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 111 \tTraining Loss: 258.369185\n",
      "output tensor([0.9785, 0.7706, 0.9785, 0.7706, 0.7706, 0.7706, 0.7706],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 121 \tTraining Loss: 241.521109\n",
      "output tensor([0.9777, 0.9765, 0.7736, 0.7736, 0.7736, 0.9777, 0.9777],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 131 \tTraining Loss: 258.058377\n",
      "output tensor([0.7739, 0.7739, 0.9777, 0.7739, 0.7739, 0.7739, 0.9777],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 141 \tTraining Loss: 258.225448\n",
      "output tensor([0.7729, 0.9722, 0.7729, 0.7729, 0.7729, 0.7729, 0.9737],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 151 \tTraining Loss: 266.171824\n",
      "output tensor([0.7754, 0.9772, 0.7754, 0.7754, 0.7754, 0.7754, 0.7754],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 161 \tTraining Loss: 224.812258\n",
      "output tensor([0.9781, 0.7734, 0.9781, 0.9781, 0.9781, 0.9781, 0.9781],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 171 \tTraining Loss: 258.161508\n",
      "output tensor([0.7727, 0.7727, 0.7727, 0.7727, 0.9779, 0.9779, 0.7727],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 181 \tTraining Loss: 249.799017\n",
      "output tensor([0.7730, 0.9779, 0.7730, 0.9779, 0.9779, 0.7730, 0.7730],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 191 \tTraining Loss: 249.839116\n",
      "output tensor([0.7720, 0.9783, 0.7720, 0.7720, 0.9783, 0.9783, 0.7720],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 201 \tTraining Loss: 249.796316\n",
      "output tensor([0.7733, 0.9782, 0.7733, 0.9782, 0.9782, 0.7733, 0.7733],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 211 \tTraining Loss: 258.325242\n",
      "output tensor([0.7711, 0.9785, 0.7711, 0.7711, 0.7711, 0.9785, 0.7711],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 221 \tTraining Loss: 249.862820\n",
      "output tensor([0.7714, 0.9785, 0.9785, 0.7714, 0.7714, 0.7714, 0.9785],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 231 \tTraining Loss: 241.463792\n",
      "output tensor([0.7672, 0.7672, 0.9796, 0.9796, 0.7672, 0.9796, 0.9796],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 241 \tTraining Loss: 249.940477\n",
      "output tensor([0.7694, 0.9782, 0.9782, 0.7694, 0.7694, 0.7694, 0.9782],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 251 \tTraining Loss: 241.469525\n",
      "output tensor([0.7695, 0.7695, 0.9784, 0.9784, 0.9771, 0.7695, 0.9784],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 261 \tTraining Loss: 249.901108\n",
      "output tensor([0.9782, 0.7702, 0.7702, 0.9782, 0.9782, 0.7702, 0.7702],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 271 \tTraining Loss: 241.426129\n",
      "output tensor([0.7710, 0.9783, 0.9783, 0.7710, 0.9783, 0.9783, 0.7710],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 281 \tTraining Loss: 241.426401\n",
      "output tensor([0.9784, 0.7711, 0.9784, 0.9784, 0.7711, 0.7711, 0.9784],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Epoch: 291 \tTraining Loss: 274.887674\n",
      "output tensor([0.7725, 0.7725, 0.7725, 0.7725, 0.7725, 0.7725, 0.7725],\n",
      "       grad_fn=<SliceBackward>)\n",
      "target tensor([1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1000\n",
    "# number of epochs to train the model\n",
    "n_epochs = 300\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(isingdataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.unsqueeze(1).type('torch.FloatTensor')\n",
    "        target = target.type('torch.FloatTensor')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[4].view(-1)\n",
    "#         print(data[:10])\n",
    "#         raise ValueError\n",
    "        loss = criterion(output, target) \n",
    "        for param in model.parameters():\n",
    "            loss += (((param)**2).sum())/20\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * batch_size\n",
    "    \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    if epoch % 10 == 1:\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))\n",
    "#         print(\"data\", data[:10])\n",
    "        print(\"output\", (output)[:10])\n",
    "        print(\"target\", (target)[:10])\n",
    "#         for name, param in model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 print (name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[-1], output[-1], target[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data[-1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d.weight tensor([[[[0.1478, 0.1485, 0.1484],\n",
      "          [0.1486, 0.1494, 0.1489],\n",
      "          [0.1480, 0.1488, 0.1482]]]])\n",
      "sum conv2d.weight tensor(1.3366)\n",
      "conv2d.bias tensor([0.1503])\n",
      "sum conv2d.bias tensor(0.1503)\n",
      "linear.weight tensor([[0.1554, 0.1531, 0.1603, 0.1560, 0.1567, 0.1573, 0.1554, 0.1589, 0.1573]])\n",
      "sum linear.weight tensor(1.4105)\n",
      "linear.bias tensor([0.8835])\n",
      "sum linear.bias tensor(0.8835)\n"
     ]
    }
   ],
   "source": [
    "#initial\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)\n",
    "        print (\"sum\", name, (param.data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)\n",
    "        print (\"sum\", name, (param.data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.unsqueeze(1).type('torch.FloatTensor')[0].unsqueeze(1)\n",
    "        print(\"data\", data)\n",
    "        target = target.type('torch.FloatTensor')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[-1].view(-1)\n",
    "        print(\"output\", output)\n",
    "        print(\"target\", target[0])\n",
    "        loss = criterion(output, target[0])\n",
    "        print(\"loss.data\", loss.data)\n",
    "        loss.backward()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print (name, param.grad)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    data = np.load(\"ising81x81_temp1.npy\")[:, :3, :3]\n",
    "\n",
    "    v = conv_autoencoder.ConvAutoencoder(3, 1)\n",
    "\n",
    "    # specify loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # specify loss function\n",
    "    optimizer = torch.optim.Adam(v.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Create training and test dataloaders\n",
    "    num_workers = 0\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 20\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(data[:8000], batch_size=batch_size, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(data[:2000], batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 500\n",
    "    l1_crit = nn.L1Loss(size_average=False)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for d in train_loader:\n",
    "            # no need to flatten images\n",
    "            d = (d.unsqueeze(1)).type(torch.FloatTensor)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = v(d).view(-1, 1, 3, 3)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, d)\n",
    "    #         for param in v.parameters():\n",
    "    #             loss += (torch.abs(param)).mean()/200\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * batch_size\n",
    "                \n",
    "        # print avg training statistics \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896872923564325"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(np.sum(-np.array([ 1.7783, -1.4541, -4.4681, -3.8083,  3.1658, -0.5841,  6.2371,  1.4833,\n",
    "         -2.3994]) ) + 1.4065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Renormalization",
   "language": "python",
   "name": "renormalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
