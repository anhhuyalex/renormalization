{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import supervised_convnet\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "uncorrelated_data = np.load(\"../ising81x81_temp1_uncorrelated9x9.npy\")\n",
    "correlated_data = np.load(\"../ising81x81_temp1.npy\")[:,:9,:9]\n",
    "data = np.vstack((uncorrelated_data, correlated_data))\n",
    "label = np.hstack((np.zeros(10000), np.ones(10000)))\n",
    "model = supervised_convnet.SupervisedConvNet(filter_size = 3, square_size = 9)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)\n",
    "# print(model(torch.Tensor(data[0]).unsqueeze(0).unsqueeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n",
    "isingdataset = IsingDataset(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isingdataset.X[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161,\n",
      "          0.1161]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1161, 0.1310, 0.1310, 0.1161, 0.1310, 0.1161, 0.1310, 0.1161,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1161, 0.1161, 0.1310, 0.1161, 0.1310, 0.1161, 0.1161, 0.1161,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1161, 0.1161, 0.1310, 0.1161, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161,\n",
      "          0.1161]],\n",
      "\n",
      "        [[0.1310, 0.1161, 0.1310, 0.1161, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1161, 0.1161, 0.1161, 0.1161, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1161, 0.1310, 0.1310, 0.1161, 0.1161, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1161, 0.1161, 0.1310, 0.1161, 0.1310,\n",
      "          0.1161]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161,\n",
      "          0.1161]],\n",
      "\n",
      "        [[0.1161, 0.1161, 0.1310, 0.1161, 0.1161, 0.1310, 0.1161, 0.1161,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1310, 0.1161, 0.1161, 0.0000, 0.1161, 0.1161, 0.1161, 0.1310,\n",
      "          0.1161]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]],\n",
      "\n",
      "        [[0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1161,\n",
      "          0.1161]],\n",
      "\n",
      "        [[0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
      "          0.1310]]], grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-8b06f397f8ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# number of epochs to train the model\n",
    "n_epochs = 500\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(isingdataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.unsqueeze(1).type('torch.FloatTensor')\n",
    "        target = target.type('torch.FloatTensor')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        for param in model.parameters():\n",
    "            loss += (torch.abs(param)).mean()/200\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d.weight tensor([[[[-0.2142,  0.2122,  0.3146],\n",
      "          [-0.3160,  0.1560,  0.1863],\n",
      "          [-0.1728, -0.0777, -0.0269]]]])\n",
      "conv2d.bias tensor([-0.2431])\n",
      "linear.weight tensor([[ 0.2419,  0.3124,  0.1725,  0.1958, -0.1595, -0.2545,  0.0331, -0.0301,\n",
      "         -0.1911]])\n",
      "linear.bias tensor([0.0855])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0219, 0.0219, 0.0219, 0.0000, 0.0219, 0.1945, 0.0000, 0.0000,\n",
       "          0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 1:\n",
    "    data = np.load(\"ising81x81_temp1.npy\")[:, :3, :3]\n",
    "\n",
    "    v = conv_autoencoder.ConvAutoencoder(3, 1)\n",
    "\n",
    "    # specify loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # specify loss function\n",
    "    optimizer = torch.optim.Adam(v.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Create training and test dataloaders\n",
    "    num_workers = 0\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 20\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(data[:8000], batch_size=batch_size, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(data[:2000], batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 500\n",
    "    l1_crit = nn.L1Loss(size_average=False)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for d in train_loader:\n",
    "            # no need to flatten images\n",
    "            d = (d.unsqueeze(1)).type(torch.FloatTensor)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = v(d).view(-1, 1, 3, 3)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, d)\n",
    "    #         for param in v.parameters():\n",
    "    #             loss += (torch.abs(param)).mean()/200\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * batch_size\n",
    "                \n",
    "        # print avg training statistics \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Renormalization",
   "language": "python",
   "name": "renormalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
