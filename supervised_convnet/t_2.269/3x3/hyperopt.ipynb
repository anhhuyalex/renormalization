{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hyperopt.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"g_engCTw8fpd","colab_type":"code","outputId":"7d9ce0a6-0889-40d2-df86-f38903e6aff5","executionInfo":{"status":"ok","timestamp":1567053740857,"user_tz":-420,"elapsed":10079,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":548}},"source":["# \n","\n","# !pip install -U --target=/usr/local/lib/python3.6/dist-packages git+https://github.com/pytorch/botorch.git\n","\n","# !pip install -U -q torch==1.2.0 torchvision\n","\n","# !pip install -U scikit-learn\n","# !pip install -U --target=/usr/local/lib/python3.6/dist-packages git+https://github.com/anhhuyalex/Ax.git\n","  \n","!pip install -U --target=/usr/local/lib/python3.6/dist-packages git+https://github.com/anhhuyalex/Ax.git\n","  \n","%cd /content\n","# %cd /usr/local/lib/python3.6/dist-packages\n","!rm -rf botorch\n","%cd /content\n","!git clone https://github.com/pytorch/botorch.git\n","%cd botorch\n","!pip install .\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n","/content\n","Cloning into 'botorch'...\n","remote: Enumerating objects: 230, done.\u001b[K\n","remote: Counting objects: 100% (230/230), done.\u001b[K\n","remote: Compressing objects: 100% (158/158), done.\u001b[K\n","remote: Total 5926 (delta 87), reused 112 (delta 34), pack-reused 5696\u001b[K\n","Receiving objects: 100% (5926/5926), 10.52 MiB | 24.92 MiB/s, done.\n","Resolving deltas: 100% (3785/3785), done.\n","/content/botorch\n","Processing /content/botorch\n","Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from botorch==0.1.3) (1.2.0)\n","Requirement already satisfied: gpytorch>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from botorch==0.1.3) (0.3.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from botorch==0.1.3) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->botorch==0.1.3) (1.17.1)\n","Building wheels for collected packages: botorch\n","  Building wheel for botorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for botorch: filename=botorch-0.1.3-cp36-none-any.whl size=175126 sha256=5907262eefb04ac8c2534ba5e4c2ec0bf6c7be3e82f2d23febe20b6160f991af\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cr81hh31/wheels/d8/0e/3f/502176509633fec729eabc1a42e465b3603faf0c30b4782f33\n","Successfully built botorch\n","Installing collected packages: botorch\n","  Found existing installation: botorch 0.1.3\n","    Uninstalling botorch-0.1.3:\n","      Successfully uninstalled botorch-0.1.3\n","Successfully installed botorch-0.1.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["botorch"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"kMpk4RwwlC4x","colab_type":"code","outputId":"da8fdbcc-4f39-4ed0-ae59-f321f4bc8d90","executionInfo":{"status":"ok","timestamp":1567052391396,"user_tz":-420,"elapsed":101538,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/pytorch/botorch.git\n","  Cloning https://github.com/pytorch/botorch.git to /tmp/pip-req-build-9kwtgry5\n","  Running command git clone -q https://github.com/pytorch/botorch.git /tmp/pip-req-build-9kwtgry5\n","Collecting torch>=1.2 (from botorch==0.1.3)\n","  Using cached https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n","Collecting gpytorch>=0.3.5 (from botorch==0.1.3)\n","Collecting scipy (from botorch==0.1.3)\n","  Using cached https://files.pythonhosted.org/packages/29/50/a552a5aff252ae915f522e44642bb49a7b7b31677f9580cfd11bcc869976/scipy-1.3.1-cp36-cp36m-manylinux1_x86_64.whl\n","Collecting numpy (from torch>=1.2->botorch==0.1.3)\n","  Using cached https://files.pythonhosted.org/packages/75/92/57179ed45307ec6179e344231c47da7f3f3da9e2eee5c8ab506bd279ce4e/numpy-1.17.1-cp36-cp36m-manylinux1_x86_64.whl\n","Building wheels for collected packages: botorch\n","  Building wheel for botorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for botorch: filename=botorch-0.1.3-cp36-none-any.whl size=175126 sha256=154fde2c7ac78b7be391d0b23e120d695155142afdc5c7a9e7b725be7ad598f5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-my6avjjg/wheels/82/70/31/16688bea51fa7ff1a0100a484854a6c2a9663fde3055aa478a\n","Successfully built botorch\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 0.25.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, torch, gpytorch, scipy, botorch\n","Successfully installed botorch-0.1.3 gpytorch-0.3.5 numpy-1.17.1 scipy-1.3.1 torch-1.2.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"uJuszQGG_m1D","colab_type":"code","outputId":"ef8173e9-58b9-4284-b968-e2e8b2642228","executionInfo":{"status":"ok","timestamp":1567067108296,"user_tz":-420,"elapsed":23461,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lIKufxcqJZzf","colab_type":"code","outputId":"21d1d35f-5f0a-44f0-e84b-f777050c3d50","executionInfo":{"status":"ok","timestamp":1567067121030,"user_tz":-420,"elapsed":3758,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["%cd /content/drive/My\\ Drive/Year\\ 4\\:\\ Synthesis/NS162\\:\\ Statistical\\ Mechanics/Renormalization\\ Research\\ Project/renormalization/supervised_convnet/t_2.269/3x3\n","!rm -rf botorch"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Year 4: Synthesis/NS162: Statistical Mechanics/Renormalization Research Project/renormalization/supervised_convnet/t_2.269/3x3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NU7OJp_nAbkW","colab_type":"code","outputId":"2cd279bb-806e-431f-c362-470b1ced54e5","executionInfo":{"status":"ok","timestamp":1567053838012,"user_tz":-420,"elapsed":7151,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":751}},"source":["import torch\n","import numpy as np\n","\n","from ax import RangeParameter, ParameterType\n","from ax.plot.contour import plot_contour\n","from ax.plot.trace import optimization_trace_single_method\n","from ax.service.managed_loop import optimize\n","from ax.utils.notebook.plotting import render, init_notebook_plotting\n","\n","import sys\n","import time\n","sys.path.insert(0, \"../../\")\n","import supervised_convnet\n","import train, frozen"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning:\n","\n","Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning:\n","\n","Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning:\n","\n","Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning:\n","\n","Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning:\n","\n","Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning:\n","\n","Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Uncorrelated [[ 1  1  1 -1 -1 -1  1 -1 -1]\n"," [ 1  1  1 -1 -1 -1 -1 -1 -1]\n"," [ 1  1  1 -1  1  1  1  1  1]\n"," [ 1  1 -1  1 -1 -1 -1 -1 -1]\n"," [ 1  1 -1  1 -1 -1 -1 -1 -1]\n"," [ 1  1  1  1 -1 -1 -1 -1  1]\n"," [ 1  1 -1 -1 -1 -1  1  1  1]\n"," [ 1 -1 -1 -1 -1 -1  1  1  1]\n"," [ 1  1  1 -1 -1 -1  1  1 -1]]\n","Correlated [[ 1  1 -1  1 -1 -1 -1 -1  1]\n"," [ 1  1 -1 -1 -1 -1 -1 -1 -1]\n"," [ 1  1 -1 -1 -1  1  1  1  1]\n"," [-1 -1 -1 -1 -1 -1  1  1  1]\n"," [ 1 -1 -1 -1 -1  1  1  1  1]\n"," [-1 -1 -1 -1 -1  1  1  1  1]\n"," [-1 -1 -1 -1 -1  1  1 -1 -1]\n"," [-1 -1 -1 -1 -1 -1  1  1  1]\n"," [-1 -1  1 -1 -1 -1 -1 -1  1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SbOqeR18C56q","colab_type":"code","outputId":"06df87ba-4a96-4db2-cdf6-6f709fef55bc","colab":{"base_uri":"https://localhost:8080/"}},"source":["num_hidden_layers = 1\n","out_channels = 1\n","weight = 0.02\n","\n","def train_evaluate(parameterization):\n","    # parameters\n","    batch_size = parameterization[\"batch_size\"]\n","    train_size = parameterization[\"train_size\"]\n","    n_epochs = parameterization[\"n_epochs\"]\n","    lr = parameterization[\"lr\"]\n","    weight_decay = parameterization[\"weight_decay\"]\n","    betas0 = parameterization[\"betas0\"]\n","    hidden_size = 10\n","\n","\n","    model = supervised_convnet.SupervisedConvNet(filter_size = 3, square_size = 3, \\\n","            hidden_size = hidden_size, out_channels = out_channels,\n","            first_activation = \"tanh\", activation_func = \"relu\",\n","            num_hidden_layers = num_hidden_layers)\n","    tic = time.time()\n","    accuracy = train.trainer(model = model, batch_size = batch_size, train_size = train_size, n_epochs = n_epochs, lr = lr,\n","                weight_decay = weight_decay,\n","                betas0 = 1-betas0, betas1 = 0.99)[0]\n","    toc = time.time()\n","    return accuracy\n","\n","best_parameters, best_values, experiment, model = optimize(\n","    parameters=[\n","        {\n","          \"name\": \"batch_size\",\n","          \"type\": \"range\",\n","          \"bounds\": [1, 5000],\n","          \"value_type\": \"int\"\n","        },\n","        {\n","          \"name\": \"train_size\",\n","          \"type\": \"range\",\n","          \"bounds\": [100, 5000],\n","          \"value_type\": \"int\"\n","        },\n","        {\n","          \"name\": \"n_epochs\",\n","          \"type\": \"range\",\n","          \"bounds\": [50, 250],\n","          \"value_type\": \"int\"\n","        },\n","        {\n","          \"name\": \"lr\",\n","          \"type\": \"range\",\n","          \"bounds\": [1e-4, 1],\n","          \"value_type\": \"float\",\n","          \"log_scale\": True\n","        },\n","        {\n","          \"name\": \"weight_decay\",\n","          \"type\": \"range\",\n","          \"bounds\": [1e-5, 2e-1],\n","          \"value_type\": \"float\",\n","          \"log_scale\": True\n","        },\n","        {\n","          \"name\": \"betas0\",\n","          \"type\": \"range\",\n","          \"bounds\": [1e-5, 2e-1],\n","          \"value_type\": \"float\",\n","          \"log_scale\": True\n","        },\n","        {\n","          \"name\": \"betas1\",\n","          \"type\": \"range\",\n","          \"bounds\": [1e-5, 2e-1],\n","          \"value_type\": \"float\",\n","          \"log_scale\": True\n","        }\n","    ],\n","    parameter_constraints=[\"0.02 * n_epochs + -1 * batch_size <= 0\"],\n","    evaluation_function=train_evaluate,\n","    minimize=False,\n","    arms_per_trial=5,\n","    total_trials=1,\n","    outcome_constraints=None,\n","    experiment_name=\"Test\"\n",")\n","\n","with open(\"hyperparameters.pl\", \"wb\") as handle:\n","    pickle.dump(optim_result, handle, protocol = pickle.HIGHEST_PROTOCOL)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[INFO 08-29 04:44:33] ax.service.utils.dispatch: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 10 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 10 will take longer to generate due to model-fitting.\n","[INFO 08-29 04:44:33] ax.service.managed_loop: Started full optimization with 20 steps.\n","[INFO 08-29 04:44:33] ax.service.managed_loop: Running optimization trial 1...\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([4021])) that is different to the input size (torch.Size([4021, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Testing out: \n","batch_size:  4242\n","train_size:  4021\n","n_epochs:  151\n","lr:  0.2059096003537947\n","weight_decay:  1.3539379101203248e-05\n","betas0:  0.9991187551142399\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2991, -0.1502, -0.0728],\n","          [ 0.2323,  0.1664,  0.2349],\n","          [ 0.1562,  0.0104,  0.2755]]]])\n","conv1.bias tensor([-0.2626])\n","first_linear.weight tensor([[-0.0826, -0.2689, -0.3244,  0.1389,  0.1754,  0.2683, -0.3155,  0.3206,\n","          0.1242],\n","        [ 0.0380,  0.2689,  0.3005, -0.0990,  0.0419, -0.1171, -0.1438,  0.1863,\n","          0.2644],\n","        [ 0.1571,  0.0092, -0.2080,  0.0356, -0.0879,  0.0993, -0.2621,  0.2306,\n","          0.0268],\n","        [-0.0141,  0.2197,  0.1910,  0.0198, -0.1197, -0.2907, -0.1822,  0.0351,\n","         -0.2345],\n","        [-0.1890,  0.0083,  0.3227, -0.2668, -0.1922,  0.2219,  0.2488,  0.2886,\n","          0.2395],\n","        [-0.0546, -0.1337,  0.3327, -0.0301, -0.1337,  0.2902,  0.3136,  0.1033,\n","         -0.0965],\n","        [ 0.3192,  0.3298, -0.2596, -0.0137,  0.3177,  0.2503,  0.1955,  0.0541,\n","         -0.0110],\n","        [ 0.2648,  0.2177, -0.0433, -0.3072, -0.2279,  0.2182,  0.2185,  0.2476,\n","         -0.2820],\n","        [ 0.1611,  0.1087,  0.2124, -0.0545,  0.2394,  0.0984, -0.2074,  0.0726,\n","         -0.0953],\n","        [-0.0723, -0.1081,  0.2100,  0.2267,  0.3068,  0.0490,  0.1296,  0.2645,\n","          0.0340]])\n","first_linear.bias tensor([-0.2004,  0.0025, -0.1167, -0.0274, -0.1233,  0.2241,  0.1789,  0.1551,\n","        -0.0031,  0.0073])\n","linear_hidden.0.weight tensor([[ 0.1923, -0.2985,  0.2023,  0.1746, -0.1854, -0.1778, -0.2404, -0.0731,\n","          0.2187,  0.1790],\n","        [-0.2678,  0.1546,  0.0198, -0.2857,  0.1506, -0.2251, -0.0187, -0.1561,\n","         -0.1255, -0.2470],\n","        [ 0.0672, -0.0519, -0.0175, -0.3074,  0.1664, -0.1930, -0.3030,  0.2753,\n","         -0.0198, -0.0182],\n","        [-0.0603,  0.1281, -0.2478,  0.1931,  0.1995, -0.1887,  0.0220,  0.1641,\n","          0.0661, -0.2699],\n","        [-0.2021,  0.0598,  0.3105,  0.0263,  0.1581,  0.0868,  0.2885,  0.0203,\n","         -0.2870,  0.0844],\n","        [-0.0513,  0.0813, -0.2757,  0.1062,  0.1439, -0.1163,  0.0945, -0.0703,\n","          0.0711, -0.1170],\n","        [ 0.1224,  0.1614,  0.2038,  0.2710, -0.2854, -0.1659, -0.2760, -0.1922,\n","          0.0316,  0.2298],\n","        [ 0.2157, -0.0439, -0.1077,  0.0857, -0.1996, -0.2572, -0.1507,  0.1673,\n","         -0.3138, -0.0757],\n","        [-0.0701, -0.1324,  0.2214, -0.0583, -0.2535,  0.0227,  0.0405, -0.1745,\n","         -0.1491, -0.0694],\n","        [-0.1896, -0.2899, -0.0875, -0.2686,  0.0119,  0.0413,  0.2310,  0.0900,\n","          0.0301, -0.2746]])\n","linear_hidden.0.bias tensor([-0.1201, -0.1669,  0.2965,  0.1794, -0.1919,  0.1257,  0.2000,  0.0442,\n","        -0.0840, -0.2044])\n","linear_output.weight tensor([[-0.0871, -0.1301, -0.1705,  0.3067,  0.2898, -0.0855,  0.0181,  0.0646,\n","         -0.1056,  0.1972]])\n","linear_output.bias tensor([0.0854])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6950774788856506 \t Validate_Accuracy: 0.555\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.7058886289596558 \t Validate_Accuracy: 0.4245\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7708263397216797 \t Validate_Accuracy: 0.515\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.7194616198539734 \t Validate_Accuracy: 0.528\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6874531507492065 \t Validate_Accuracy: 0.677\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6707026958465576 \t Validate_Accuracy: 0.5215\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6684058904647827 \t Validate_Accuracy: 0.7185\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6436222195625305 \t Validate_Accuracy: 0.6135\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6252893209457397 \t Validate_Accuracy: 0.638\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6017640233039856 \t Validate_Accuracy: 0.7905\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.5645011067390442 \t Validate_Accuracy: 0.792\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.5418821573257446 \t Validate_Accuracy: 0.7975\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.5120754241943359 \t Validate_Accuracy: 0.8095\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.4775799810886383 \t Validate_Accuracy: 0.804\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.46367019414901733 \t Validate_Accuracy: 0.807\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.45064809918403625 \t Validate_Accuracy: 0.813\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.43337932229042053 \t Validate_Accuracy: 0.816\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.42266157269477844 \t Validate_Accuracy: 0.8155\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.4144466519355774 \t Validate_Accuracy: 0.8215\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.40337201952934265 \t Validate_Accuracy: 0.8285\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3991345465183258 \t Validate_Accuracy: 0.828\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.39296194911003113 \t Validate_Accuracy: 0.8255\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3874700963497162 \t Validate_Accuracy: 0.827\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3836578130722046 \t Validate_Accuracy: 0.824\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3769005835056305 \t Validate_Accuracy: 0.8225\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3747016489505768 \t Validate_Accuracy: 0.831\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.36998769640922546 \t Validate_Accuracy: 0.831\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.36659860610961914 \t Validate_Accuracy: 0.826\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.36089494824409485 \t Validate_Accuracy: 0.834\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3566439747810364 \t Validate_Accuracy: 0.832\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.3523200750350952 \t Validate_Accuracy: 0.83\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.3488052785396576 \t Validate_Accuracy: 0.8275\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3465580642223358 \t Validate_Accuracy: 0.8315\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3424975275993347 \t Validate_Accuracy: 0.833\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.3390011191368103 \t Validate_Accuracy: 0.8325\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3360767662525177 \t Validate_Accuracy: 0.8365\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.33433958888053894 \t Validate_Accuracy: 0.84\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3325406610965729 \t Validate_Accuracy: 0.839\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.3304654061794281 \t Validate_Accuracy: 0.8395\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.3282281160354614 \t Validate_Accuracy: 0.838\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.3267027735710144 \t Validate_Accuracy: 0.8385\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.3250309228897095 \t Validate_Accuracy: 0.837\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.3240460753440857 \t Validate_Accuracy: 0.8375\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.322343647480011 \t Validate_Accuracy: 0.8405\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.32056963443756104 \t Validate_Accuracy: 0.8385\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.3194369375705719 \t Validate_Accuracy: 0.835\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.31920066475868225 \t Validate_Accuracy: 0.837\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.3176380693912506 \t Validate_Accuracy: 0.8375\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.31565117835998535 \t Validate_Accuracy: 0.8405\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.3144705593585968 \t Validate_Accuracy: 0.8395\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.31371498107910156 \t Validate_Accuracy: 0.8365\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3128543496131897 \t Validate_Accuracy: 0.837\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.31153514981269836 \t Validate_Accuracy: 0.8405\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.31017446517944336 \t Validate_Accuracy: 0.843\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.3088025450706482 \t Validate_Accuracy: 0.8425\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.30833524465560913 \t Validate_Accuracy: 0.842\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.3080786466598511 \t Validate_Accuracy: 0.844\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.3082449734210968 \t Validate_Accuracy: 0.844\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.30958977341651917 \t Validate_Accuracy: 0.8435\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.3096548914909363 \t Validate_Accuracy: 0.8425\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.30736276507377625 \t Validate_Accuracy: 0.8455\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.30537018179893494 \t Validate_Accuracy: 0.8495\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.3064720332622528 \t Validate_Accuracy: 0.846\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.30662280321121216 \t Validate_Accuracy: 0.851\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.30436062812805176 \t Validate_Accuracy: 0.844\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.3030591905117035 \t Validate_Accuracy: 0.839\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3052307665348053 \t Validate_Accuracy: 0.8425\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.3012688457965851 \t Validate_Accuracy: 0.8445\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2997737228870392 \t Validate_Accuracy: 0.842\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.30156177282333374 \t Validate_Accuracy: 0.8415\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.29816484451293945 \t Validate_Accuracy: 0.8385\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.2986137568950653 \t Validate_Accuracy: 0.8385\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.30128002166748047 \t Validate_Accuracy: 0.84\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2965245842933655 \t Validate_Accuracy: 0.843\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.299039363861084 \t Validate_Accuracy: 0.8385\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2980941832065582 \t Validate_Accuracy: 0.838\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.295318067073822 \t Validate_Accuracy: 0.841\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.2978111505508423 \t Validate_Accuracy: 0.8395\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2930503189563751 \t Validate_Accuracy: 0.8345\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2943079173564911 \t Validate_Accuracy: 0.8415\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2923460304737091 \t Validate_Accuracy: 0.8405\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.29165440797805786 \t Validate_Accuracy: 0.841\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2913796603679657 \t Validate_Accuracy: 0.8415\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.2899927794933319 \t Validate_Accuracy: 0.838\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.29043203592300415 \t Validate_Accuracy: 0.839\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.2890457212924957 \t Validate_Accuracy: 0.838\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.28919169306755066 \t Validate_Accuracy: 0.841\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.28847718238830566 \t Validate_Accuracy: 0.843\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.28778520226478577 \t Validate_Accuracy: 0.842\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.28789079189300537 \t Validate_Accuracy: 0.838\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2877912223339081 \t Validate_Accuracy: 0.8375\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.28695008158683777 \t Validate_Accuracy: 0.8435\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.28665784001350403 \t Validate_Accuracy: 0.8415\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.28597724437713623 \t Validate_Accuracy: 0.8415\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.28615802526474 \t Validate_Accuracy: 0.8435\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.28585559129714966 \t Validate_Accuracy: 0.8435\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.28542932868003845 \t Validate_Accuracy: 0.8395\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2851746082305908 \t Validate_Accuracy: 0.841\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.28462615609169006 \t Validate_Accuracy: 0.842\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2844199538230896 \t Validate_Accuracy: 0.8385\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2844269871711731 \t Validate_Accuracy: 0.8375\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.28432875871658325 \t Validate_Accuracy: 0.8395\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.28440266847610474 \t Validate_Accuracy: 0.8405\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2844572961330414 \t Validate_Accuracy: 0.8375\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2842281460762024 \t Validate_Accuracy: 0.839\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.28340354561805725 \t Validate_Accuracy: 0.84\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2835000157356262 \t Validate_Accuracy: 0.8395\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2832391560077667 \t Validate_Accuracy: 0.838\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2828655242919922 \t Validate_Accuracy: 0.8355\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2832043468952179 \t Validate_Accuracy: 0.839\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2831316292285919 \t Validate_Accuracy: 0.838\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.28380343317985535 \t Validate_Accuracy: 0.838\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2819366455078125 \t Validate_Accuracy: 0.8385\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2828323245048523 \t Validate_Accuracy: 0.839\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2846549153327942 \t Validate_Accuracy: 0.8405\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.2825709879398346 \t Validate_Accuracy: 0.838\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.28391018509864807 \t Validate_Accuracy: 0.8365\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.2832373082637787 \t Validate_Accuracy: 0.8395\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.28263792395591736 \t Validate_Accuracy: 0.842\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.28178587555885315 \t Validate_Accuracy: 0.8395\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.2833886444568634 \t Validate_Accuracy: 0.843\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.28099924325942993 \t Validate_Accuracy: 0.841\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2818296253681183 \t Validate_Accuracy: 0.8415\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.2816488444805145 \t Validate_Accuracy: 0.8415\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.28099507093429565 \t Validate_Accuracy: 0.842\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.28055036067962646 \t Validate_Accuracy: 0.846\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2799774408340454 \t Validate_Accuracy: 0.8425\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.28086453676223755 \t Validate_Accuracy: 0.842\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.2796880304813385 \t Validate_Accuracy: 0.844\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.2799568176269531 \t Validate_Accuracy: 0.844\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.27927494049072266 \t Validate_Accuracy: 0.8405\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.27939778566360474 \t Validate_Accuracy: 0.84\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.27960821986198425 \t Validate_Accuracy: 0.8425\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.27927136421203613 \t Validate_Accuracy: 0.842\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.27947697043418884 \t Validate_Accuracy: 0.8405\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2788550853729248 \t Validate_Accuracy: 0.838\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.27859002351760864 \t Validate_Accuracy: 0.8425\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.27868032455444336 \t Validate_Accuracy: 0.8375\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.27823567390441895 \t Validate_Accuracy: 0.8405\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.27829959988594055 \t Validate_Accuracy: 0.841\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2783334255218506 \t Validate_Accuracy: 0.838\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.2780466675758362 \t Validate_Accuracy: 0.842\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.2784779965877533 \t Validate_Accuracy: 0.84\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.277929425239563 \t Validate_Accuracy: 0.8405\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2783273458480835 \t Validate_Accuracy: 0.8405\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.27846091985702515 \t Validate_Accuracy: 0.8425\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.27764376997947693 \t Validate_Accuracy: 0.8405\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.2773178815841675 \t Validate_Accuracy: 0.841\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.277312308549881 \t Validate_Accuracy: 0.8405\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.27753233909606934 \t Validate_Accuracy: 0.8405\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2779076397418976 \t Validate_Accuracy: 0.843\n","model parameters! \n","\n","conv1.weight tensor([[[[-1.7378, -0.6140, -1.6496],\n","          [-1.9302,  0.1593, -1.3414],\n","          [-1.8748, -0.2129, -2.4551]]]])\n","conv1.bias tensor([-0.7370])\n","first_linear.weight tensor([[-1.7183,  0.3732, -3.7658,  1.2876, -0.8311,  0.2624, -0.3405,  0.1939,\n","         -2.3565],\n","        [ 0.3655,  0.9825,  0.4897, -0.2408,  0.4718,  2.0900, -0.0324, -0.4078,\n","          0.0377],\n","        [-0.0379, -0.2430, -0.4929,  0.4935,  0.6669,  0.4003,  1.0812,  0.9213,\n","          0.6151],\n","        [-2.7440, -1.0940,  1.0759, -0.8631, -0.1072, -3.3336, -1.2835,  0.9268,\n","          1.2598],\n","        [-0.4937, -1.3595,  0.5664,  1.1274, -1.8473,  3.3198, -0.0613, -1.4463,\n","          1.2298],\n","        [ 2.0285, -3.5946, -2.5731,  0.6361, -2.1040, -0.8040, -2.9238, -0.4939,\n","          4.3242],\n","        [ 2.9027, -0.5150,  0.6126,  3.1566,  0.0940, -0.8810, -3.0972,  0.5320,\n","          0.6187],\n","        [-0.6369, -1.7342,  1.4785,  1.0044, -2.5936, -1.3439, -1.5415,  0.5112,\n","         -1.5218],\n","        [ 0.4089, -1.5899, -1.2516, -2.0457, -1.4649, -0.5991,  2.2211, -4.0338,\n","          2.0086],\n","        [ 0.0597,  0.4605,  2.2249,  1.4542,  1.6899, -1.6206,  1.4290, -0.4314,\n","          1.6145]])\n","first_linear.bias tensor([-0.0746, -2.8181, -2.6740, -0.9490, -1.0453, -0.3574,  2.0866, -1.4462,\n","         0.4835, -0.5134])\n","linear_hidden.0.weight tensor([[-0.1389,  1.8147,  1.1295,  1.2383,  0.4777,  1.9912, -1.2721,  2.3254,\n","          0.7332, -2.2387],\n","        [ 1.0390,  2.0811, -4.1304,  0.5796,  0.8764,  1.0410, -1.8159,  1.1877,\n","          1.7675, -1.9747],\n","        [ 0.6235, -0.7737, -1.6632, -1.8608, -1.7046, -1.0490,  1.4395,  0.2226,\n","         -0.3381,  0.4917],\n","        [ 1.1954, -0.5503, -1.3515, -3.4556,  1.2670,  0.4958, -4.6607,  0.0350,\n","         -0.1117,  1.4754],\n","        [-2.3611,  4.0052,  2.8847, -0.1483,  0.1086, -0.0789, -0.6580,  1.0741,\n","         -0.4988,  1.0749],\n","        [-2.9583,  1.6492, -2.8321,  2.4487,  1.7162,  2.2187,  0.8774, -2.0609,\n","         -1.8697, -1.5356],\n","        [ 0.3295,  0.3581,  0.4048, -0.8537, -1.3396, -0.9838, -1.6038, -0.7649,\n","          1.0081, -0.4226],\n","        [-1.4997,  2.8004,  2.9332,  1.7804,  3.9953, -0.5029, -0.4838, -2.1896,\n","         -1.7381, -0.5510],\n","        [ 2.4862, -2.3268, -1.8489,  1.4408, -1.2500, -0.8611,  0.3644,  1.7033,\n","          4.4237, -1.6355],\n","        [ 1.5144, -0.1657,  1.0208, -1.3008, -2.1738, -2.6260, -1.3704, -2.0175,\n","          0.2644,  2.2382]])\n","linear_hidden.0.bias tensor([-2.5149,  2.8210,  1.4576, -2.4388, -2.0195,  1.4393,  1.8542, -1.4937,\n","         3.3211,  0.3957])\n","linear_output.weight tensor([[ 2.2137, -0.7735, -0.6213, -1.4824,  1.0426, -1.2172,  1.6837,  0.8422,\n","         -1.2236, -0.8947]])\n","linear_output.bias tensor([2.7115])\n","Testing out: \n","batch_size:  3422\n","train_size:  268\n","n_epochs:  215\n","lr:  0.00027871382494138505\n","weight_decay:  0.00015990468113405264\n","betas0:  0.9971696720330007\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1157, -0.1591,  0.2961],\n","          [-0.2093, -0.1253, -0.1696],\n","          [-0.1135,  0.0828,  0.1589]]]])\n","conv1.bias tensor([-0.3270])\n","first_linear.weight tensor([[-0.1747,  0.3217, -0.1437,  0.2551,  0.1081,  0.2703, -0.0032,  0.1301,\n","          0.2380],\n","        [-0.2555, -0.0308, -0.0155, -0.0833, -0.1919, -0.2053,  0.2745,  0.1322,\n","         -0.1012],\n","        [-0.0274, -0.0562, -0.2749, -0.2804,  0.1093,  0.0210,  0.1588, -0.2579,\n","         -0.2749],\n","        [ 0.1602,  0.0170, -0.0595, -0.1595, -0.2956,  0.2632,  0.2673, -0.0702,\n","          0.1114],\n","        [ 0.2516, -0.1258, -0.3087,  0.2805, -0.2815,  0.2547, -0.0293, -0.3237,\n","         -0.1469],\n","        [-0.2156, -0.1248, -0.0950, -0.0356,  0.1767,  0.1619, -0.0700,  0.1232,\n","          0.0745],\n","        [-0.2961,  0.1596,  0.2057, -0.0916,  0.1583,  0.0115, -0.2498,  0.1710,\n","          0.0683],\n","        [-0.1430, -0.0204,  0.2810,  0.2878, -0.2218, -0.0201, -0.2661,  0.2571,\n","         -0.1777],\n","        [ 0.0505, -0.0198,  0.0589,  0.1927,  0.1671,  0.2407,  0.0216, -0.2326,\n","          0.1441],\n","        [ 0.1521, -0.2009, -0.0468, -0.0754,  0.3313, -0.1270, -0.1145, -0.2278,\n","          0.1860]])\n","first_linear.bias tensor([ 0.2843, -0.0278, -0.1442, -0.2604, -0.0458, -0.0720,  0.3317, -0.3215,\n","         0.1012,  0.2346])\n","linear_hidden.0.weight tensor([[-0.2855, -0.3044, -0.1380, -0.1246,  0.2482,  0.1536,  0.0794, -0.2719,\n","         -0.1156,  0.1946],\n","        [-0.2917, -0.0767, -0.2640,  0.2048, -0.0528, -0.2933,  0.0650,  0.2934,\n","         -0.2887,  0.0295],\n","        [-0.3080, -0.1524,  0.0587, -0.0422, -0.0480, -0.2904, -0.2590,  0.2700,\n","         -0.2840, -0.1102],\n","        [ 0.1435,  0.0245,  0.2306, -0.2040,  0.0617, -0.0932,  0.2960, -0.0898,\n","          0.1027,  0.1710],\n","        [-0.2138, -0.1003,  0.0489, -0.1820, -0.2649,  0.1360, -0.2925, -0.1177,\n","          0.0078, -0.0137],\n","        [ 0.1153,  0.2581, -0.2402,  0.0926,  0.1570, -0.2401,  0.3075, -0.1864,\n","          0.2720, -0.0744],\n","        [ 0.0791,  0.1887, -0.2611, -0.2194,  0.0211, -0.0314, -0.2973,  0.3152,\n","         -0.0727,  0.2848],\n","        [-0.0596,  0.1486,  0.1353,  0.0669,  0.1639,  0.1266, -0.3139,  0.0131,\n","          0.0352,  0.2585],\n","        [-0.0706, -0.1506, -0.2561,  0.2732, -0.1443,  0.1373,  0.1546,  0.2224,\n","         -0.2158, -0.2649],\n","        [-0.0984,  0.3139,  0.1343, -0.1084,  0.1259, -0.0504, -0.1571, -0.1305,\n","         -0.1254,  0.1428]])\n","linear_hidden.0.bias tensor([-0.1934, -0.2447,  0.1440, -0.0027, -0.0872,  0.1323, -0.2064,  0.1378,\n","        -0.1746, -0.0466])\n","linear_output.weight tensor([[-0.2127,  0.1131, -0.0467,  0.1933, -0.0286,  0.2841,  0.2709,  0.3113,\n","          0.1093, -0.2881]])\n","linear_output.bias tensor([-0.1434])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7072016596794128 \t Validate_Accuracy: 0.503\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.7068790197372437 \t Validate_Accuracy: 0.503\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7065585255622864 \t Validate_Accuracy: 0.503\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.7062397003173828 \t Validate_Accuracy: 0.503\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.7059236764907837 \t Validate_Accuracy: 0.503\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.7056093215942383 \t Validate_Accuracy: 0.503\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.7052974104881287 \t Validate_Accuracy: 0.503\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.7049877047538757 \t Validate_Accuracy: 0.5035\n","epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([268])) that is different to the input size (torch.Size([268, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 9 \t Train Loss: 0.7046806812286377 \t Validate_Accuracy: 0.503\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.7043759822845459 \t Validate_Accuracy: 0.5035\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.7040738463401794 \t Validate_Accuracy: 0.5035\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.7037736773490906 \t Validate_Accuracy: 0.5035\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.703475832939148 \t Validate_Accuracy: 0.5035\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.7031807899475098 \t Validate_Accuracy: 0.503\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.7028876543045044 \t Validate_Accuracy: 0.504\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.7025976181030273 \t Validate_Accuracy: 0.5035\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.7023091316223145 \t Validate_Accuracy: 0.5035\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.7020238041877747 \t Validate_Accuracy: 0.503\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.7017409205436707 \t Validate_Accuracy: 0.504\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.7014603018760681 \t Validate_Accuracy: 0.505\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.7011823654174805 \t Validate_Accuracy: 0.507\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.7009068727493286 \t Validate_Accuracy: 0.505\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.7006335854530334 \t Validate_Accuracy: 0.5055\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.7003632187843323 \t Validate_Accuracy: 0.5065\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.7000951766967773 \t Validate_Accuracy: 0.508\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6998294591903687 \t Validate_Accuracy: 0.508\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6995668411254883 \t Validate_Accuracy: 0.507\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6993067860603333 \t Validate_Accuracy: 0.5055\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6990490555763245 \t Validate_Accuracy: 0.505\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6987937688827515 \t Validate_Accuracy: 0.5055\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6985414624214172 \t Validate_Accuracy: 0.5055\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6982913613319397 \t Validate_Accuracy: 0.505\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6980441212654114 \t Validate_Accuracy: 0.5055\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.697799026966095 \t Validate_Accuracy: 0.505\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6975569725036621 \t Validate_Accuracy: 0.507\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6973174214363098 \t Validate_Accuracy: 0.507\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6970800161361694 \t Validate_Accuracy: 0.51\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.6968461275100708 \t Validate_Accuracy: 0.51\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6966139674186707 \t Validate_Accuracy: 0.5135\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.6963843107223511 \t Validate_Accuracy: 0.515\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6961572766304016 \t Validate_Accuracy: 0.517\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.69593346118927 \t Validate_Accuracy: 0.509\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6957113742828369 \t Validate_Accuracy: 0.5055\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6954922676086426 \t Validate_Accuracy: 0.508\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6952753663063049 \t Validate_Accuracy: 0.5115\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.695061206817627 \t Validate_Accuracy: 0.5085\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6948495507240295 \t Validate_Accuracy: 0.5045\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6946397423744202 \t Validate_Accuracy: 0.5105\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.6944330930709839 \t Validate_Accuracy: 0.5065\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6942282915115356 \t Validate_Accuracy: 0.512\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6940264701843262 \t Validate_Accuracy: 0.5045\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.6938267946243286 \t Validate_Accuracy: 0.5055\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6936295628547668 \t Validate_Accuracy: 0.5085\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6934345960617065 \t Validate_Accuracy: 0.5125\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6932418346405029 \t Validate_Accuracy: 0.5155\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.6930514574050903 \t Validate_Accuracy: 0.5155\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.6928637027740479 \t Validate_Accuracy: 0.5135\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6926782727241516 \t Validate_Accuracy: 0.5145\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.6924943327903748 \t Validate_Accuracy: 0.515\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6923134922981262 \t Validate_Accuracy: 0.5215\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.6921347379684448 \t Validate_Accuracy: 0.5225\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6919578313827515 \t Validate_Accuracy: 0.524\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6917833089828491 \t Validate_Accuracy: 0.5235\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6916112303733826 \t Validate_Accuracy: 0.5205\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.691440999507904 \t Validate_Accuracy: 0.522\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.6912728548049927 \t Validate_Accuracy: 0.519\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6911072134971619 \t Validate_Accuracy: 0.5175\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6909434199333191 \t Validate_Accuracy: 0.519\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6907813549041748 \t Validate_Accuracy: 0.514\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.6906220316886902 \t Validate_Accuracy: 0.5095\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.6904643774032593 \t Validate_Accuracy: 0.511\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.6903093457221985 \t Validate_Accuracy: 0.512\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.6901556849479675 \t Validate_Accuracy: 0.511\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.6900039315223694 \t Validate_Accuracy: 0.51\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.6898548007011414 \t Validate_Accuracy: 0.506\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.6897070407867432 \t Validate_Accuracy: 0.5045\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.6895614266395569 \t Validate_Accuracy: 0.503\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.689417839050293 \t Validate_Accuracy: 0.503\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.6892762184143066 \t Validate_Accuracy: 0.5005\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.6891359686851501 \t Validate_Accuracy: 0.501\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.6889979243278503 \t Validate_Accuracy: 0.5005\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.688861608505249 \t Validate_Accuracy: 0.4985\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.6887274384498596 \t Validate_Accuracy: 0.497\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.6885949373245239 \t Validate_Accuracy: 0.497\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.6884639859199524 \t Validate_Accuracy: 0.4995\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.6883355379104614 \t Validate_Accuracy: 0.4995\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.6882079839706421 \t Validate_Accuracy: 0.499\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.6880825161933899 \t Validate_Accuracy: 0.499\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.6879588961601257 \t Validate_Accuracy: 0.4995\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.6878365874290466 \t Validate_Accuracy: 0.5\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.687716543674469 \t Validate_Accuracy: 0.499\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.6875978708267212 \t Validate_Accuracy: 0.499\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.687480628490448 \t Validate_Accuracy: 0.4985\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.6873651146888733 \t Validate_Accuracy: 0.4985\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.6872512102127075 \t Validate_Accuracy: 0.4985\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.6871387958526611 \t Validate_Accuracy: 0.498\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.6870281100273132 \t Validate_Accuracy: 0.4985\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.686919093132019 \t Validate_Accuracy: 0.498\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.6868113279342651 \t Validate_Accuracy: 0.498\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.6867052912712097 \t Validate_Accuracy: 0.498\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.6866002082824707 \t Validate_Accuracy: 0.498\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.6864970922470093 \t Validate_Accuracy: 0.498\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.6863950490951538 \t Validate_Accuracy: 0.498\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.686294436454773 \t Validate_Accuracy: 0.498\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.6861958503723145 \t Validate_Accuracy: 0.4975\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.6860979199409485 \t Validate_Accuracy: 0.4975\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.6860014796257019 \t Validate_Accuracy: 0.497\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.6859066486358643 \t Validate_Accuracy: 0.497\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.6858124732971191 \t Validate_Accuracy: 0.497\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.6857206225395203 \t Validate_Accuracy: 0.497\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.6856293082237244 \t Validate_Accuracy: 0.497\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.685539186000824 \t Validate_Accuracy: 0.497\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.6854501962661743 \t Validate_Accuracy: 0.497\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.6853630542755127 \t Validate_Accuracy: 0.497\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.6852761507034302 \t Validate_Accuracy: 0.497\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.6851907968521118 \t Validate_Accuracy: 0.497\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.6851062774658203 \t Validate_Accuracy: 0.497\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.6850230693817139 \t Validate_Accuracy: 0.497\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.6849409341812134 \t Validate_Accuracy: 0.497\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.6848598718643188 \t Validate_Accuracy: 0.497\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.6847798824310303 \t Validate_Accuracy: 0.497\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.6847005486488342 \t Validate_Accuracy: 0.497\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.6846221685409546 \t Validate_Accuracy: 0.497\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.6845450401306152 \t Validate_Accuracy: 0.497\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.6844680309295654 \t Validate_Accuracy: 0.497\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.6843929886817932 \t Validate_Accuracy: 0.497\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.6843183040618896 \t Validate_Accuracy: 0.497\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.6842443346977234 \t Validate_Accuracy: 0.497\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.6841713190078735 \t Validate_Accuracy: 0.497\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.6840991377830505 \t Validate_Accuracy: 0.497\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.6840273141860962 \t Validate_Accuracy: 0.497\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.6839563846588135 \t Validate_Accuracy: 0.497\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.6838862299919128 \t Validate_Accuracy: 0.497\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.6838162541389465 \t Validate_Accuracy: 0.497\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.6837480068206787 \t Validate_Accuracy: 0.497\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.6836796998977661 \t Validate_Accuracy: 0.497\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.6836119890213013 \t Validate_Accuracy: 0.497\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.6835448741912842 \t Validate_Accuracy: 0.497\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.6834782958030701 \t Validate_Accuracy: 0.497\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.6834124326705933 \t Validate_Accuracy: 0.497\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.6833467483520508 \t Validate_Accuracy: 0.497\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.6832817792892456 \t Validate_Accuracy: 0.497\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.6832170486450195 \t Validate_Accuracy: 0.497\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.6831524968147278 \t Validate_Accuracy: 0.497\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.6830887794494629 \t Validate_Accuracy: 0.497\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.6830253005027771 \t Validate_Accuracy: 0.497\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.6829620599746704 \t Validate_Accuracy: 0.497\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.6828992366790771 \t Validate_Accuracy: 0.497\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.6828365921974182 \t Validate_Accuracy: 0.497\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.6827741265296936 \t Validate_Accuracy: 0.497\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.6827123761177063 \t Validate_Accuracy: 0.497\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.682650625705719 \t Validate_Accuracy: 0.497\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.6825886368751526 \t Validate_Accuracy: 0.497\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.6825273036956787 \t Validate_Accuracy: 0.497\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.6824659705162048 \t Validate_Accuracy: 0.497\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.6824046969413757 \t Validate_Accuracy: 0.497\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.6823436617851257 \t Validate_Accuracy: 0.497\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.6822826862335205 \t Validate_Accuracy: 0.497\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.682221531867981 \t Validate_Accuracy: 0.497\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.6821608543395996 \t Validate_Accuracy: 0.497\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.6820995211601257 \t Validate_Accuracy: 0.497\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.6820387244224548 \t Validate_Accuracy: 0.497\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.6819779276847839 \t Validate_Accuracy: 0.497\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.6819168329238892 \t Validate_Accuracy: 0.497\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.6818557381629944 \t Validate_Accuracy: 0.497\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.6817941665649414 \t Validate_Accuracy: 0.497\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.6817328929901123 \t Validate_Accuracy: 0.497\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.681671679019928 \t Validate_Accuracy: 0.497\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.6816097497940063 \t Validate_Accuracy: 0.497\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.6815481185913086 \t Validate_Accuracy: 0.497\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.6814858913421631 \t Validate_Accuracy: 0.497\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.6814236044883728 \t Validate_Accuracy: 0.497\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.6813607811927795 \t Validate_Accuracy: 0.497\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.6812981963157654 \t Validate_Accuracy: 0.497\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.6812350749969482 \t Validate_Accuracy: 0.497\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.681171715259552 \t Validate_Accuracy: 0.497\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.6811078786849976 \t Validate_Accuracy: 0.497\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.6810437440872192 \t Validate_Accuracy: 0.497\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.6809795498847961 \t Validate_Accuracy: 0.497\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.6809143424034119 \t Validate_Accuracy: 0.497\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.6808492541313171 \t Validate_Accuracy: 0.497\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.6807840466499329 \t Validate_Accuracy: 0.497\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.6807176470756531 \t Validate_Accuracy: 0.497\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.6806509494781494 \t Validate_Accuracy: 0.497\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.680583655834198 \t Validate_Accuracy: 0.497\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.680516242980957 \t Validate_Accuracy: 0.497\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.6804478764533997 \t Validate_Accuracy: 0.497\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.6803792715072632 \t Validate_Accuracy: 0.497\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.6803101301193237 \t Validate_Accuracy: 0.497\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.6802403330802917 \t Validate_Accuracy: 0.497\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.6801698803901672 \t Validate_Accuracy: 0.497\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.6800989508628845 \t Validate_Accuracy: 0.497\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.6800272464752197 \t Validate_Accuracy: 0.497\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.6799545288085938 \t Validate_Accuracy: 0.497\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.6798816323280334 \t Validate_Accuracy: 0.497\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.6798079013824463 \t Validate_Accuracy: 0.497\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.6797336935997009 \t Validate_Accuracy: 0.497\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.6796582937240601 \t Validate_Accuracy: 0.497\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.6795822978019714 \t Validate_Accuracy: 0.497\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.6795054078102112 \t Validate_Accuracy: 0.497\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.6794278025627136 \t Validate_Accuracy: 0.497\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.6793494820594788 \t Validate_Accuracy: 0.497\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.6792703866958618 \t Validate_Accuracy: 0.497\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.6791902184486389 \t Validate_Accuracy: 0.497\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.6791092753410339 \t Validate_Accuracy: 0.497\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.6790273785591125 \t Validate_Accuracy: 0.497\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.6789445281028748 \t Validate_Accuracy: 0.497\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.6788609027862549 \t Validate_Accuracy: 0.497\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.678776204586029 \t Validate_Accuracy: 0.497\n","epoch 210\n","Epoch: 210 \t Train Loss: 0.6786909699440002 \t Validate_Accuracy: 0.497\n","epoch 211\n","Epoch: 211 \t Train Loss: 0.6786038875579834 \t Validate_Accuracy: 0.497\n","epoch 212\n","Epoch: 212 \t Train Loss: 0.6785168647766113 \t Validate_Accuracy: 0.497\n","epoch 213\n","Epoch: 213 \t Train Loss: 0.678428053855896 \t Validate_Accuracy: 0.497\n","epoch 214\n","Epoch: 214 \t Train Loss: 0.6783387064933777 \t Validate_Accuracy: 0.497\n","epoch 215\n","Epoch: 215 \t Train Loss: 0.6782479882240295 \t Validate_Accuracy: 0.497\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.0285, -0.1920,  0.2484],\n","          [-0.1731, -0.1912, -0.2238],\n","          [-0.1273,  0.1483,  0.1734]]]])\n","conv1.bias tensor([-0.3103])\n","first_linear.weight tensor([[-0.2222,  0.3443, -0.1965,  0.2352,  0.1219,  0.2313, -0.0383,  0.1010,\n","          0.1783],\n","        [-0.2996, -0.0060, -0.0649, -0.0983, -0.1684, -0.2364,  0.2429,  0.1088,\n","         -0.1579],\n","        [ 0.0026, -0.0437, -0.2459, -0.2556,  0.1217,  0.0450,  0.1829, -0.2341,\n","         -0.2436],\n","        [ 0.1348, -0.0037, -0.0820, -0.1820, -0.2967,  0.2422,  0.2416, -0.0886,\n","          0.0876],\n","        [ 0.1978, -0.0958, -0.3773,  0.2579, -0.2324,  0.2128, -0.0676, -0.3321,\n","         -0.2207],\n","        [-0.1688, -0.1491, -0.0436, -0.0148,  0.1541,  0.1970, -0.0342,  0.1489,\n","          0.1328],\n","        [-0.3476,  0.1980,  0.1519, -0.0902,  0.1656, -0.0326, -0.2737,  0.1349,\n","          0.0040],\n","        [-0.1777, -0.0208,  0.2414,  0.2689, -0.2292, -0.0501, -0.2939,  0.2310,\n","         -0.2206],\n","        [ 0.0130, -0.0055,  0.0189,  0.1716,  0.1725,  0.2099, -0.0067, -0.2558,\n","          0.0977],\n","        [ 0.1083, -0.1871, -0.1009, -0.0883,  0.3324, -0.1736, -0.1499, -0.2592,\n","          0.1271]])\n","first_linear.bias tensor([ 0.3232,  0.0094, -0.1696, -0.2373,  0.0039, -0.1104,  0.3723, -0.2911,\n","         0.1330,  0.2773])\n","linear_hidden.0.weight tensor([[-3.5146e-01, -3.5465e-01, -1.8595e-01, -8.4716e-02,  1.9034e-01,\n","          2.0736e-01,  3.8046e-02, -2.4804e-01, -8.0940e-02,  1.6515e-01],\n","        [-2.2964e-01, -3.1789e-02, -2.2093e-01,  1.6889e-01, -2.6226e-04,\n","         -3.3927e-01,  1.0159e-01,  2.7134e-01, -3.1548e-01,  5.6071e-02],\n","        [-3.8252e-01, -2.1230e-01,  9.7641e-04,  5.4885e-03, -1.1606e-01,\n","         -2.2704e-01, -3.0689e-01,  2.9195e-01, -2.3634e-01, -1.4123e-01],\n","        [ 2.0816e-01,  7.1594e-02,  2.7619e-01, -2.4279e-01,  1.1654e-01,\n","         -1.4570e-01,  3.3475e-01, -1.1020e-01,  6.8359e-02,  1.9694e-01],\n","        [-2.8845e-01, -1.5691e-01, -8.3390e-03, -1.3463e-01, -3.3080e-01,\n","          1.9687e-01, -3.3930e-01, -9.1229e-02,  5.0978e-02, -4.3000e-02],\n","        [ 1.7869e-01,  3.0525e-01, -1.9586e-01,  5.4753e-02,  2.1070e-01,\n","         -2.9089e-01,  3.4698e-01, -2.0910e-01,  2.3879e-01, -4.6082e-02],\n","        [ 1.4246e-01,  2.3650e-01, -2.1549e-01, -2.5708e-01,  7.6495e-02,\n","         -8.2555e-02, -2.5773e-01,  2.9187e-01, -1.0491e-01,  3.1321e-01],\n","        [ 4.7732e-03,  1.9681e-01,  1.8155e-01,  2.7621e-02,  2.2100e-01,\n","          7.3886e-02, -2.7353e-01, -1.0976e-02,  1.8991e-03,  2.8796e-01],\n","        [-1.0997e-02, -1.0670e-01, -2.1389e-01,  2.3710e-01, -9.1487e-02,\n","          8.8884e-02,  1.9059e-01,  1.9965e-01, -2.4219e-01, -2.3579e-01],\n","        [-1.6069e-01,  2.6567e-01,  8.9316e-02, -7.0687e-02,  7.0196e-02,\n","          1.2480e-03, -1.9607e-01, -1.0715e-01, -9.2513e-02,  1.1382e-01]])\n","linear_hidden.0.bias tensor([-0.2283, -0.2129,  0.1024,  0.0306, -0.1274,  0.1658, -0.1726,  0.1725,\n","        -0.1430, -0.0801])\n","linear_output.weight tensor([[-0.2710,  0.0799, -0.1042,  0.2361, -0.0802,  0.3268,  0.2455,  0.3531,\n","          0.0752, -0.2652]])\n","linear_output.bias tensor([-0.1092])\n","Testing out: \n","batch_size:  2090\n","train_size:  3222\n","n_epochs:  77\n","lr:  0.046595223394329505\n","weight_decay:  0.004237440309116579\n","betas0:  0.9999344409591338\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1448,  0.1648, -0.2655],\n","          [-0.2085,  0.1019, -0.0566],\n","          [ 0.0386, -0.3114,  0.0081]]]])\n","conv1.bias tensor([0.1810])\n","first_linear.weight tensor([[ 0.0451,  0.3255, -0.1283,  0.1989, -0.2505,  0.0357,  0.0804, -0.3067,\n","         -0.0992],\n","        [ 0.2191, -0.2506,  0.0702,  0.1571, -0.3296,  0.0252,  0.0319, -0.1590,\n","         -0.3313],\n","        [-0.2954, -0.0619, -0.2284,  0.1538, -0.1187, -0.1093,  0.2760, -0.3175,\n","         -0.2595],\n","        [-0.1593,  0.2674,  0.2808,  0.2662, -0.2451, -0.3125,  0.0873, -0.0208,\n","         -0.0347],\n","        [ 0.0375,  0.0209,  0.2015,  0.0126, -0.3284,  0.1155, -0.3189, -0.0852,\n","          0.1403],\n","        [-0.0060, -0.1254,  0.1842, -0.0964, -0.0629, -0.2942, -0.2430,  0.1608,\n","         -0.0565],\n","        [-0.2212,  0.1241, -0.0199, -0.3014, -0.2270, -0.1919, -0.1437,  0.0551,\n","          0.0562],\n","        [ 0.1047,  0.0420,  0.0006,  0.1946,  0.2044,  0.2340, -0.2697,  0.1631,\n","         -0.2993],\n","        [-0.0376,  0.2560,  0.1790, -0.0647, -0.1321, -0.0956, -0.0583, -0.0353,\n","          0.1980],\n","        [-0.2545,  0.3066, -0.3030,  0.2298, -0.2177, -0.2398,  0.0545, -0.2776,\n","         -0.2260]])\n","first_linear.bias tensor([ 0.1494, -0.1380,  0.3307,  0.0546, -0.2070, -0.2060,  0.2218,  0.1685,\n","         0.2615, -0.2494])\n","linear_hidden.0.weight tensor([[-0.3062,  0.2364,  0.0615, -0.0886,  0.2196, -0.3040, -0.1181, -0.2752,\n","          0.1231,  0.3057],\n","        [-0.1903, -0.1512, -0.2966, -0.0698,  0.1242,  0.2666, -0.1688,  0.1369,\n","         -0.1545,  0.1343],\n","        [-0.1420,  0.0628, -0.0159, -0.3142,  0.2826,  0.2507,  0.0065, -0.3145,\n","         -0.1952, -0.1227],\n","        [-0.1450,  0.1110,  0.0379,  0.2231,  0.1326,  0.0369, -0.0158,  0.2819,\n","          0.0418,  0.2990],\n","        [ 0.2150, -0.2167, -0.1912,  0.2001, -0.2719, -0.0639, -0.0800,  0.0322,\n","          0.1005, -0.1763],\n","        [-0.1204,  0.1864, -0.2616, -0.2085,  0.2736, -0.0555,  0.2047, -0.0871,\n","          0.3002, -0.2290],\n","        [-0.2947, -0.2525,  0.0677,  0.1560, -0.2503,  0.0661,  0.0630, -0.2081,\n","          0.0529,  0.0083],\n","        [-0.2850, -0.1073,  0.1486,  0.2115, -0.2019,  0.2907, -0.0739,  0.2925,\n","         -0.1675,  0.2278],\n","        [-0.0701, -0.2361, -0.1842,  0.1732,  0.1977, -0.2584,  0.3143,  0.0122,\n","          0.0439,  0.2148],\n","        [ 0.0383, -0.2467, -0.2914, -0.1408,  0.0453, -0.1788, -0.2473,  0.2161,\n","          0.1127,  0.0420]])\n","linear_hidden.0.bias tensor([ 0.1781,  0.0546,  0.3050,  0.0545, -0.2932,  0.2576, -0.0671, -0.1156,\n","         0.2155, -0.2211])\n","linear_output.weight tensor([[ 0.1253, -0.1366, -0.2705,  0.2501, -0.0457,  0.1230, -0.1005,  0.2397,\n","          0.2526, -0.0364]])\n","linear_output.bias tensor([-0.2679])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6971009373664856 \t Validate_Accuracy: 0.5975\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2090])) that is different to the input size (torch.Size([2090, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1132])) that is different to the input size (torch.Size([1132, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6881621181964874 \t Validate_Accuracy: 0.536\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6879068613052368 \t Validate_Accuracy: 0.502\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.683305025100708 \t Validate_Accuracy: 0.6045\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6724039614200592 \t Validate_Accuracy: 0.601\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6546335816383362 \t Validate_Accuracy: 0.7025\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6202370524406433 \t Validate_Accuracy: 0.779\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.577989012002945 \t Validate_Accuracy: 0.794\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5234812498092651 \t Validate_Accuracy: 0.829\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.47700898349285126 \t Validate_Accuracy: 0.8175\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.44731713831424713 \t Validate_Accuracy: 0.8045\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4374745637178421 \t Validate_Accuracy: 0.8195\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.4431181848049164 \t Validate_Accuracy: 0.812\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.4103652983903885 \t Validate_Accuracy: 0.8235\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.4049793481826782 \t Validate_Accuracy: 0.811\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.40304024517536163 \t Validate_Accuracy: 0.834\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3790171146392822 \t Validate_Accuracy: 0.83\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.37187357246875763 \t Validate_Accuracy: 0.835\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.35441456735134125 \t Validate_Accuracy: 0.8375\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.35286544263362885 \t Validate_Accuracy: 0.841\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.34321455657482147 \t Validate_Accuracy: 0.8445\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3345327526330948 \t Validate_Accuracy: 0.8495\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3307884782552719 \t Validate_Accuracy: 0.844\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3299119174480438 \t Validate_Accuracy: 0.8505\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.31376464664936066 \t Validate_Accuracy: 0.858\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.30941735208034515 \t Validate_Accuracy: 0.846\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3042711168527603 \t Validate_Accuracy: 0.852\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.30549994111061096 \t Validate_Accuracy: 0.853\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.29656754434108734 \t Validate_Accuracy: 0.851\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2955200523138046 \t Validate_Accuracy: 0.859\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.2920040488243103 \t Validate_Accuracy: 0.8595\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.2930515259504318 \t Validate_Accuracy: 0.856\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.29205359518527985 \t Validate_Accuracy: 0.8605\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.2897292822599411 \t Validate_Accuracy: 0.8645\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.28800511360168457 \t Validate_Accuracy: 0.8615\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.28385671973228455 \t Validate_Accuracy: 0.864\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.28508956730365753 \t Validate_Accuracy: 0.8665\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.28096117079257965 \t Validate_Accuracy: 0.8655\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.2802654057741165 \t Validate_Accuracy: 0.862\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2762124687433243 \t Validate_Accuracy: 0.8685\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.27280838787555695 \t Validate_Accuracy: 0.864\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2757284343242645 \t Validate_Accuracy: 0.867\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2719515711069107 \t Validate_Accuracy: 0.864\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2735888957977295 \t Validate_Accuracy: 0.867\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2672520726919174 \t Validate_Accuracy: 0.8655\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.27201882004737854 \t Validate_Accuracy: 0.8665\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2684551030397415 \t Validate_Accuracy: 0.8645\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.27451011538505554 \t Validate_Accuracy: 0.861\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.2685653120279312 \t Validate_Accuracy: 0.8695\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.2719590663909912 \t Validate_Accuracy: 0.8755\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.26979267597198486 \t Validate_Accuracy: 0.873\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.260549396276474 \t Validate_Accuracy: 0.8675\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2604079842567444 \t Validate_Accuracy: 0.8675\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.26025137305259705 \t Validate_Accuracy: 0.8645\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.26525773853063583 \t Validate_Accuracy: 0.872\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.25998449325561523 \t Validate_Accuracy: 0.8745\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.25572727620601654 \t Validate_Accuracy: 0.869\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2555485665798187 \t Validate_Accuracy: 0.876\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.2526993006467819 \t Validate_Accuracy: 0.874\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.24749600142240524 \t Validate_Accuracy: 0.873\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2527848333120346 \t Validate_Accuracy: 0.881\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.24905773997306824 \t Validate_Accuracy: 0.8825\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.24650729447603226 \t Validate_Accuracy: 0.878\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.24393285810947418 \t Validate_Accuracy: 0.8815\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2495139315724373 \t Validate_Accuracy: 0.876\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2526441365480423 \t Validate_Accuracy: 0.875\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.25444401800632477 \t Validate_Accuracy: 0.881\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.253498800098896 \t Validate_Accuracy: 0.883\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.24992366135120392 \t Validate_Accuracy: 0.884\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.24524636566638947 \t Validate_Accuracy: 0.88\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.24175699055194855 \t Validate_Accuracy: 0.876\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.24675939977169037 \t Validate_Accuracy: 0.8775\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.241021566092968 \t Validate_Accuracy: 0.8795\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.24009870737791061 \t Validate_Accuracy: 0.8795\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.24334656447172165 \t Validate_Accuracy: 0.8805\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2384055331349373 \t Validate_Accuracy: 0.8855\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.24177376180887222 \t Validate_Accuracy: 0.884\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.2330, -0.0921, -0.2524],\n","          [-0.1128,  0.0133, -0.0701],\n","          [-0.2156, -0.0863, -0.2616]]]])\n","conv1.bias tensor([0.0489])\n","first_linear.weight tensor([[ 2.9684e-03,  1.1208e-02,  3.2531e-04, -2.1813e-03, -5.0904e-03,\n","         -3.5481e-03,  2.4001e-03, -2.0245e-03,  2.1315e-03],\n","        [-2.5665e-01, -1.4909e-01, -6.7153e-02, -1.5288e-01, -1.4224e-01,\n","          3.9990e-02, -9.1917e-02, -1.0401e-02,  1.0332e-01],\n","        [ 8.2926e-02,  1.7497e-01,  3.0616e-01,  7.4738e-02,  2.4061e-01,\n","          2.9450e-01, -2.4028e-03,  8.3422e-02,  1.5268e-01],\n","        [-4.8078e-01, -1.8104e-02,  3.7304e-01,  4.6182e-01,  2.7839e-01,\n","         -9.2567e-01, -1.9405e-01, -2.5779e-01,  6.1887e-01],\n","        [-3.0452e-01,  9.1392e-01, -6.2336e-02, -3.3630e-01, -2.3752e-02,\n","         -3.7700e-01,  9.3044e-02,  1.0893e-01,  6.5210e-02],\n","        [-7.5205e-02, -2.3133e-01,  2.7185e-01,  9.7281e-01, -4.6972e-01,\n","         -5.0578e-02, -8.6765e-01,  7.3419e-01, -1.3797e-01],\n","        [-4.7568e-01,  8.0747e-01, -5.1934e-01,  5.7392e-01, -7.9303e-01,\n","          5.6835e-01, -1.1733e-01,  2.8998e-01, -2.0735e-01],\n","        [ 4.9955e-01, -2.0730e-01,  2.3362e-01, -4.4128e-01, -2.5429e-02,\n","         -1.6789e-01, -3.0973e-01,  9.9877e-01, -3.4186e-01],\n","        [ 1.0086e-03,  6.1314e-04,  1.0489e-05,  1.8206e-03, -5.3965e-04,\n","         -1.3407e-03,  8.1343e-04, -4.6414e-04, -2.1588e-03],\n","        [-2.2810e-01, -1.5286e-01, -1.5829e-01, -1.3835e-01, -2.7607e-01,\n","         -1.8605e-01, -1.7236e-01, -2.1188e-01, -2.0193e-01]])\n","first_linear.bias tensor([-3.4005e-04,  5.2660e-01, -8.7422e-01,  8.3211e-01, -5.8579e-01,\n","        -9.2561e-01,  9.2184e-01,  7.9292e-01, -2.6779e-03, -9.8325e-01])\n","linear_hidden.0.weight tensor([[ 1.5757e-04, -1.8606e-01,  3.2418e-01,  3.4791e-01, -3.0566e-01,\n","         -3.9690e-01,  3.8668e-01,  3.4906e-01,  9.5465e-04,  4.6052e-01],\n","        [-1.2920e-04,  1.8922e-01, -3.2988e-01, -3.5331e-01,  3.1025e-01,\n","          4.0340e-01, -3.9362e-01, -3.5536e-01, -9.6819e-04, -4.7072e-01],\n","        [ 4.4320e-04,  2.4043e-01, -4.0868e-01, -4.2571e-01,  3.7330e-01,\n","          4.8896e-01, -4.8034e-01, -4.4153e-01, -1.0109e-03, -6.3123e-01],\n","        [-4.4344e-04, -2.3817e-01,  4.0567e-01,  4.2381e-01, -3.7170e-01,\n","         -4.8723e-01,  4.7864e-01,  4.3983e-01,  1.0398e-03,  6.2578e-01],\n","        [-4.3591e-05, -1.0375e-04,  2.4264e-04, -2.5529e-04,  1.7265e-04,\n","          7.2733e-05, -6.3916e-05,  5.2706e-05,  6.5236e-05,  1.7748e-04],\n","        [ 1.0489e-03, -1.3915e-01,  2.5041e-01,  2.7655e-01, -2.4553e-01,\n","         -3.1417e-01,  3.0162e-01,  2.6631e-01,  8.4268e-04,  3.2590e-01],\n","        [-9.6900e-05, -2.1690e-01,  3.7278e-01,  3.9653e-01, -3.4668e-01,\n","         -4.5481e-01,  4.4407e-01,  4.0658e-01,  8.8445e-04,  5.5226e-01],\n","        [-4.3269e-04, -2.4321e-01,  4.1346e-01,  4.2854e-01, -3.7643e-01,\n","         -4.9289e-01,  4.8424e-01,  4.4528e-01,  1.0756e-03,  6.4066e-01],\n","        [ 6.4100e-04, -2.3229e-01,  3.8820e-01,  4.0406e-01, -3.5975e-01,\n","         -4.5649e-01,  4.4708e-01,  3.4421e-01,  1.7379e-03,  5.9649e-01],\n","        [-3.2429e-04,  1.7728e-01, -3.1094e-01, -3.4055e-01,  2.9879e-01,\n","          3.8889e-01, -3.7722e-01, -3.4021e-01, -1.0758e-03, -4.3011e-01]])\n","linear_hidden.0.bias tensor([-3.5703e-01,  3.6124e-01,  3.9249e-01, -3.9482e-01, -1.0190e-05,\n","        -3.1055e-01, -3.9251e-01, -3.9130e-01, -3.1938e-01,  3.6862e-01])\n","linear_output.weight tensor([[ 1.0291e+00, -1.0449e+00, -1.3076e+00,  1.2992e+00,  1.4396e-05,\n","          7.7024e-01,  1.2004e+00,  1.3187e+00,  1.1763e+00, -9.9705e-01]])\n","linear_output.bias tensor([-0.3521])\n","Testing out: \n","batch_size:  1386\n","train_size:  848\n","n_epochs:  199\n","lr:  0.0023476101187024146\n","weight_decay:  0.0005866835306383562\n","betas0:  0.9999859396644383\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1129, -0.1657, -0.2601],\n","          [-0.0104, -0.1859, -0.3067],\n","          [ 0.1195, -0.0881, -0.2257]]]])\n","conv1.bias tensor([-0.0588])\n","first_linear.weight tensor([[-0.2064,  0.0338, -0.0223, -0.1390,  0.2452,  0.2842,  0.3043,  0.2642,\n","          0.0963],\n","        [ 0.3220, -0.2184,  0.1267, -0.1094,  0.3327, -0.2494,  0.2805, -0.2840,\n","         -0.2223],\n","        [-0.0294,  0.1198, -0.1422, -0.1288, -0.1967, -0.1893,  0.1444,  0.1764,\n","          0.0935],\n","        [ 0.3048, -0.2335, -0.2189, -0.2184, -0.2339, -0.2467, -0.1271,  0.0676,\n","         -0.1775],\n","        [ 0.1656,  0.0644,  0.0518, -0.1096, -0.0181,  0.0733, -0.2979,  0.0166,\n","          0.1108],\n","        [ 0.0548,  0.3050,  0.2813,  0.0524, -0.0127, -0.2861, -0.3110, -0.1364,\n","          0.1179],\n","        [-0.1471,  0.1472, -0.1786,  0.0796, -0.0972,  0.2632,  0.1817,  0.2313,\n","         -0.0562],\n","        [ 0.1667,  0.3174, -0.2271,  0.2278,  0.0424, -0.1481,  0.0680, -0.0701,\n","          0.1285],\n","        [-0.2732, -0.0292, -0.2981, -0.1487, -0.1585, -0.1328, -0.1067,  0.2703,\n","          0.3036],\n","        [-0.3286,  0.0933, -0.1535,  0.1108, -0.0556,  0.2625, -0.0720, -0.2442,\n","          0.2760]])\n","first_linear.bias tensor([ 0.3223,  0.0546, -0.1708, -0.3101,  0.2845,  0.1317, -0.2642,  0.0402,\n","        -0.0351,  0.2322])\n","linear_hidden.0.weight tensor([[-0.2404, -0.1890, -0.0437, -0.1847,  0.1299,  0.2431, -0.0856, -0.0785,\n","         -0.2532, -0.1828],\n","        [ 0.1962, -0.1145,  0.1738, -0.1961,  0.2612,  0.0187,  0.1584, -0.1859,\n","         -0.1976,  0.0199],\n","        [ 0.2690, -0.0429,  0.1234, -0.2562,  0.2564, -0.0674,  0.0698,  0.0293,\n","         -0.1143, -0.0105],\n","        [ 0.1916,  0.0299,  0.0458,  0.0436, -0.1074,  0.1117,  0.2588,  0.1330,\n","         -0.2102, -0.0367],\n","        [-0.2588,  0.1231,  0.0393,  0.0031,  0.1774, -0.1284,  0.2912, -0.0290,\n","          0.2612, -0.0763],\n","        [-0.1323, -0.2425,  0.1914, -0.0389,  0.1051, -0.1571, -0.1450, -0.3122,\n","         -0.1004, -0.1646],\n","        [ 0.1719, -0.2591, -0.2064,  0.0734,  0.0406,  0.0514,  0.1329,  0.1118,\n","         -0.1745,  0.0941],\n","        [-0.0413,  0.1873, -0.0504, -0.2589,  0.0272, -0.1818,  0.0803,  0.1489,\n","          0.1112, -0.1497],\n","        [-0.0872,  0.2683,  0.1436, -0.2890,  0.0466,  0.1985, -0.2271, -0.0137,\n","         -0.2625, -0.0822],\n","        [ 0.1739, -0.3055,  0.1825, -0.2432, -0.1788,  0.1514,  0.2303,  0.0395,\n","          0.2870,  0.1529]])\n","linear_hidden.0.bias tensor([ 0.0194, -0.1644,  0.1295, -0.0376,  0.2168, -0.0119,  0.2247, -0.1439,\n","         0.0732, -0.1265])\n","linear_output.weight tensor([[ 0.0720, -0.0013,  0.3040,  0.2865, -0.2678,  0.1185, -0.0531, -0.0038,\n","         -0.2080, -0.0114]])\n","linear_output.bias tensor([0.3038])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([848])) that is different to the input size (torch.Size([848, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.7016726732254028 \t Validate_Accuracy: 0.497\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.7004754543304443 \t Validate_Accuracy: 0.497\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6993572115898132 \t Validate_Accuracy: 0.497\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6983165144920349 \t Validate_Accuracy: 0.497\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6973515152931213 \t Validate_Accuracy: 0.497\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.696456789970398 \t Validate_Accuracy: 0.497\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6956295967102051 \t Validate_Accuracy: 0.497\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6948613524436951 \t Validate_Accuracy: 0.497\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6941515803337097 \t Validate_Accuracy: 0.497\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6934939622879028 \t Validate_Accuracy: 0.497\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6928831934928894 \t Validate_Accuracy: 0.497\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6923156380653381 \t Validate_Accuracy: 0.497\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6917862892150879 \t Validate_Accuracy: 0.497\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.691289484500885 \t Validate_Accuracy: 0.497\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6908188462257385 \t Validate_Accuracy: 0.497\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.690370500087738 \t Validate_Accuracy: 0.498\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6899380683898926 \t Validate_Accuracy: 0.4985\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6895162463188171 \t Validate_Accuracy: 0.5045\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6891000866889954 \t Validate_Accuracy: 0.5065\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.688683807849884 \t Validate_Accuracy: 0.513\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6882616877555847 \t Validate_Accuracy: 0.5155\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6878301501274109 \t Validate_Accuracy: 0.4955\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6873827576637268 \t Validate_Accuracy: 0.456\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6869163513183594 \t Validate_Accuracy: 0.4415\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6864256262779236 \t Validate_Accuracy: 0.4345\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6859068274497986 \t Validate_Accuracy: 0.4335\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6853553652763367 \t Validate_Accuracy: 0.4365\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6847665905952454 \t Validate_Accuracy: 0.44\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6841367483139038 \t Validate_Accuracy: 0.4505\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6834620237350464 \t Validate_Accuracy: 0.4585\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6827363967895508 \t Validate_Accuracy: 0.466\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6819561123847961 \t Validate_Accuracy: 0.4715\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6811152100563049 \t Validate_Accuracy: 0.479\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6802096366882324 \t Validate_Accuracy: 0.485\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6792349815368652 \t Validate_Accuracy: 0.4875\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.67818683385849 \t Validate_Accuracy: 0.4925\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6770616769790649 \t Validate_Accuracy: 0.497\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.6758561134338379 \t Validate_Accuracy: 0.504\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6745691895484924 \t Validate_Accuracy: 0.5125\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.673198401927948 \t Validate_Accuracy: 0.5205\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6717424392700195 \t Validate_Accuracy: 0.526\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.670200765132904 \t Validate_Accuracy: 0.533\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6685728430747986 \t Validate_Accuracy: 0.535\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6668593883514404 \t Validate_Accuracy: 0.535\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6650623679161072 \t Validate_Accuracy: 0.5375\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6631810665130615 \t Validate_Accuracy: 0.5425\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6612195372581482 \t Validate_Accuracy: 0.5505\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6591789126396179 \t Validate_Accuracy: 0.5615\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.6570611000061035 \t Validate_Accuracy: 0.601\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6548696756362915 \t Validate_Accuracy: 0.6335\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6526074409484863 \t Validate_Accuracy: 0.6505\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.650275468826294 \t Validate_Accuracy: 0.6695\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6478767395019531 \t Validate_Accuracy: 0.6815\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6454140543937683 \t Validate_Accuracy: 0.694\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6428908109664917 \t Validate_Accuracy: 0.7035\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.6403086185455322 \t Validate_Accuracy: 0.711\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.637670636177063 \t Validate_Accuracy: 0.7185\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6349795460700989 \t Validate_Accuracy: 0.7265\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.632239580154419 \t Validate_Accuracy: 0.7255\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6294541954994202 \t Validate_Accuracy: 0.7295\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.6266273856163025 \t Validate_Accuracy: 0.7355\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6237620115280151 \t Validate_Accuracy: 0.739\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6208638548851013 \t Validate_Accuracy: 0.7435\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6179339289665222 \t Validate_Accuracy: 0.7445\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.6149786710739136 \t Validate_Accuracy: 0.7485\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.6120012402534485 \t Validate_Accuracy: 0.7545\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6090049147605896 \t Validate_Accuracy: 0.757\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6059936881065369 \t Validate_Accuracy: 0.759\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6029695272445679 \t Validate_Accuracy: 0.7625\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.5999375581741333 \t Validate_Accuracy: 0.7655\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.5969006419181824 \t Validate_Accuracy: 0.7685\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.5938618779182434 \t Validate_Accuracy: 0.7705\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.590824544429779 \t Validate_Accuracy: 0.775\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.5877927541732788 \t Validate_Accuracy: 0.7765\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.5847682356834412 \t Validate_Accuracy: 0.7795\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.5817549228668213 \t Validate_Accuracy: 0.7835\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.5787551403045654 \t Validate_Accuracy: 0.784\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.5757721662521362 \t Validate_Accuracy: 0.7855\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.572808563709259 \t Validate_Accuracy: 0.7895\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.5698654055595398 \t Validate_Accuracy: 0.7935\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.5669465065002441 \t Validate_Accuracy: 0.794\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.5640526413917542 \t Validate_Accuracy: 0.7965\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.5611864328384399 \t Validate_Accuracy: 0.799\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.5583486557006836 \t Validate_Accuracy: 0.8\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.5555406212806702 \t Validate_Accuracy: 0.8\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.552764892578125 \t Validate_Accuracy: 0.8005\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.5500205159187317 \t Validate_Accuracy: 0.803\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.5473096370697021 \t Validate_Accuracy: 0.803\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.5446321368217468 \t Validate_Accuracy: 0.8045\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.5419890284538269 \t Validate_Accuracy: 0.806\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.5393813848495483 \t Validate_Accuracy: 0.8075\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.5368086099624634 \t Validate_Accuracy: 0.808\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.5342718362808228 \t Validate_Accuracy: 0.811\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.5317714810371399 \t Validate_Accuracy: 0.811\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.5293062925338745 \t Validate_Accuracy: 0.8115\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.5268786549568176 \t Validate_Accuracy: 0.8125\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.5244877338409424 \t Validate_Accuracy: 0.812\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.522133469581604 \t Validate_Accuracy: 0.8125\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.5198161005973816 \t Validate_Accuracy: 0.8145\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.5175358653068542 \t Validate_Accuracy: 0.8135\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.5152923464775085 \t Validate_Accuracy: 0.8145\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.5130850672721863 \t Validate_Accuracy: 0.8135\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.5109155774116516 \t Validate_Accuracy: 0.813\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.508781373500824 \t Validate_Accuracy: 0.8115\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.5066838264465332 \t Validate_Accuracy: 0.8125\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.5046217441558838 \t Validate_Accuracy: 0.8115\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.5025948286056519 \t Validate_Accuracy: 0.811\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.5006020665168762 \t Validate_Accuracy: 0.813\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.4986438453197479 \t Validate_Accuracy: 0.8135\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.49671873450279236 \t Validate_Accuracy: 0.8125\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.49482738971710205 \t Validate_Accuracy: 0.813\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.4929679334163666 \t Validate_Accuracy: 0.8125\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.4911397099494934 \t Validate_Accuracy: 0.812\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.4893425405025482 \t Validate_Accuracy: 0.812\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.487574964761734 \t Validate_Accuracy: 0.811\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.4858364462852478 \t Validate_Accuracy: 0.81\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.48412561416625977 \t Validate_Accuracy: 0.8105\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.48244136571884155 \t Validate_Accuracy: 0.811\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.48078346252441406 \t Validate_Accuracy: 0.81\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.4791497588157654 \t Validate_Accuracy: 0.81\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.4775395691394806 \t Validate_Accuracy: 0.8115\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.4759516417980194 \t Validate_Accuracy: 0.8115\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.4743848145008087 \t Validate_Accuracy: 0.811\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.4728376567363739 \t Validate_Accuracy: 0.8105\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.4713100492954254 \t Validate_Accuracy: 0.8095\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.4697993993759155 \t Validate_Accuracy: 0.8105\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.4683050215244293 \t Validate_Accuracy: 0.8105\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.46682649850845337 \t Validate_Accuracy: 0.8105\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.46536150574684143 \t Validate_Accuracy: 0.8105\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.4639100432395935 \t Validate_Accuracy: 0.8115\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.46247047185897827 \t Validate_Accuracy: 0.812\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.46104153990745544 \t Validate_Accuracy: 0.8115\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.459622859954834 \t Validate_Accuracy: 0.811\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.45821288228034973 \t Validate_Accuracy: 0.8105\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.4568121135234833 \t Validate_Accuracy: 0.81\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.45541882514953613 \t Validate_Accuracy: 0.8095\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.4540317952632904 \t Validate_Accuracy: 0.809\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.45265111327171326 \t Validate_Accuracy: 0.8085\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.45127537846565247 \t Validate_Accuracy: 0.8095\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.44990527629852295 \t Validate_Accuracy: 0.8095\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.44853806495666504 \t Validate_Accuracy: 0.81\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.4471744894981384 \t Validate_Accuracy: 0.8095\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.4458131492137909 \t Validate_Accuracy: 0.8095\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.44445371627807617 \t Validate_Accuracy: 0.8105\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.443094402551651 \t Validate_Accuracy: 0.81\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.44173550605773926 \t Validate_Accuracy: 0.81\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.4403756260871887 \t Validate_Accuracy: 0.81\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.43901410698890686 \t Validate_Accuracy: 0.81\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.4376503527164459 \t Validate_Accuracy: 0.81\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.43628400564193726 \t Validate_Accuracy: 0.8105\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.4349134564399719 \t Validate_Accuracy: 0.8095\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.4335388243198395 \t Validate_Accuracy: 0.809\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.43215981125831604 \t Validate_Accuracy: 0.81\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.4307759404182434 \t Validate_Accuracy: 0.81\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.42938685417175293 \t Validate_Accuracy: 0.81\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.4279923737049103 \t Validate_Accuracy: 0.8095\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.4265929162502289 \t Validate_Accuracy: 0.8105\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.425187349319458 \t Validate_Accuracy: 0.8115\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.4237767457962036 \t Validate_Accuracy: 0.812\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.4223611354827881 \t Validate_Accuracy: 0.811\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.4209402799606323 \t Validate_Accuracy: 0.811\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.41951513290405273 \t Validate_Accuracy: 0.811\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.4180845320224762 \t Validate_Accuracy: 0.8115\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.41665005683898926 \t Validate_Accuracy: 0.812\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.4152109622955322 \t Validate_Accuracy: 0.812\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.41376861929893494 \t Validate_Accuracy: 0.813\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.41232237219810486 \t Validate_Accuracy: 0.8135\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.4108724892139435 \t Validate_Accuracy: 0.8145\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.4094191789627075 \t Validate_Accuracy: 0.815\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.4079629182815552 \t Validate_Accuracy: 0.815\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.4065043330192566 \t Validate_Accuracy: 0.816\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.40504270792007446 \t Validate_Accuracy: 0.8175\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.4035784900188446 \t Validate_Accuracy: 0.819\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.40211203694343567 \t Validate_Accuracy: 0.82\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.4006426930427551 \t Validate_Accuracy: 0.821\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.39917150139808655 \t Validate_Accuracy: 0.8215\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.39769813418388367 \t Validate_Accuracy: 0.8195\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.3962230086326599 \t Validate_Accuracy: 0.82\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.3947460353374481 \t Validate_Accuracy: 0.8205\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.3932667374610901 \t Validate_Accuracy: 0.821\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.39178624749183655 \t Validate_Accuracy: 0.821\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.3903038203716278 \t Validate_Accuracy: 0.8215\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.38881996273994446 \t Validate_Accuracy: 0.821\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.3873352110385895 \t Validate_Accuracy: 0.8205\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.3858492076396942 \t Validate_Accuracy: 0.822\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.3843618929386139 \t Validate_Accuracy: 0.8225\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.3828747272491455 \t Validate_Accuracy: 0.822\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.38138705492019653 \t Validate_Accuracy: 0.822\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.37989896535873413 \t Validate_Accuracy: 0.8225\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.3784119486808777 \t Validate_Accuracy: 0.8235\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.37692567706108093 \t Validate_Accuracy: 0.823\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.3754405677318573 \t Validate_Accuracy: 0.8235\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.3739568293094635 \t Validate_Accuracy: 0.824\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.37247607111930847 \t Validate_Accuracy: 0.8235\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.37099719047546387 \t Validate_Accuracy: 0.824\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.3695211708545685 \t Validate_Accuracy: 0.8245\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.36804890632629395 \t Validate_Accuracy: 0.824\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.36658114194869995 \t Validate_Accuracy: 0.825\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.3651173412799835 \t Validate_Accuracy: 0.825\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.3749, -0.4965, -0.5416],\n","          [-0.3367,  0.0972, -0.1477],\n","          [-0.4161, -0.4120, -0.2573]]]])\n","conv1.bias tensor([-0.1343])\n","first_linear.weight tensor([[ 0.0491,  0.2221, -0.3874, -0.3166,  0.0153,  0.5504,  0.2409,  0.0314,\n","         -0.2413],\n","        [ 0.3897, -0.5549,  0.5611, -0.2975,  0.4911, -0.1867,  0.2029, -0.2947,\n","         -0.0015],\n","        [-0.3164, -0.4261, -0.3734, -0.2890, -0.4324, -0.1632, -0.0643, -0.1474,\n","         -0.1701],\n","        [ 0.0303, -0.1561, -0.1299, -0.2991, -0.6004, -0.1656, -0.3699, -0.2720,\n","         -0.0629],\n","        [ 0.0944, -0.0635,  0.1555, -0.4828, -0.4813, -0.1697, -0.4521, -0.4310,\n","         -0.3220],\n","        [ 0.1545,  0.9036,  0.5560,  0.4851, -0.0273, -0.5288, -0.5160,  0.2809,\n","          0.1996],\n","        [-0.1855,  0.1081, -0.1144,  0.0656,  0.3592,  0.1400,  0.3332,  0.6608,\n","          0.4941],\n","        [ 0.2504,  0.5844, -0.2755, -0.0311,  0.0731,  0.4373,  0.3349,  0.1792,\n","          0.2153],\n","        [-0.2519, -0.3923, -0.2351, -0.2496, -0.2610, -0.4094,  0.0519,  0.0949,\n","          0.1109],\n","        [ 0.0848,  0.0348,  0.2581, -0.0893,  0.2879,  0.5389,  0.3304,  0.1369,\n","          0.5095]])\n","first_linear.bias tensor([ 0.6664,  0.5716, -0.6216, -0.7071,  0.7251, -0.0730, -0.7045,  0.3691,\n","         0.3789,  0.6902])\n","linear_hidden.0.weight tensor([[ 0.1766,  0.4836,  0.4590,  0.2125, -0.2868,  0.3295,  0.2696, -0.0190,\n","         -0.2963, -0.4472],\n","        [-0.3955, -0.5962, -0.3611, -0.6951,  0.5663, -0.2794, -0.3276, -0.0514,\n","          0.2604,  0.4566],\n","        [ 0.4889,  0.2877,  0.4974,  0.0487, -0.0964,  0.2111,  0.3495,  0.0096,\n","         -0.2418, -0.3696],\n","        [ 0.2147,  0.4418,  0.3029,  0.2550, -0.4363,  0.5381,  0.6067,  0.3115,\n","         -0.4163, -0.0924],\n","        [-0.3061, -0.2669, -0.1891, -0.1983,  0.4973, -0.6262, -0.0960, -0.2955,\n","          0.5560, -0.1101],\n","        [-0.4087, -0.5687,  0.5409,  0.2873, -0.2337, -0.5126,  0.2001, -0.6846,\n","          0.0831, -0.5793],\n","        [ 0.4526,  0.1541, -0.5809, -0.2778,  0.2210,  0.4617, -0.1029,  0.4658,\n","         -0.4300,  0.4721],\n","        [-0.5684, -0.5274, -0.5487, -0.7195,  0.4350, -0.4227, -0.4034,  0.0155,\n","          0.3352,  0.3330],\n","        [ 0.1837,  0.4173, -0.2310, -0.6078,  0.3273,  0.5717, -0.3842,  0.3853,\n","         -0.5606,  0.3297],\n","        [-0.3893, -0.8228, -0.3243, -0.7112,  0.2630, -0.0573, -0.2549, -0.2157,\n","          0.4309,  0.3122]])\n","linear_hidden.0.bias tensor([ 0.4858, -0.6049,  0.4920, -0.2451,  0.4468, -0.3120,  0.5196, -0.5471,\n","         0.3415, -0.5847])\n","linear_output.weight tensor([[ 0.4824, -0.5717,  0.6294,  0.6573, -0.6897,  0.5160, -0.4035, -0.5877,\n","         -0.6026, -0.6033]])\n","linear_output.bias tensor([0.6518])\n","Testing out: \n","batch_size:  2562\n","train_size:  2797\n","n_epochs:  112\n","lr:  0.32795989723324626\n","weight_decay:  0.010066680522772462\n","betas0:  0.9928938920797175\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2508, -0.2653, -0.3319],\n","          [-0.0438,  0.0350, -0.2013],\n","          [-0.2050, -0.2908, -0.0065]]]])\n","conv1.bias tensor([0.2613])\n","first_linear.weight tensor([[ 0.2569,  0.2513, -0.2958, -0.0974, -0.0937, -0.0274, -0.2582, -0.3287,\n","          0.2060],\n","        [-0.3229, -0.3140,  0.0676,  0.0240,  0.0975, -0.1205,  0.0416, -0.0760,\n","          0.0426],\n","        [ 0.0645,  0.2502,  0.2344,  0.0162,  0.1665,  0.2291,  0.1098, -0.3016,\n","         -0.2298],\n","        [ 0.2148,  0.3299,  0.2535,  0.1771, -0.2894, -0.1175,  0.0802,  0.1722,\n","         -0.0903],\n","        [-0.2982, -0.0266, -0.1796,  0.3301,  0.1507, -0.2228, -0.1910,  0.2158,\n","          0.1619],\n","        [-0.2435, -0.2499,  0.2730, -0.2352,  0.1517,  0.2615,  0.2994, -0.2660,\n","         -0.0014],\n","        [-0.3082, -0.0572, -0.3131, -0.0888,  0.1566, -0.0947,  0.0017,  0.1440,\n","         -0.3042],\n","        [ 0.1990, -0.2693, -0.0645,  0.2798,  0.1013,  0.0128,  0.2712, -0.3268,\n","         -0.3181],\n","        [-0.2003,  0.1282, -0.0188,  0.1196,  0.2338, -0.3179, -0.2929, -0.2209,\n","         -0.2174],\n","        [-0.3273, -0.0366, -0.0464, -0.1387,  0.1348, -0.0476, -0.0366,  0.1614,\n","          0.3096]])\n","first_linear.bias tensor([ 0.2956, -0.1254,  0.0678, -0.3320,  0.2914,  0.3186, -0.2479, -0.0579,\n","        -0.0673,  0.2283])\n","linear_hidden.0.weight tensor([[-0.1723,  0.3146, -0.1054,  0.1044, -0.2613, -0.1394,  0.1477, -0.1038,\n","          0.1428,  0.1264],\n","        [-0.1972, -0.0477, -0.3006,  0.2763, -0.0639, -0.1962, -0.0255, -0.2864,\n","         -0.2521, -0.1270],\n","        [-0.0203,  0.0895, -0.3025,  0.2159, -0.1884,  0.0242,  0.2822, -0.1152,\n","          0.0118, -0.0757],\n","        [-0.2856, -0.1152, -0.2189,  0.1807,  0.2273, -0.1760, -0.0300, -0.0293,\n","         -0.1375,  0.0623],\n","        [-0.2426, -0.1681, -0.2680,  0.2208, -0.1402,  0.2999,  0.2140,  0.1110,\n","         -0.1235,  0.2241],\n","        [-0.1640, -0.1390,  0.2279,  0.1543, -0.3152,  0.1232,  0.2643,  0.2278,\n","         -0.3135,  0.1982],\n","        [-0.0636,  0.0406, -0.1541,  0.0955,  0.2560, -0.3064, -0.0529,  0.2957,\n","         -0.2076, -0.1004],\n","        [ 0.2819,  0.2100,  0.1590,  0.0521,  0.2535,  0.0369,  0.1209, -0.1994,\n","         -0.1870, -0.2579],\n","        [ 0.0384, -0.1664, -0.0675, -0.0845,  0.2380, -0.1576, -0.1371,  0.1087,\n","         -0.2631, -0.0605],\n","        [ 0.0641,  0.1900,  0.1428, -0.1113,  0.1381, -0.2074, -0.1637,  0.2973,\n","         -0.0786, -0.1506]])\n","linear_hidden.0.bias tensor([-0.2998, -0.1449, -0.1823, -0.0239, -0.0105, -0.2376, -0.2011, -0.0794,\n","        -0.1762, -0.0706])\n","linear_output.weight tensor([[ 0.2969,  0.1552,  0.2614, -0.1902,  0.1419,  0.1245, -0.2144,  0.2643,\n","          0.0051, -0.2686]])\n","linear_output.bias tensor([-0.1468])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2562])) that is different to the input size (torch.Size([2562, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([235])) that is different to the input size (torch.Size([235, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.761236846446991 \t Validate_Accuracy: 0.4745\n","epoch 2\n","Epoch: 2 \t Train Loss: 1.0149559676647186 \t Validate_Accuracy: 0.593\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7085904777050018 \t Validate_Accuracy: 0.417\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.7387906610965729 \t Validate_Accuracy: 0.6165\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6341555714607239 \t Validate_Accuracy: 0.663\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6019546985626221 \t Validate_Accuracy: 0.725\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.5720411837100983 \t Validate_Accuracy: 0.655\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5556181371212006 \t Validate_Accuracy: 0.7725\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5435733795166016 \t Validate_Accuracy: 0.794\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.49341633915901184 \t Validate_Accuracy: 0.7805\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.49670206010341644 \t Validate_Accuracy: 0.811\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4873667359352112 \t Validate_Accuracy: 0.807\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.530965581536293 \t Validate_Accuracy: 0.7215\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.528795063495636 \t Validate_Accuracy: 0.797\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.4582911282777786 \t Validate_Accuracy: 0.7725\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.48593825101852417 \t Validate_Accuracy: 0.816\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.4861203730106354 \t Validate_Accuracy: 0.7905\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.4848870486021042 \t Validate_Accuracy: 0.805\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.4865231066942215 \t Validate_Accuracy: 0.799\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.4926593452692032 \t Validate_Accuracy: 0.7815\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.46742454171180725 \t Validate_Accuracy: 0.796\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.45727846026420593 \t Validate_Accuracy: 0.8085\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.4372577518224716 \t Validate_Accuracy: 0.8115\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.4753427505493164 \t Validate_Accuracy: 0.793\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.4421980232000351 \t Validate_Accuracy: 0.801\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.4579475671052933 \t Validate_Accuracy: 0.8205\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.46240319311618805 \t Validate_Accuracy: 0.799\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.4433693140745163 \t Validate_Accuracy: 0.818\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.4530719667673111 \t Validate_Accuracy: 0.793\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.444491446018219 \t Validate_Accuracy: 0.7905\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.42862896621227264 \t Validate_Accuracy: 0.788\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.49333830177783966 \t Validate_Accuracy: 0.8065\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.43401864171028137 \t Validate_Accuracy: 0.81\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.45708511769771576 \t Validate_Accuracy: 0.736\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.5043460726737976 \t Validate_Accuracy: 0.789\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.47013045847415924 \t Validate_Accuracy: 0.808\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.470723494887352 \t Validate_Accuracy: 0.7995\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.46604955196380615 \t Validate_Accuracy: 0.7965\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.45659463107585907 \t Validate_Accuracy: 0.7885\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.4831012040376663 \t Validate_Accuracy: 0.8065\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.4603385925292969 \t Validate_Accuracy: 0.81\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.4436122626066208 \t Validate_Accuracy: 0.7655\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.4837881624698639 \t Validate_Accuracy: 0.818\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.44540391862392426 \t Validate_Accuracy: 0.8065\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.4605897516012192 \t Validate_Accuracy: 0.8065\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.48850271105766296 \t Validate_Accuracy: 0.802\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.43257689476013184 \t Validate_Accuracy: 0.804\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.44853532314300537 \t Validate_Accuracy: 0.81\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.4296990633010864 \t Validate_Accuracy: 0.772\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.4556739926338196 \t Validate_Accuracy: 0.8105\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.4331185966730118 \t Validate_Accuracy: 0.7925\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.4509643465280533 \t Validate_Accuracy: 0.794\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.44908833503723145 \t Validate_Accuracy: 0.777\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.4742191433906555 \t Validate_Accuracy: 0.7465\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.46719929575920105 \t Validate_Accuracy: 0.805\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.4425229877233505 \t Validate_Accuracy: 0.816\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.44528551399707794 \t Validate_Accuracy: 0.799\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.45109669864177704 \t Validate_Accuracy: 0.7975\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.45908060669898987 \t Validate_Accuracy: 0.815\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.471130833029747 \t Validate_Accuracy: 0.817\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.4775130897760391 \t Validate_Accuracy: 0.8045\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.4501059949398041 \t Validate_Accuracy: 0.791\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.45038624107837677 \t Validate_Accuracy: 0.7965\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.46280813217163086 \t Validate_Accuracy: 0.788\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.4393175095319748 \t Validate_Accuracy: 0.8005\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.4597293585538864 \t Validate_Accuracy: 0.802\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.45999933779239655 \t Validate_Accuracy: 0.787\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.46914122998714447 \t Validate_Accuracy: 0.7905\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.4883866459131241 \t Validate_Accuracy: 0.809\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.4548215866088867 \t Validate_Accuracy: 0.821\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.4465341418981552 \t Validate_Accuracy: 0.8155\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.45305484533309937 \t Validate_Accuracy: 0.8005\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.4565492123365402 \t Validate_Accuracy: 0.8195\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.4684546887874603 \t Validate_Accuracy: 0.8175\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.4357451945543289 \t Validate_Accuracy: 0.808\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.44101032614707947 \t Validate_Accuracy: 0.821\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.4252903014421463 \t Validate_Accuracy: 0.779\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.44621624052524567 \t Validate_Accuracy: 0.796\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.4504406899213791 \t Validate_Accuracy: 0.7975\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.4297947883605957 \t Validate_Accuracy: 0.812\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.4451986849308014 \t Validate_Accuracy: 0.8005\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.4160139560699463 \t Validate_Accuracy: 0.7815\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.4560461491346359 \t Validate_Accuracy: 0.814\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.44823917746543884 \t Validate_Accuracy: 0.796\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.47105568647384644 \t Validate_Accuracy: 0.782\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.4575907737016678 \t Validate_Accuracy: 0.784\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.48024052381515503 \t Validate_Accuracy: 0.795\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.4557039439678192 \t Validate_Accuracy: 0.799\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.44627708196640015 \t Validate_Accuracy: 0.8005\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.45252525806427 \t Validate_Accuracy: 0.8045\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.4304243177175522 \t Validate_Accuracy: 0.801\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.4530383199453354 \t Validate_Accuracy: 0.811\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.4695899039506912 \t Validate_Accuracy: 0.7955\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.45728935301303864 \t Validate_Accuracy: 0.781\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.47348691523075104 \t Validate_Accuracy: 0.7725\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.45879651606082916 \t Validate_Accuracy: 0.808\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.4492209553718567 \t Validate_Accuracy: 0.8055\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.4681423455476761 \t Validate_Accuracy: 0.804\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.44814930856227875 \t Validate_Accuracy: 0.8065\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.45673491060733795 \t Validate_Accuracy: 0.7925\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.48306673765182495 \t Validate_Accuracy: 0.81\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.4287914037704468 \t Validate_Accuracy: 0.779\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.5263930559158325 \t Validate_Accuracy: 0.7545\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.47108176350593567 \t Validate_Accuracy: 0.8\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.5069589614868164 \t Validate_Accuracy: 0.8185\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.4519137144088745 \t Validate_Accuracy: 0.8\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.48543885350227356 \t Validate_Accuracy: 0.8075\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.47126324474811554 \t Validate_Accuracy: 0.796\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.4460199177265167 \t Validate_Accuracy: 0.803\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.4213518500328064 \t Validate_Accuracy: 0.808\n","epoch 111\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:45:38] ax.service.managed_loop: Running optimization trial 2...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 111 \t Train Loss: 0.4230744242668152 \t Validate_Accuracy: 0.818\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.427002415060997 \t Validate_Accuracy: 0.8215\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.4353,  0.2995,  0.1776],\n","          [ 0.4093, -0.0217,  0.4423],\n","          [ 0.3078,  0.1560,  0.4550]]]])\n","conv1.bias tensor([0.0535])\n","first_linear.weight tensor([[-0.2283,  0.0580, -0.0557, -0.1622,  0.0533, -0.0237, -0.2513, -0.0357,\n","          0.1293],\n","        [-0.0684, -0.1042,  0.0265, -0.0051, -0.1942, -0.1104, -0.0928, -0.1016,\n","         -0.3626],\n","        [ 0.1035,  0.0494, -0.0499, -0.1441,  0.0489,  0.0742,  0.0325,  0.1293,\n","          0.3516],\n","        [-0.0297, -0.1061, -0.3063,  0.1392, -0.2816, -0.5844,  0.1403,  0.0107,\n","         -0.3679],\n","        [ 0.1194,  0.2122, -0.0599,  0.0033,  0.1717,  0.1041, -0.0054, -0.0755,\n","          0.2286],\n","        [-0.3472, -0.1230,  0.1820, -0.6748, -0.3763, -0.0243, -0.4404, -0.1573,\n","         -0.1046],\n","        [ 0.2990,  0.3735,  0.1273,  0.3150,  0.4812,  0.3689,  0.2348,  0.2093,\n","          0.2513],\n","        [-0.1431, -0.2223, -0.1343, -0.2119, -0.4133, -0.3119, -0.0278, -0.3389,\n","         -0.3609],\n","        [ 0.1723,  0.1596,  0.1389,  0.1043,  0.1475,  0.0149,  0.2211,  0.1815,\n","          0.0922],\n","        [-0.2197, -0.1624, -0.2304, -0.0893, -0.1821, -0.2246, -0.1948, -0.2172,\n","         -0.2897]])\n","first_linear.bias tensor([-0.2809,  0.0790,  0.3085,  0.9572, -0.2572,  0.9900,  1.4222,  0.6902,\n","        -0.0815,  0.1313])\n","linear_hidden.0.weight tensor([[-5.7528e-03,  2.0581e-03, -8.3552e-03, -1.5355e-02, -1.1359e-02,\n","         -1.6626e-02, -2.8880e-03,  8.1633e-03,  1.5551e-03, -3.7711e-03],\n","        [ 4.9937e-03, -1.4087e-02,  2.1189e-02,  4.4684e-03, -8.1398e-04,\n","          2.7635e-04,  1.4583e-02, -7.6537e-03, -4.8112e-04, -2.5186e-03],\n","        [-1.0256e-03,  4.7050e-03, -7.1647e-03, -1.9253e-03, -4.3350e-04,\n","         -1.0197e-03, -6.0217e-03,  2.9116e-03, -4.4547e-04,  1.1938e-03],\n","        [-3.1162e-04,  1.5143e-03, -2.8429e-03,  3.0707e-04, -6.4712e-04,\n","         -1.9990e-03, -1.9501e-03, -4.8053e-04,  1.7590e-04,  2.3440e-04],\n","        [-9.3682e-03,  4.4716e-02,  4.7937e-03,  1.8031e-01,  9.2082e-02,\n","          2.3481e-01,  4.1877e-01,  4.6885e-02, -7.5361e-02,  7.8005e-02],\n","        [ 7.4440e-02, -8.2793e-02,  7.7532e-02,  3.4144e-01,  2.0083e-01,\n","          4.8227e-01,  9.0994e-01,  3.2057e-02, -1.5420e-01,  1.2013e-01],\n","        [-1.1904e-03, -7.0932e-05,  6.2846e-04, -2.6824e-03, -1.5888e-03,\n","         -3.2675e-03, -3.3116e-03,  1.9892e-03, -1.1631e-03,  9.4124e-04],\n","        [-4.4718e-03, -1.1217e-02,  1.3390e-02, -2.0058e-02, -2.0242e-02,\n","         -2.1268e-02,  1.6455e-03,  2.9489e-03,  2.9670e-03, -2.1211e-03],\n","        [-7.3050e-03, -1.3890e-02, -3.1204e-02, -2.3304e-01, -1.1049e-01,\n","         -3.0918e-01, -5.9319e-01, -3.5744e-02,  9.0256e-02, -7.9612e-02],\n","        [-1.0303e-02, -1.3165e-01,  8.8424e-02,  6.2321e-01,  3.8804e-02,\n","          7.3593e-01,  1.4486e+00,  3.2361e-01, -8.6895e-02,  7.2807e-02]])\n","linear_hidden.0.bias tensor([-7.6551e-04,  7.6265e-03, -2.0039e-03,  1.0490e-03, -2.2835e-01,\n","        -6.5213e-01, -3.8536e-05, -4.9500e-03,  3.4735e-01, -1.1903e+00])\n","linear_output.weight tensor([[-3.3264e-03, -7.0866e-03,  2.6243e-03, -8.4129e-05, -3.8693e-01,\n","         -8.8489e-01, -1.2430e-03,  2.4390e-03,  6.0952e-01, -1.8069e+00]])\n","linear_output.bias tensor([0.6821])\n","Testing out: \n","batch_size:  4788\n","train_size:  1496\n","n_epochs:  73\n","lr:  0.00044389453575161263\n","weight_decay:  0.11996077288605116\n","betas0:  0.9998139498961299\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1629, -0.1614, -0.0264],\n","          [-0.0704,  0.1528,  0.2459],\n","          [ 0.0967,  0.1307,  0.0278]]]])\n","conv1.bias tensor([0.0644])\n","first_linear.weight tensor([[ 0.0312,  0.2696,  0.2613,  0.1242,  0.1441, -0.3131,  0.0539, -0.3105,\n","         -0.0127],\n","        [ 0.2170, -0.1491,  0.3083,  0.0294,  0.0740, -0.2887, -0.1328,  0.0317,\n","         -0.2307],\n","        [-0.0980,  0.1238, -0.2039, -0.0379, -0.3136,  0.2308,  0.0095,  0.0861,\n","         -0.1753],\n","        [ 0.1339, -0.3069,  0.2654, -0.1721,  0.2715,  0.0749,  0.1421,  0.2673,\n","          0.3201],\n","        [-0.0545, -0.1176, -0.0133,  0.1159, -0.0307,  0.0838, -0.2308,  0.2718,\n","         -0.3173],\n","        [ 0.0198,  0.2943,  0.1006, -0.1489, -0.1309, -0.0220, -0.2839, -0.2020,\n","         -0.2258],\n","        [-0.0345,  0.1176,  0.2114, -0.2726,  0.0245,  0.2720, -0.2137, -0.3195,\n","          0.0681],\n","        [-0.2887,  0.1878,  0.1615, -0.1415,  0.1512,  0.2024,  0.1031, -0.1209,\n","         -0.2952],\n","        [-0.3201, -0.2335, -0.1062,  0.2827,  0.0277,  0.1239, -0.0120, -0.0231,\n","          0.0116],\n","        [ 0.1612,  0.2461,  0.2072, -0.2562, -0.3029, -0.1786, -0.2869,  0.1444,\n","         -0.3101]])\n","first_linear.bias tensor([ 0.2697, -0.0560, -0.2690,  0.2598, -0.2184,  0.1173, -0.0382,  0.2326,\n","        -0.1199,  0.1078])\n","linear_hidden.0.weight tensor([[-0.2769,  0.0370, -0.1880, -0.1438, -0.2293, -0.2032,  0.0431,  0.2339,\n","         -0.2593, -0.0930],\n","        [ 0.0280,  0.0546, -0.1412, -0.0090,  0.2866,  0.1635,  0.0039,  0.1763,\n","         -0.1191,  0.3055],\n","        [-0.1126,  0.0596,  0.2513,  0.2966,  0.0746, -0.0377, -0.2129, -0.0007,\n","          0.1474,  0.1911],\n","        [ 0.0422, -0.1001, -0.0397, -0.1156, -0.1689,  0.0339, -0.0097, -0.0727,\n","          0.1727, -0.1988],\n","        [-0.1066, -0.1831,  0.1142,  0.1015, -0.0566, -0.1544,  0.1429,  0.2111,\n","          0.0827, -0.1289],\n","        [-0.2570, -0.1526, -0.0830,  0.1645, -0.2702,  0.0584, -0.3049,  0.2199,\n","         -0.0066, -0.1290],\n","        [ 0.0813, -0.0845, -0.0674, -0.1178, -0.0865, -0.0698,  0.0995,  0.1510,\n","         -0.1454, -0.1631],\n","        [-0.0165, -0.2270, -0.2492,  0.0948, -0.1365,  0.2299,  0.2755, -0.1967,\n","          0.2863, -0.0257],\n","        [ 0.2039, -0.2634,  0.0895, -0.2070,  0.1902, -0.3152, -0.0880, -0.0656,\n","          0.2655,  0.0751],\n","        [-0.1707, -0.2698, -0.2704, -0.0558,  0.0317, -0.2772,  0.0856, -0.0571,\n","          0.0692, -0.0513]])\n","linear_hidden.0.bias tensor([ 0.2111,  0.1054,  0.1499,  0.2076, -0.0863,  0.1870, -0.3064, -0.0210,\n","        -0.2215,  0.1293])\n","linear_output.weight tensor([[-0.0769, -0.2186, -0.2673, -0.0619,  0.1166,  0.3092,  0.1543, -0.0671,\n","          0.2858, -0.0573]])\n","linear_output.bias tensor([0.0176])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1496])) that is different to the input size (torch.Size([1496, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6964678168296814 \t Validate_Accuracy: 0.48\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.696405827999115 \t Validate_Accuracy: 0.4805\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6963456869125366 \t Validate_Accuracy: 0.4795\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6962863802909851 \t Validate_Accuracy: 0.479\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6962275505065918 \t Validate_Accuracy: 0.4795\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6961707472801208 \t Validate_Accuracy: 0.479\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6961139440536499 \t Validate_Accuracy: 0.479\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6960570216178894 \t Validate_Accuracy: 0.4775\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6960023045539856 \t Validate_Accuracy: 0.4785\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6959474682807922 \t Validate_Accuracy: 0.478\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6958920955657959 \t Validate_Accuracy: 0.4785\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.695838987827301 \t Validate_Accuracy: 0.479\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6957864165306091 \t Validate_Accuracy: 0.4805\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6957342028617859 \t Validate_Accuracy: 0.4805\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6956825256347656 \t Validate_Accuracy: 0.4805\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6956321001052856 \t Validate_Accuracy: 0.4815\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6955809593200684 \t Validate_Accuracy: 0.482\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6955326795578003 \t Validate_Accuracy: 0.4825\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6954840421676636 \t Validate_Accuracy: 0.482\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6954359412193298 \t Validate_Accuracy: 0.482\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6953889727592468 \t Validate_Accuracy: 0.481\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6953423619270325 \t Validate_Accuracy: 0.4805\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6952977180480957 \t Validate_Accuracy: 0.4815\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6952527761459351 \t Validate_Accuracy: 0.481\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6952090263366699 \t Validate_Accuracy: 0.4795\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6951655149459839 \t Validate_Accuracy: 0.4795\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6951229572296143 \t Validate_Accuracy: 0.4795\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6950811147689819 \t Validate_Accuracy: 0.481\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6950410008430481 \t Validate_Accuracy: 0.481\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.695000171661377 \t Validate_Accuracy: 0.4815\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6949599981307983 \t Validate_Accuracy: 0.4815\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6949204802513123 \t Validate_Accuracy: 0.482\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6948823928833008 \t Validate_Accuracy: 0.4805\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6948447823524475 \t Validate_Accuracy: 0.48\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6948071122169495 \t Validate_Accuracy: 0.479\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6947702169418335 \t Validate_Accuracy: 0.4795\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6947342157363892 \t Validate_Accuracy: 0.4795\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.6946988701820374 \t Validate_Accuracy: 0.479\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6946643590927124 \t Validate_Accuracy: 0.4795\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.69463050365448 \t Validate_Accuracy: 0.479\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6945964694023132 \t Validate_Accuracy: 0.4795\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6945633292198181 \t Validate_Accuracy: 0.4795\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6945316195487976 \t Validate_Accuracy: 0.4795\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.694499135017395 \t Validate_Accuracy: 0.4785\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6944684386253357 \t Validate_Accuracy: 0.4785\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.69443678855896 \t Validate_Accuracy: 0.479\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6944067478179932 \t Validate_Accuracy: 0.4785\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6943768262863159 \t Validate_Accuracy: 0.4785\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.694348156452179 \t Validate_Accuracy: 0.478\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6943188309669495 \t Validate_Accuracy: 0.478\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6942916512489319 \t Validate_Accuracy: 0.4785\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.6942633986473083 \t Validate_Accuracy: 0.4785\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6942360401153564 \t Validate_Accuracy: 0.4795\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6942095160484314 \t Validate_Accuracy: 0.48\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6941824555397034 \t Validate_Accuracy: 0.48\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.6941573023796082 \t Validate_Accuracy: 0.4805\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.6941316723823547 \t Validate_Accuracy: 0.4795\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6941069960594177 \t Validate_Accuracy: 0.48\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.6940824389457703 \t Validate_Accuracy: 0.48\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6940584778785706 \t Validate_Accuracy: 0.48\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.69403475522995 \t Validate_Accuracy: 0.4805\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6940115690231323 \t Validate_Accuracy: 0.4795\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6939886808395386 \t Validate_Accuracy: 0.4785\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6939663290977478 \t Validate_Accuracy: 0.478\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.6939447522163391 \t Validate_Accuracy: 0.478\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.6939232349395752 \t Validate_Accuracy: 0.479\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6939018964767456 \t Validate_Accuracy: 0.4785\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6938815116882324 \t Validate_Accuracy: 0.4775\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6938613057136536 \t Validate_Accuracy: 0.478\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.6938410401344299 \t Validate_Accuracy: 0.4765\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.6938219666481018 \t Validate_Accuracy: 0.4775\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.6938019394874573 \t Validate_Accuracy: 0.478\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.6937835216522217 \t Validate_Accuracy: 0.4785\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1317, -0.1299, -0.0014],\n","          [-0.0405,  0.1217,  0.2143],\n","          [ 0.0663,  0.0999,  0.0008]]]])\n","conv1.bias tensor([0.0335])\n","first_linear.weight tensor([[ 6.2047e-03,  2.3779e-01,  2.2954e-01,  9.3142e-02,  1.1283e-01,\n","         -2.8122e-01,  2.5085e-02, -2.7860e-01,  1.6303e-03],\n","        [ 1.8540e-01, -1.1782e-01,  2.7649e-01,  5.0385e-03,  4.3896e-02,\n","         -2.5680e-01, -1.0178e-01,  6.3137e-03, -1.9896e-01],\n","        [-6.7482e-02,  9.2797e-02, -1.7232e-01, -1.0898e-02, -2.8171e-01,\n","          1.9912e-01,  8.1932e-04,  5.5722e-02, -1.4392e-01],\n","        [ 1.0280e-01, -2.7504e-01,  2.3363e-01, -1.4076e-01,  2.3970e-01,\n","          4.4699e-02,  1.1076e-01,  2.3548e-01,  2.8821e-01],\n","        [-2.5636e-02, -8.6612e-02,  2.6318e-04,  8.4888e-02, -6.4817e-03,\n","          5.3447e-02, -1.9911e-01,  2.3993e-01, -2.8543e-01],\n","        [-3.8642e-03,  2.6251e-01,  7.0051e-02, -1.1757e-01, -9.9779e-02,\n","         -4.3382e-03, -2.5203e-01, -1.7036e-01, -1.9411e-01],\n","        [-8.4818e-03,  8.6526e-02,  1.7972e-01, -2.4082e-01,  1.1563e-03,\n","          2.4013e-01, -1.8216e-01, -2.8764e-01,  3.8255e-02],\n","        [-2.5684e-01,  1.5627e-01,  1.3014e-01, -1.1023e-01,  1.1991e-01,\n","          1.7082e-01,  7.2456e-02, -8.9833e-02, -2.6338e-01],\n","        [-2.8824e-01, -2.0177e-01, -7.5442e-02,  2.5090e-01,  4.3685e-03,\n","          9.2903e-02, -2.0280e-03, -1.0662e-03,  1.0419e-03],\n","        [ 1.2996e-01,  2.1443e-01,  1.7574e-01, -2.2439e-01, -2.7099e-01,\n","         -1.4701e-01, -2.5508e-01,  1.1334e-01, -2.7818e-01]])\n","first_linear.bias tensor([ 0.2378, -0.0240, -0.2373,  0.2282, -0.1863,  0.0867, -0.0116,  0.2006,\n","        -0.0889,  0.0777])\n","linear_hidden.0.weight tensor([[-0.2451,  0.0102, -0.1566, -0.1124, -0.1977, -0.1716,  0.0153,  0.2023,\n","         -0.2276, -0.0624],\n","        [ 0.0027,  0.0256, -0.1103, -0.0151,  0.2546,  0.1322, -0.0033,  0.1450,\n","         -0.0884,  0.2737],\n","        [-0.0812,  0.0303,  0.2192,  0.2650,  0.0441, -0.0104, -0.1813, -0.0054,\n","          0.1159,  0.1598],\n","        [ 0.0148, -0.0695, -0.0126, -0.0846, -0.1376,  0.0080, -0.0008, -0.0427,\n","          0.1412, -0.1672],\n","        [-0.0761, -0.1516,  0.0835,  0.0706, -0.0273, -0.1231,  0.1117,  0.1793,\n","          0.0526, -0.0980],\n","        [-0.2254, -0.1213, -0.0521,  0.1329, -0.2382,  0.0286, -0.2730,  0.1880,\n","         -0.0048, -0.0983],\n","        [ 0.0507, -0.0542, -0.0373, -0.0871, -0.0559, -0.0402,  0.0689,  0.1196,\n","         -0.1139, -0.1319],\n","        [-0.0013, -0.1953, -0.2176,  0.0644, -0.1055,  0.1983,  0.2437, -0.1651,\n","          0.2544, -0.0022],\n","        [ 0.1720, -0.2316,  0.0595, -0.1756,  0.1589, -0.2834, -0.0575, -0.0364,\n","          0.2339,  0.0445],\n","        [-0.1392, -0.2380, -0.2387, -0.0266,  0.0061, -0.2453,  0.0553, -0.0279,\n","          0.0393, -0.0225]])\n","linear_hidden.0.bias tensor([ 0.1798,  0.0754,  0.1195,  0.1763, -0.0566,  0.1539, -0.2749, -0.0058,\n","        -0.1905,  0.0986])\n","linear_output.weight tensor([[-0.0477, -0.1878, -0.2357, -0.0329,  0.0865,  0.2764,  0.1237, -0.0373,\n","          0.2548, -0.0279]])\n","linear_output.bias tensor([0.0454])\n","Testing out: \n","batch_size:  956\n","train_size:  4447\n","n_epochs:  235\n","lr:  0.03143848122728416\n","weight_decay:  4.922518421883815e-05\n","betas0:  0.911594057468009\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1031,  0.3287, -0.1763],\n","          [-0.2671,  0.1494,  0.2706],\n","          [-0.2525,  0.2301,  0.2180]]]])\n","conv1.bias tensor([0.0266])\n","first_linear.weight tensor([[ 0.1670, -0.1034, -0.0069,  0.3207, -0.1473, -0.1588, -0.1893, -0.1355,\n","          0.1203],\n","        [-0.3159, -0.3065, -0.2746,  0.0622,  0.2135,  0.0527, -0.1980,  0.0995,\n","         -0.1046],\n","        [-0.0433, -0.1486,  0.0464, -0.2026,  0.0280, -0.3134,  0.2192,  0.2328,\n","         -0.1584],\n","        [ 0.2205, -0.2802,  0.2299,  0.2787, -0.0076, -0.0020,  0.1454, -0.0368,\n","         -0.1734],\n","        [ 0.1617,  0.1766,  0.2434, -0.1106,  0.0721, -0.1595, -0.3147,  0.2932,\n","         -0.2238],\n","        [-0.1369,  0.0522,  0.2017,  0.1266,  0.1476,  0.3262,  0.2705, -0.1582,\n","         -0.2363],\n","        [-0.3312, -0.3163,  0.1143, -0.0899, -0.0070,  0.0172,  0.1102,  0.2920,\n","          0.2051],\n","        [-0.1658,  0.1720, -0.0488,  0.2479,  0.1411, -0.0325,  0.2750,  0.1526,\n","         -0.0507],\n","        [-0.2474,  0.3209, -0.0043, -0.0078,  0.2557, -0.0802, -0.2133, -0.1428,\n","         -0.2824],\n","        [-0.0559, -0.3177,  0.1847,  0.1152,  0.1329, -0.0714, -0.0332,  0.0641,\n","          0.1438]])\n","first_linear.bias tensor([-0.1508, -0.1131, -0.0973, -0.0843, -0.0012, -0.0914,  0.3215, -0.2327,\n","         0.0407, -0.1549])\n","linear_hidden.0.weight tensor([[-0.1198, -0.1047,  0.0250,  0.0744,  0.2633, -0.2074, -0.0561, -0.2753,\n","         -0.0501,  0.3133],\n","        [ 0.2925,  0.1686,  0.0482,  0.2462,  0.2031, -0.2482,  0.2029,  0.1355,\n","          0.2895, -0.1479],\n","        [-0.2015,  0.0282,  0.2336,  0.1930,  0.0813, -0.3120, -0.1452, -0.2602,\n","         -0.1756,  0.2613],\n","        [-0.1935, -0.2202,  0.0838,  0.1905,  0.2702, -0.0193,  0.2539, -0.2567,\n","          0.1822,  0.0256],\n","        [ 0.1297,  0.2559, -0.2993, -0.2366, -0.1565, -0.0759, -0.0941,  0.1052,\n","          0.0195, -0.3025],\n","        [-0.1260,  0.1820, -0.0083,  0.0785, -0.1545, -0.3101, -0.2108, -0.0603,\n","          0.1428, -0.1344],\n","        [-0.0389,  0.2712,  0.2989,  0.1821,  0.1223,  0.2367, -0.0264, -0.1394,\n","          0.2254,  0.2746],\n","        [ 0.2085, -0.0511,  0.2055,  0.0332,  0.1360, -0.0353, -0.2695, -0.2160,\n","          0.2594, -0.0307],\n","        [ 0.0584,  0.1438, -0.0953,  0.2007,  0.0983,  0.1795, -0.1641,  0.2891,\n","          0.2553,  0.0655],\n","        [-0.2517, -0.0531,  0.2441,  0.2287, -0.1078,  0.0916, -0.0173,  0.0582,\n","          0.2067,  0.3155]])\n","linear_hidden.0.bias tensor([ 0.0842, -0.0949,  0.1321, -0.2571,  0.1014, -0.0269, -0.1060,  0.0180,\n","         0.0124,  0.2619])\n","linear_output.weight tensor([[-0.1351, -0.1487,  0.2089,  0.0076,  0.2991, -0.2179,  0.0630,  0.2260,\n","          0.2394, -0.1949]])\n","linear_output.bias tensor([0.1328])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.695639145374298 \t Validate_Accuracy: 0.4975\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([956])) that is different to the input size (torch.Size([956, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([623])) that is different to the input size (torch.Size([623, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6930514335632324 \t Validate_Accuracy: 0.524\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.681201946735382 \t Validate_Accuracy: 0.708\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6379942774772644 \t Validate_Accuracy: 0.803\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.5618987798690795 \t Validate_Accuracy: 0.797\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.4829409122467041 \t Validate_Accuracy: 0.7955\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.45164580941200255 \t Validate_Accuracy: 0.8045\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.4204069197177887 \t Validate_Accuracy: 0.807\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.39954766631126404 \t Validate_Accuracy: 0.819\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.37166479229927063 \t Validate_Accuracy: 0.831\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.3484026253223419 \t Validate_Accuracy: 0.8365\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.3400856971740723 \t Validate_Accuracy: 0.833\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.3273621737957001 \t Validate_Accuracy: 0.8515\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.32858832478523253 \t Validate_Accuracy: 0.85\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.32929043769836425 \t Validate_Accuracy: 0.8525\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3251114010810852 \t Validate_Accuracy: 0.852\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.32356507182121275 \t Validate_Accuracy: 0.8455\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3231805145740509 \t Validate_Accuracy: 0.856\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.31918694972991946 \t Validate_Accuracy: 0.856\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.32011234760284424 \t Validate_Accuracy: 0.8605\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3157101035118103 \t Validate_Accuracy: 0.8545\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.315982723236084 \t Validate_Accuracy: 0.8575\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.31322773098945617 \t Validate_Accuracy: 0.8555\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3092081606388092 \t Validate_Accuracy: 0.857\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3117851853370667 \t Validate_Accuracy: 0.859\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3103090047836304 \t Validate_Accuracy: 0.856\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.30472018718719485 \t Validate_Accuracy: 0.851\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3061935842037201 \t Validate_Accuracy: 0.854\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3034133195877075 \t Validate_Accuracy: 0.8555\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3055999934673309 \t Validate_Accuracy: 0.8595\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.3049750804901123 \t Validate_Accuracy: 0.8515\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.305435448884964 \t Validate_Accuracy: 0.842\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.29952908158302305 \t Validate_Accuracy: 0.852\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.29704893231391905 \t Validate_Accuracy: 0.856\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.293964159488678 \t Validate_Accuracy: 0.8625\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2923910140991211 \t Validate_Accuracy: 0.8635\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.2922301352024078 \t Validate_Accuracy: 0.8555\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.29398313164711 \t Validate_Accuracy: 0.863\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.29132176041603086 \t Validate_Accuracy: 0.859\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2905385494232178 \t Validate_Accuracy: 0.8595\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.29102537631988523 \t Validate_Accuracy: 0.8615\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2855121731758118 \t Validate_Accuracy: 0.8615\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2853411018848419 \t Validate_Accuracy: 0.863\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.285748291015625 \t Validate_Accuracy: 0.8565\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2926538288593292 \t Validate_Accuracy: 0.8575\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.28359709978103637 \t Validate_Accuracy: 0.8615\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2819121479988098 \t Validate_Accuracy: 0.8645\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.28002638816833497 \t Validate_Accuracy: 0.8635\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.28004204034805297 \t Validate_Accuracy: 0.8635\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.280866339802742 \t Validate_Accuracy: 0.861\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2760989904403687 \t Validate_Accuracy: 0.8545\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.27798120975494384 \t Validate_Accuracy: 0.8645\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2709150850772858 \t Validate_Accuracy: 0.869\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.27507035434246063 \t Validate_Accuracy: 0.8595\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.26648427844047545 \t Validate_Accuracy: 0.87\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.26243131756782534 \t Validate_Accuracy: 0.8725\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2587092936038971 \t Validate_Accuracy: 0.8685\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2609271019697189 \t Validate_Accuracy: 0.872\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.25658553242683413 \t Validate_Accuracy: 0.868\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2570894569158554 \t Validate_Accuracy: 0.8655\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2535022675991058 \t Validate_Accuracy: 0.8735\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.24957680106163024 \t Validate_Accuracy: 0.873\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.2533425599336624 \t Validate_Accuracy: 0.861\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2500670224428177 \t Validate_Accuracy: 0.8685\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2470252752304077 \t Validate_Accuracy: 0.869\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.24391618371009827 \t Validate_Accuracy: 0.869\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.245788636803627 \t Validate_Accuracy: 0.8735\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.23984580338001252 \t Validate_Accuracy: 0.868\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2503023982048035 \t Validate_Accuracy: 0.8765\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.245585697889328 \t Validate_Accuracy: 0.8745\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2510407119989395 \t Validate_Accuracy: 0.8805\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.24717269837856293 \t Validate_Accuracy: 0.866\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2452824145555496 \t Validate_Accuracy: 0.8785\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2404716581106186 \t Validate_Accuracy: 0.8695\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2395181506872177 \t Validate_Accuracy: 0.879\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2411629378795624 \t Validate_Accuracy: 0.877\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.23927867114543916 \t Validate_Accuracy: 0.875\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.24158293306827544 \t Validate_Accuracy: 0.8755\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.23788174390792846 \t Validate_Accuracy: 0.878\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2378039240837097 \t Validate_Accuracy: 0.8755\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.23709492683410643 \t Validate_Accuracy: 0.881\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.23536055386066437 \t Validate_Accuracy: 0.8745\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.23618020117282867 \t Validate_Accuracy: 0.8755\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.23645612299442292 \t Validate_Accuracy: 0.878\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2346417248249054 \t Validate_Accuracy: 0.875\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.23646046817302704 \t Validate_Accuracy: 0.878\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2356102555990219 \t Validate_Accuracy: 0.8695\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.24292992949485778 \t Validate_Accuracy: 0.877\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2364293932914734 \t Validate_Accuracy: 0.8795\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.23527941703796387 \t Validate_Accuracy: 0.876\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2417074918746948 \t Validate_Accuracy: 0.8705\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.24249785244464875 \t Validate_Accuracy: 0.8695\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.23639356791973115 \t Validate_Accuracy: 0.8745\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.24800700843334197 \t Validate_Accuracy: 0.8745\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.23462791740894318 \t Validate_Accuracy: 0.8745\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.23672500848770142 \t Validate_Accuracy: 0.878\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.23704108595848083 \t Validate_Accuracy: 0.8795\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.23768644630908967 \t Validate_Accuracy: 0.869\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.24153360426425935 \t Validate_Accuracy: 0.8795\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.23935311436653137 \t Validate_Accuracy: 0.8765\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.23633271157741548 \t Validate_Accuracy: 0.8685\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.235211580991745 \t Validate_Accuracy: 0.8785\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.23467469215393066 \t Validate_Accuracy: 0.8765\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.23429015874862671 \t Validate_Accuracy: 0.873\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2335006535053253 \t Validate_Accuracy: 0.8735\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2331191122531891 \t Validate_Accuracy: 0.877\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.23083972930908203 \t Validate_Accuracy: 0.881\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.23550003468990327 \t Validate_Accuracy: 0.871\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.23117267787456514 \t Validate_Accuracy: 0.881\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2372239500284195 \t Validate_Accuracy: 0.878\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.23613054752349855 \t Validate_Accuracy: 0.8705\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.23476459980010986 \t Validate_Accuracy: 0.8725\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.23506282269954681 \t Validate_Accuracy: 0.88\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2350629299879074 \t Validate_Accuracy: 0.88\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2354535162448883 \t Validate_Accuracy: 0.8685\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.2328568398952484 \t Validate_Accuracy: 0.8785\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.23536699116230012 \t Validate_Accuracy: 0.874\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.23333667814731598 \t Validate_Accuracy: 0.8745\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.2332081288099289 \t Validate_Accuracy: 0.875\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.23238953053951264 \t Validate_Accuracy: 0.879\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.2336825430393219 \t Validate_Accuracy: 0.8755\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.2334754854440689 \t Validate_Accuracy: 0.876\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.23127237856388091 \t Validate_Accuracy: 0.8705\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.23513264060020447 \t Validate_Accuracy: 0.879\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.23157006204128266 \t Validate_Accuracy: 0.8745\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.23357008695602416 \t Validate_Accuracy: 0.8715\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.23369181156158447 \t Validate_Accuracy: 0.8755\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.23225535750389098 \t Validate_Accuracy: 0.8805\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23215048611164094 \t Validate_Accuracy: 0.88\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.22657247483730317 \t Validate_Accuracy: 0.8715\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.2307622343301773 \t Validate_Accuracy: 0.8765\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.22824418544769287 \t Validate_Accuracy: 0.876\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.2314678341150284 \t Validate_Accuracy: 0.876\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.23507318794727325 \t Validate_Accuracy: 0.8675\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.2382501631975174 \t Validate_Accuracy: 0.875\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2332891643047333 \t Validate_Accuracy: 0.8745\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.2316813886165619 \t Validate_Accuracy: 0.867\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.23326036036014558 \t Validate_Accuracy: 0.872\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.22576602101325988 \t Validate_Accuracy: 0.8715\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.23110349774360656 \t Validate_Accuracy: 0.875\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.22868283987045288 \t Validate_Accuracy: 0.876\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.22445040941238403 \t Validate_Accuracy: 0.8735\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.22481526732444762 \t Validate_Accuracy: 0.874\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.22775455713272094 \t Validate_Accuracy: 0.8765\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.22479074597358703 \t Validate_Accuracy: 0.8705\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.2261481314897537 \t Validate_Accuracy: 0.872\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.2258044183254242 \t Validate_Accuracy: 0.8745\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.22437637448310851 \t Validate_Accuracy: 0.873\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.22699018120765685 \t Validate_Accuracy: 0.873\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.22536710500717164 \t Validate_Accuracy: 0.8755\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.22769058048725127 \t Validate_Accuracy: 0.875\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.22930788695812226 \t Validate_Accuracy: 0.8705\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.22710077166557313 \t Validate_Accuracy: 0.876\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.22528135478496553 \t Validate_Accuracy: 0.872\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.23080256283283235 \t Validate_Accuracy: 0.8735\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.22430904805660248 \t Validate_Accuracy: 0.877\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.22912387251853944 \t Validate_Accuracy: 0.8705\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.22716010808944703 \t Validate_Accuracy: 0.875\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.22613948583602905 \t Validate_Accuracy: 0.8765\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.22223060429096222 \t Validate_Accuracy: 0.8735\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2263023316860199 \t Validate_Accuracy: 0.869\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.22549436688423158 \t Validate_Accuracy: 0.874\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.22451250553131102 \t Validate_Accuracy: 0.8755\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.22275315523147582 \t Validate_Accuracy: 0.871\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.2241052895784378 \t Validate_Accuracy: 0.878\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.22379117608070373 \t Validate_Accuracy: 0.872\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.22588021755218507 \t Validate_Accuracy: 0.869\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.2230610281229019 \t Validate_Accuracy: 0.875\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.22641603350639344 \t Validate_Accuracy: 0.8785\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.22281344830989838 \t Validate_Accuracy: 0.875\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.2190062075853348 \t Validate_Accuracy: 0.8735\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.22173493206501008 \t Validate_Accuracy: 0.873\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.22338818609714509 \t Validate_Accuracy: 0.88\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.2226150870323181 \t Validate_Accuracy: 0.876\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.22405052781105042 \t Validate_Accuracy: 0.872\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2164467304944992 \t Validate_Accuracy: 0.875\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.21721308529376984 \t Validate_Accuracy: 0.878\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.21727019548416138 \t Validate_Accuracy: 0.8755\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.21580631136894227 \t Validate_Accuracy: 0.872\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.21902106702327728 \t Validate_Accuracy: 0.877\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.21896317303180696 \t Validate_Accuracy: 0.878\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.22137001156806946 \t Validate_Accuracy: 0.872\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.21928012371063232 \t Validate_Accuracy: 0.877\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.22428512573242188 \t Validate_Accuracy: 0.8795\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.22108237445354462 \t Validate_Accuracy: 0.8735\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.21941973268985748 \t Validate_Accuracy: 0.8735\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.22153085470199585 \t Validate_Accuracy: 0.8735\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.2206861585378647 \t Validate_Accuracy: 0.873\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.21837844252586364 \t Validate_Accuracy: 0.8765\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.21702731549739837 \t Validate_Accuracy: 0.8785\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.21905094981193543 \t Validate_Accuracy: 0.872\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.21948969662189483 \t Validate_Accuracy: 0.8785\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.2164336234331131 \t Validate_Accuracy: 0.877\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.2187485694885254 \t Validate_Accuracy: 0.873\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.21916366517543792 \t Validate_Accuracy: 0.88\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.21919774115085602 \t Validate_Accuracy: 0.874\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.2265807569026947 \t Validate_Accuracy: 0.8735\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.22162058353424072 \t Validate_Accuracy: 0.8745\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.22298904955387117 \t Validate_Accuracy: 0.8785\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.2209905594587326 \t Validate_Accuracy: 0.874\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.22010306119918824 \t Validate_Accuracy: 0.8715\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.21921741366386413 \t Validate_Accuracy: 0.882\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.2133331388235092 \t Validate_Accuracy: 0.8755\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.216925972700119 \t Validate_Accuracy: 0.8715\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.21680729687213898 \t Validate_Accuracy: 0.8725\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.21651884615421296 \t Validate_Accuracy: 0.874\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.21855376064777374 \t Validate_Accuracy: 0.874\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.21750518083572387 \t Validate_Accuracy: 0.874\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.21460946202278136 \t Validate_Accuracy: 0.877\n","epoch 210\n","Epoch: 210 \t Train Loss: 0.2177481472492218 \t Validate_Accuracy: 0.873\n","epoch 211\n","Epoch: 211 \t Train Loss: 0.2182399332523346 \t Validate_Accuracy: 0.874\n","epoch 212\n","Epoch: 212 \t Train Loss: 0.21912165582180024 \t Validate_Accuracy: 0.8805\n","epoch 213\n","Epoch: 213 \t Train Loss: 0.21470683813095093 \t Validate_Accuracy: 0.8765\n","epoch 214\n","Epoch: 214 \t Train Loss: 0.2160983830690384 \t Validate_Accuracy: 0.872\n","epoch 215\n","Epoch: 215 \t Train Loss: 0.2186306893825531 \t Validate_Accuracy: 0.8805\n","epoch 216\n","Epoch: 216 \t Train Loss: 0.21679109632968901 \t Validate_Accuracy: 0.8725\n","epoch 217\n","Epoch: 217 \t Train Loss: 0.21440865099430084 \t Validate_Accuracy: 0.876\n","epoch 218\n","Epoch: 218 \t Train Loss: 0.213301682472229 \t Validate_Accuracy: 0.879\n","epoch 219\n","Epoch: 219 \t Train Loss: 0.21503411531448363 \t Validate_Accuracy: 0.877\n","epoch 220\n","Epoch: 220 \t Train Loss: 0.21356890201568604 \t Validate_Accuracy: 0.878\n","epoch 221\n","Epoch: 221 \t Train Loss: 0.21338295042514802 \t Validate_Accuracy: 0.8735\n","epoch 222\n","Epoch: 222 \t Train Loss: 0.21281173825263977 \t Validate_Accuracy: 0.8735\n","epoch 223\n","Epoch: 223 \t Train Loss: 0.21403350234031676 \t Validate_Accuracy: 0.878\n","epoch 224\n","Epoch: 224 \t Train Loss: 0.2129360556602478 \t Validate_Accuracy: 0.878\n","epoch 225\n","Epoch: 225 \t Train Loss: 0.21323578357696532 \t Validate_Accuracy: 0.881\n","epoch 226\n","Epoch: 226 \t Train Loss: 0.2130951166152954 \t Validate_Accuracy: 0.8795\n","epoch 227\n","Epoch: 227 \t Train Loss: 0.21545718610286713 \t Validate_Accuracy: 0.876\n","epoch 228\n","Epoch: 228 \t Train Loss: 0.21146132051944733 \t Validate_Accuracy: 0.875\n","epoch 229\n","Epoch: 229 \t Train Loss: 0.21745982468128205 \t Validate_Accuracy: 0.8735\n","epoch 230\n","Epoch: 230 \t Train Loss: 0.21150831878185272 \t Validate_Accuracy: 0.877\n","epoch 231\n","Epoch: 231 \t Train Loss: 0.209170863032341 \t Validate_Accuracy: 0.876\n","epoch 232\n","Epoch: 232 \t Train Loss: 0.21251379251480101 \t Validate_Accuracy: 0.878\n","epoch 233\n","Epoch: 233 \t Train Loss: 0.21701689660549164 \t Validate_Accuracy: 0.8765\n","epoch 234\n","Epoch: 234 \t Train Loss: 0.21704504191875457 \t Validate_Accuracy: 0.8745\n","epoch 235\n","Epoch: 235 \t Train Loss: 0.2159275382757187 \t Validate_Accuracy: 0.874\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1450,  0.0400,  0.1752],\n","          [ 0.0638, -0.0143,  0.0591],\n","          [ 0.1525,  0.0567,  0.1665]]]])\n","conv1.bias tensor([0.0149])\n","first_linear.weight tensor([[-1.5719,  1.5559, -1.1806,  0.7524,  0.0834, -0.5054, -1.2473, -0.6306,\n","          0.0458],\n","        [-0.0598, -0.7645, -0.5593, -0.2875, -0.5062, -0.9359,  0.2637,  0.3761,\n","         -0.0240],\n","        [-0.8202, -0.1276,  0.4842,  0.0539,  1.1729, -1.4085,  0.2846, -1.1569,\n","          1.2001],\n","        [ 0.4634, -0.2248,  0.1237,  1.2358,  0.7637,  0.3418,  0.4321,  1.5760,\n","          0.2191],\n","        [-0.5852, -0.0272,  0.3789,  1.5703, -0.7936, -0.3162, -1.3358,  1.0012,\n","         -0.0422],\n","        [-0.7782,  1.2114, -0.9182, -0.0647, -0.6753,  1.1252,  0.4999, -0.2694,\n","         -0.0578],\n","        [-2.9781,  0.1110, -0.3147, -0.3992,  0.3236,  0.1275, -0.2049,  0.2483,\n","          2.5683],\n","        [ 0.6792,  0.1687,  0.1967,  0.1965,  0.5168,  0.1614,  0.0055,  0.1474,\n","          0.0163],\n","        [-0.2439,  0.2251,  0.8520,  0.3504, -1.1943, -0.2081,  0.5277,  0.1434,\n","          0.7451],\n","        [ 0.8014, -0.0174,  0.2345, -0.8167,  0.3282,  1.1147,  0.5645,  0.5330,\n","          0.1503]])\n","first_linear.bias tensor([-1.7487, -0.8083,  1.2890,  1.2482,  1.0719,  1.2054,  0.9984, -1.7162,\n","        -0.4174, -0.9932])\n","linear_hidden.0.weight tensor([[-0.1639, -0.8892, -0.3468,  0.2488, -0.1055, -0.2902, -0.1815, -1.4306,\n","          0.0770, -0.6667],\n","        [ 1.6606, -0.2193, -2.0050,  0.3548,  1.4030, -1.7317,  0.5388, -0.3448,\n","          0.0945,  0.2136],\n","        [-1.9575,  0.0951,  0.9093, -0.2649,  1.0178, -0.5415,  0.6680,  0.7822,\n","         -2.2861,  0.4999],\n","        [ 0.4620, -0.3726, -1.0576,  1.0109, -0.3442, -1.3391,  0.9891, -1.1521,\n","         -0.2820, -0.9112],\n","        [ 1.0074,  0.8057,  0.1547, -0.8835, -0.2853, -0.3404,  0.1221,  1.3962,\n","         -0.6170,  0.5353],\n","        [ 0.5466,  0.1741, -0.9015,  0.1279, -1.7044, -1.2597,  0.5315, -1.2986,\n","          1.0716, -1.0907],\n","        [-0.2820,  1.3803,  0.3842,  0.3584,  0.9264,  0.5328, -0.2337,  0.8699,\n","          0.6088,  0.6282],\n","        [ 0.1043,  0.8415,  1.2172,  0.5182,  1.3335,  1.5581,  0.1444,  0.0666,\n","          2.1275, -0.5341],\n","        [ 0.1329,  0.4070,  0.4838,  0.0968,  0.4255, -0.1052,  0.3741,  1.4919,\n","          0.3671,  0.4937],\n","        [-0.2605, -1.1358,  0.5373,  2.0333, -0.1018,  0.0753,  0.2356, -0.1697,\n","          1.4314,  1.1759]])\n","linear_hidden.0.bias tensor([ 0.3718,  0.2774, -0.0190, -0.4306, -0.2938,  0.5968, -0.5329, -0.1495,\n","        -0.9564,  0.9786])\n","linear_output.weight tensor([[-1.3334, -1.8087,  2.1400, -1.4738,  1.5831, -2.4156,  1.5406,  1.9481,\n","          1.4920, -1.2860]])\n","linear_output.bias tensor([0.0493])\n","Testing out: \n","batch_size:  750\n","train_size:  669\n","n_epochs:  96\n","lr:  0.9872235836653859\n","weight_decay:  3.262865452079406e-05\n","betas0:  0.8789477505926764\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0587,  0.1489, -0.0810],\n","          [-0.1523, -0.2158, -0.2013],\n","          [-0.0913,  0.1222, -0.0270]]]])\n","conv1.bias tensor([-0.2193])\n","first_linear.weight tensor([[ 0.3202, -0.1440, -0.0013,  0.2725, -0.1454,  0.1996,  0.2932, -0.2433,\n","         -0.0485],\n","        [ 0.0615,  0.2532,  0.0034, -0.1102,  0.2491,  0.2541,  0.2570, -0.0796,\n","         -0.0502],\n","        [ 0.2732,  0.2837,  0.2760, -0.1595, -0.0031,  0.0589,  0.0603, -0.1301,\n","          0.0077],\n","        [-0.2098,  0.1727, -0.3220,  0.1510, -0.1776,  0.0503, -0.0032,  0.0664,\n","         -0.0407],\n","        [-0.2004,  0.0235, -0.2830, -0.1907, -0.3310, -0.2679,  0.0493,  0.1511,\n","          0.1785],\n","        [ 0.0856,  0.2781,  0.0491,  0.2963,  0.1344,  0.0239, -0.0070,  0.0242,\n","          0.0313],\n","        [-0.2702, -0.1642, -0.2769,  0.3101, -0.3191, -0.3285, -0.2623,  0.1029,\n","          0.1985],\n","        [-0.2026,  0.0369, -0.2418, -0.0832, -0.0640, -0.0133,  0.3056, -0.0222,\n","          0.2068],\n","        [ 0.2428,  0.2754,  0.2883,  0.1460, -0.1609,  0.3093,  0.2524,  0.3285,\n","          0.1313],\n","        [-0.0617, -0.1793, -0.1099,  0.0308, -0.0041, -0.0465,  0.0487, -0.1364,\n","         -0.1103]])\n","first_linear.bias tensor([-0.0407,  0.0377,  0.1439,  0.1959, -0.0143, -0.0673, -0.3229,  0.0297,\n","         0.1827, -0.2435])\n","linear_hidden.0.weight tensor([[ 0.3068,  0.2781, -0.0035,  0.3152,  0.0552,  0.1008,  0.1671,  0.0678,\n","          0.2614, -0.1425],\n","        [ 0.0210,  0.1226,  0.0130, -0.2012, -0.2794,  0.0679,  0.2991, -0.0682,\n","          0.0252,  0.2694],\n","        [-0.0009, -0.2582, -0.1259,  0.1461, -0.0771,  0.1916,  0.1385,  0.2189,\n","         -0.2648, -0.2259],\n","        [ 0.0585, -0.1528,  0.2187,  0.0390,  0.1635,  0.2762, -0.1675,  0.0697,\n","          0.1777,  0.2919],\n","        [-0.1584,  0.2693,  0.2994, -0.1373, -0.1265,  0.2549,  0.2389,  0.1265,\n","         -0.2862,  0.2708],\n","        [-0.1051, -0.0247, -0.2725,  0.0011,  0.2360,  0.2414,  0.1307, -0.2613,\n","          0.0982,  0.0467],\n","        [ 0.1877,  0.2538,  0.1519,  0.3093, -0.2466, -0.1919,  0.1168,  0.1140,\n","         -0.0457,  0.0850],\n","        [-0.1154, -0.1006, -0.1181, -0.2803,  0.0983, -0.2457, -0.1058,  0.0595,\n","          0.3048, -0.1658],\n","        [ 0.0561, -0.0285,  0.1950,  0.0612,  0.1079,  0.2561, -0.2434, -0.2880,\n","         -0.1970,  0.0019],\n","        [ 0.0490, -0.2042, -0.1718, -0.1631,  0.2761,  0.0039,  0.1987, -0.2499,\n","         -0.0572, -0.3071]])\n","linear_hidden.0.bias tensor([ 0.2616, -0.1729,  0.3098,  0.2590, -0.1407,  0.1409,  0.2758,  0.0719,\n","        -0.1976, -0.0887])\n","linear_output.weight tensor([[-0.2864,  0.2780, -0.1195, -0.2290, -0.2896,  0.2685, -0.1541, -0.0394,\n","         -0.2973,  0.0492]])\n","linear_output.bias tensor([0.2697])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([669])) that is different to the input size (torch.Size([669, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6882159113883972 \t Validate_Accuracy: 0.497\n","epoch 2\n","Epoch: 2 \t Train Loss: 1.8460215330123901 \t Validate_Accuracy: 0.5605\n","epoch 3\n","Epoch: 3 \t Train Loss: 2.4899582862854004 \t Validate_Accuracy: 0.5305\n","epoch 4\n","Epoch: 4 \t Train Loss: 1.3706258535385132 \t Validate_Accuracy: 0.48\n","epoch 5\n","Epoch: 5 \t Train Loss: 1.3583641052246094 \t Validate_Accuracy: 0.5795\n","epoch 6\n","Epoch: 6 \t Train Loss: 1.0117826461791992 \t Validate_Accuracy: 0.505\n","epoch 7\n","Epoch: 7 \t Train Loss: 1.0609171390533447 \t Validate_Accuracy: 0.5195\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.869448721408844 \t Validate_Accuracy: 0.606\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.8168125152587891 \t Validate_Accuracy: 0.525\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.8284881114959717 \t Validate_Accuracy: 0.6305\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.7296432256698608 \t Validate_Accuracy: 0.625\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.7133619785308838 \t Validate_Accuracy: 0.6205\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.7326235175132751 \t Validate_Accuracy: 0.6135\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.7408235669136047 \t Validate_Accuracy: 0.6185\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.7481487989425659 \t Validate_Accuracy: 0.6245\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.769743025302887 \t Validate_Accuracy: 0.625\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.7309055328369141 \t Validate_Accuracy: 0.615\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6938380002975464 \t Validate_Accuracy: 0.617\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.7094395160675049 \t Validate_Accuracy: 0.6145\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6704989075660706 \t Validate_Accuracy: 0.6285\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6611921787261963 \t Validate_Accuracy: 0.63\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.666420578956604 \t Validate_Accuracy: 0.641\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6597360968589783 \t Validate_Accuracy: 0.6295\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6294972896575928 \t Validate_Accuracy: 0.627\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6404922604560852 \t Validate_Accuracy: 0.6335\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6329897046089172 \t Validate_Accuracy: 0.6455\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.629112184047699 \t Validate_Accuracy: 0.6455\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6247918009757996 \t Validate_Accuracy: 0.6565\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6045224666595459 \t Validate_Accuracy: 0.657\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.5967255234718323 \t Validate_Accuracy: 0.667\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6005899310112 \t Validate_Accuracy: 0.6655\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.588589608669281 \t Validate_Accuracy: 0.6625\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.5921540260314941 \t Validate_Accuracy: 0.6565\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.5851417779922485 \t Validate_Accuracy: 0.658\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.5843411684036255 \t Validate_Accuracy: 0.66\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.5735750794410706 \t Validate_Accuracy: 0.668\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.5704188346862793 \t Validate_Accuracy: 0.671\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.5601959824562073 \t Validate_Accuracy: 0.671\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.5549256801605225 \t Validate_Accuracy: 0.674\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.5524708032608032 \t Validate_Accuracy: 0.667\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.549728274345398 \t Validate_Accuracy: 0.6815\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.5500720739364624 \t Validate_Accuracy: 0.6865\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.5543884634971619 \t Validate_Accuracy: 0.6905\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.5440151691436768 \t Validate_Accuracy: 0.6975\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.5447804927825928 \t Validate_Accuracy: 0.6955\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.5336642265319824 \t Validate_Accuracy: 0.6865\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.5372670292854309 \t Validate_Accuracy: 0.682\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.5412322878837585 \t Validate_Accuracy: 0.6775\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.5474348068237305 \t Validate_Accuracy: 0.68\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.5507761240005493 \t Validate_Accuracy: 0.684\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.5439350605010986 \t Validate_Accuracy: 0.684\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.5377588868141174 \t Validate_Accuracy: 0.6905\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.5359438061714172 \t Validate_Accuracy: 0.6915\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.5388913750648499 \t Validate_Accuracy: 0.6975\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.5476325154304504 \t Validate_Accuracy: 0.702\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.5446227192878723 \t Validate_Accuracy: 0.709\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.5296029448509216 \t Validate_Accuracy: 0.7145\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.5337454080581665 \t Validate_Accuracy: 0.712\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.5328673124313354 \t Validate_Accuracy: 0.7235\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.5336118340492249 \t Validate_Accuracy: 0.7315\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.5292510986328125 \t Validate_Accuracy: 0.722\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.51242995262146 \t Validate_Accuracy: 0.723\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.5175249576568604 \t Validate_Accuracy: 0.741\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.5083129405975342 \t Validate_Accuracy: 0.747\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.49948954582214355 \t Validate_Accuracy: 0.759\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.4881691336631775 \t Validate_Accuracy: 0.7635\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.4761291742324829 \t Validate_Accuracy: 0.7635\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.4702526926994324 \t Validate_Accuracy: 0.7635\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.4635135233402252 \t Validate_Accuracy: 0.761\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.46280157566070557 \t Validate_Accuracy: 0.753\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.4724283814430237 \t Validate_Accuracy: 0.756\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.47160470485687256 \t Validate_Accuracy: 0.7595\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.4705663323402405 \t Validate_Accuracy: 0.763\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.4613572657108307 \t Validate_Accuracy: 0.7555\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.4543026387691498 \t Validate_Accuracy: 0.75\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.44715195894241333 \t Validate_Accuracy: 0.755\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.44792452454566956 \t Validate_Accuracy: 0.758\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.4551393985748291 \t Validate_Accuracy: 0.7505\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.43411532044410706 \t Validate_Accuracy: 0.7455\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.42800161242485046 \t Validate_Accuracy: 0.7455\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.4300802946090698 \t Validate_Accuracy: 0.7335\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.4140947163105011 \t Validate_Accuracy: 0.735\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.41556817293167114 \t Validate_Accuracy: 0.7465\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.416970431804657 \t Validate_Accuracy: 0.7405\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.4203435182571411 \t Validate_Accuracy: 0.736\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.4284694492816925 \t Validate_Accuracy: 0.7475\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.4152331054210663 \t Validate_Accuracy: 0.743\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.4069865643978119 \t Validate_Accuracy: 0.738\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.4234617352485657 \t Validate_Accuracy: 0.7355\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.4192124009132385 \t Validate_Accuracy: 0.742\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.41016873717308044 \t Validate_Accuracy: 0.737\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.40594837069511414 \t Validate_Accuracy: 0.748\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.40735772252082825 \t Validate_Accuracy: 0.747\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.4117588996887207 \t Validate_Accuracy: 0.741\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.4047820568084717 \t Validate_Accuracy: 0.741\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.40329158306121826 \t Validate_Accuracy: 0.738\n","model parameters! \n","\n","conv1.weight tensor([[[[ 7.7265,  9.4576, -6.9217],\n","          [10.1827, -3.9708,  6.9933],\n","          [ 7.0511, 11.7411, 13.6096]]]])\n","conv1.bias tensor([2.3393])\n","first_linear.weight tensor([[  1.6130,   0.4065,   3.4796, -10.6640,   3.0924,  -1.1027,   5.4343,\n","           9.3949,   0.9721],\n","        [  0.1981,  -2.6618,  -0.2243,  -0.8664,   9.7289,   6.1812,   0.3752,\n","          -2.0488,  -5.2405],\n","        [  0.5021,   5.8648,   5.6650,  12.3989,  -0.9274,  -1.8266,   5.0965,\n","           0.5556,   3.3130],\n","        [ -4.2141,  -6.0537,  -2.8711,  12.3306,   1.7532,   5.1622,  -3.4343,\n","           5.1874,   2.5354],\n","        [  0.2202,   3.0707,   8.8823,   7.1351,   9.0618,   8.7053,   0.9382,\n","          -1.5595,  11.3405],\n","        [  4.8872,   3.4222,  -1.1071,   5.7481,  -0.2408,  13.7526,   1.7817,\n","          -0.5871,   5.9798],\n","        [  5.1003,   2.2755,  -0.6675,   7.1763,  -6.4700,  -9.5798, -10.3286,\n","          -3.7004,  -3.6198],\n","        [  3.1335,   8.6472,   7.6192,   2.4290,  -0.0609,  14.7520,  -0.6350,\n","           0.9732,   8.2634],\n","        [  6.2044,   6.6269,   0.7916,  -6.7142,   3.9456,  -0.9735,   0.1896,\n","           4.2813,   9.2585],\n","        [  0.2212,   9.0877,   3.5076,   3.1744,   3.2438,  -0.3808,   2.6628,\n","          -0.8345,  -0.9856]])\n","first_linear.bias tensor([-6.3485,  3.0651, -0.5430,  1.3777, -1.2262, -2.1703, -0.5720, 10.4372,\n","        -8.2927, 12.1025])\n","linear_hidden.0.weight tensor([[ 4.7609,  0.6875,  8.2227,  2.1036, -0.7056,  0.7243, -3.9958, -1.2533,\n","          4.2488,  0.8194],\n","        [ 1.1602,  0.8021,  5.4437,  5.7819,  9.6732,  8.4591, -2.2643,  6.0029,\n","          5.1129,  2.1310],\n","        [-2.8321, -0.2605, -4.1792, -6.1988, -7.9477,  2.1439,  5.3371, -3.9317,\n","         -2.2885, -4.2641],\n","        [ 5.5947, -2.7435,  2.4139,  2.9549,  0.6647,  4.8541, -1.8687,  1.5843,\n","          4.5274,  2.8570],\n","        [-5.5355, -3.0997, -3.5079, -1.1422, -7.6851,  3.8456,  2.3186, -1.7636,\n","         -1.1090, -3.4665],\n","        [-0.8399,  0.3868,  0.1883,  2.6907,  6.5959,  6.3813, -1.0298,  6.2829,\n","          3.4721,  2.7068],\n","        [ 7.2983,  4.3570,  2.9957,  2.7923, -6.4452, -3.3030, -6.7171,  3.7872,\n","          6.3329,  5.4189],\n","        [-4.5677, -3.0703, -6.1603, -7.6730,  6.1441,  5.4780,  3.7305, -4.0218,\n","         -3.2144, -6.9164],\n","        [-4.1932, -6.6440, -5.7822, -5.6329,  0.2037,  2.7790,  3.9411, -6.3346,\n","         -6.3877, -4.1382],\n","        [-0.7935, -5.7964, -2.6378, -3.8385, -0.5803, -0.5993,  1.2975, -1.0784,\n","          1.8667, -3.6911]])\n","linear_hidden.0.bias tensor([ -8.3210,   6.6173,   3.2838, -10.0377,   0.1553,   8.2059,  -3.7450,\n","          3.0259,   2.6886,  11.3350])\n","linear_output.weight tensor([[ 1.2376,  0.4410, -0.9487,  1.7482,  1.0568, -1.5154, -1.0577,  1.1616,\n","          0.2557, -1.4770]])\n","linear_output.bias tensor([2.4630])\n","Testing out: \n","batch_size:  4446\n","train_size:  3591\n","n_epochs:  209\n","lr:  0.0013885802132894717\n","weight_decay:  0.020741516844913405\n","betas0:  0.9997572718202594\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.0822,  0.2967,  0.1902],\n","          [ 0.2226, -0.1726, -0.2386],\n","          [ 0.1542,  0.1566, -0.1075]]]])\n","conv1.bias tensor([0.1086])\n","first_linear.weight tensor([[ 0.0486, -0.0404, -0.1674, -0.1123,  0.1379,  0.0499,  0.1039,  0.2614,\n","          0.1337],\n","        [ 0.1981,  0.1239,  0.2844,  0.1229,  0.1374,  0.0971,  0.2901,  0.0006,\n","          0.1067],\n","        [ 0.0900, -0.0669, -0.2832, -0.2215, -0.2004, -0.0885, -0.2607, -0.0019,\n","          0.2865],\n","        [-0.2111,  0.2172,  0.1117,  0.2642, -0.1694, -0.0919,  0.1325, -0.1580,\n","         -0.0277],\n","        [ 0.1951, -0.1311,  0.0711, -0.1054,  0.3021, -0.0048,  0.1655, -0.1119,\n","         -0.2332],\n","        [-0.0405,  0.1806,  0.0675,  0.1190,  0.1409,  0.2240,  0.1434,  0.2050,\n","         -0.0131],\n","        [-0.0278, -0.1865, -0.2064,  0.1422, -0.0069,  0.2922, -0.2399, -0.0754,\n","          0.2069],\n","        [ 0.2680, -0.0006,  0.3211, -0.2583,  0.0754, -0.1800,  0.0628, -0.2963,\n","         -0.0787],\n","        [ 0.2246, -0.1044, -0.2427, -0.1195,  0.2172, -0.2434,  0.0283,  0.1401,\n","          0.1264],\n","        [ 0.1136,  0.1289,  0.2869,  0.0852, -0.0575,  0.3125, -0.2761, -0.2255,\n","         -0.2918]])\n","first_linear.bias tensor([-0.1702, -0.2339,  0.1054, -0.3227, -0.1989,  0.1138, -0.0474,  0.1339,\n","         0.2963,  0.1312])\n","linear_hidden.0.weight tensor([[ 1.0574e-01,  1.2905e-01,  2.9057e-01, -1.2741e-01, -1.0207e-01,\n","          2.0103e-01,  6.9279e-02,  1.2921e-01, -1.4975e-01, -2.6946e-01],\n","        [-1.7544e-01, -7.8644e-02,  7.6934e-02,  2.9024e-01, -2.2166e-01,\n","         -2.4037e-01,  1.9886e-01,  1.5153e-01, -9.5356e-02, -2.4780e-01],\n","        [-2.3482e-02, -6.7364e-02, -2.3946e-01, -3.1567e-04, -1.2821e-01,\n","         -2.9437e-01,  2.3715e-01,  1.2509e-01, -1.4901e-01, -2.9592e-01],\n","        [ 1.9875e-01,  1.7142e-01, -8.6544e-02, -3.1604e-01,  1.6080e-01,\n","          2.1718e-01, -3.5448e-02,  1.3069e-01,  1.2112e-01,  1.8388e-01],\n","        [ 3.7113e-02, -1.6930e-01, -2.4476e-01,  1.0805e-01,  8.1739e-03,\n","          2.3351e-01, -2.1482e-01, -2.1783e-01, -1.3045e-01,  2.5129e-01],\n","        [-8.8552e-02,  1.8143e-01,  2.8443e-01, -1.5475e-01,  4.7746e-02,\n","         -2.7238e-01, -2.3465e-01, -8.9443e-02, -1.3795e-03,  2.0157e-01],\n","        [-1.1828e-01,  1.5529e-01,  2.0971e-01, -1.5887e-01, -3.1023e-02,\n","          2.5317e-01,  2.8884e-01, -2.9007e-01, -1.8175e-01,  2.1201e-02],\n","        [ 1.1999e-01, -3.0342e-01,  2.6746e-01,  2.3091e-01, -8.9427e-02,\n","         -1.3315e-01, -2.0263e-01, -1.0420e-01,  3.1090e-01,  2.9656e-01],\n","        [-2.2866e-01, -6.0036e-03, -2.7526e-01, -1.9341e-01,  2.6735e-01,\n","          2.3200e-01,  1.9382e-02,  4.3610e-02, -1.4042e-01,  1.6816e-01],\n","        [-2.2926e-01, -3.4566e-02,  1.1163e-01,  1.2308e-01,  9.7248e-02,\n","          1.2131e-01, -2.5174e-01, -3.1340e-01,  2.0848e-01,  1.1415e-01]])\n","linear_hidden.0.bias tensor([-0.3159,  0.2065, -0.0365,  0.0813, -0.2413,  0.0557,  0.0144, -0.2683,\n","        -0.2348, -0.2623])\n","linear_output.weight tensor([[-0.2485, -0.1055,  0.2456,  0.1458,  0.0777, -0.3122,  0.1853, -0.0222,\n","         -0.2296,  0.0435]])\n","linear_output.bias tensor([-0.0336])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6941397190093994 \t Validate_Accuracy: 0.492\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([3591])) that is different to the input size (torch.Size([3591, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6940745711326599 \t Validate_Accuracy: 0.493\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6940128207206726 \t Validate_Accuracy: 0.499\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6939554810523987 \t Validate_Accuracy: 0.5065\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6939035654067993 \t Validate_Accuracy: 0.508\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6938562393188477 \t Validate_Accuracy: 0.511\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.693810224533081 \t Validate_Accuracy: 0.517\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6937682032585144 \t Validate_Accuracy: 0.5165\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6937322616577148 \t Validate_Accuracy: 0.521\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6936966180801392 \t Validate_Accuracy: 0.523\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6936655640602112 \t Validate_Accuracy: 0.5245\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6936352849006653 \t Validate_Accuracy: 0.5285\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6936059594154358 \t Validate_Accuracy: 0.5295\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6935812830924988 \t Validate_Accuracy: 0.531\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6935556530952454 \t Validate_Accuracy: 0.532\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6935331225395203 \t Validate_Accuracy: 0.5305\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6935103535652161 \t Validate_Accuracy: 0.5295\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6934893131256104 \t Validate_Accuracy: 0.528\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6934708952903748 \t Validate_Accuracy: 0.528\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.693452000617981 \t Validate_Accuracy: 0.528\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6934344172477722 \t Validate_Accuracy: 0.5265\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6934185028076172 \t Validate_Accuracy: 0.5265\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6934052109718323 \t Validate_Accuracy: 0.5255\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6933899521827698 \t Validate_Accuracy: 0.5245\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6933764219284058 \t Validate_Accuracy: 0.525\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.693366527557373 \t Validate_Accuracy: 0.5235\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6933534741401672 \t Validate_Accuracy: 0.5225\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6933428049087524 \t Validate_Accuracy: 0.521\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6933333873748779 \t Validate_Accuracy: 0.5215\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6933231353759766 \t Validate_Accuracy: 0.52\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6933143138885498 \t Validate_Accuracy: 0.52\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6933067440986633 \t Validate_Accuracy: 0.52\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6932982802391052 \t Validate_Accuracy: 0.5185\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6932907700538635 \t Validate_Accuracy: 0.5175\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6932834386825562 \t Validate_Accuracy: 0.518\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6932756304740906 \t Validate_Accuracy: 0.519\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6932698488235474 \t Validate_Accuracy: 0.52\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.693264901638031 \t Validate_Accuracy: 0.5205\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6932592391967773 \t Validate_Accuracy: 0.5225\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.6932541131973267 \t Validate_Accuracy: 0.5235\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6932483911514282 \t Validate_Accuracy: 0.5245\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6932434439659119 \t Validate_Accuracy: 0.5245\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6932389140129089 \t Validate_Accuracy: 0.5235\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6932362914085388 \t Validate_Accuracy: 0.524\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6932317614555359 \t Validate_Accuracy: 0.5255\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6932277083396912 \t Validate_Accuracy: 0.5245\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6932240724563599 \t Validate_Accuracy: 0.5245\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6932217478752136 \t Validate_Accuracy: 0.5255\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.6932169198989868 \t Validate_Accuracy: 0.5245\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6932147145271301 \t Validate_Accuracy: 0.5245\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6932119727134705 \t Validate_Accuracy: 0.525\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.693208634853363 \t Validate_Accuracy: 0.5245\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6932057738304138 \t Validate_Accuracy: 0.524\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6932032704353333 \t Validate_Accuracy: 0.5235\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6932014226913452 \t Validate_Accuracy: 0.5235\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.6931985020637512 \t Validate_Accuracy: 0.523\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.6931972503662109 \t Validate_Accuracy: 0.522\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6931943297386169 \t Validate_Accuracy: 0.5215\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.6931915879249573 \t Validate_Accuracy: 0.5195\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6931901574134827 \t Validate_Accuracy: 0.5205\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.6931881904602051 \t Validate_Accuracy: 0.52\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6931877732276917 \t Validate_Accuracy: 0.52\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6931851506233215 \t Validate_Accuracy: 0.5195\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6931843757629395 \t Validate_Accuracy: 0.52\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.693182110786438 \t Validate_Accuracy: 0.5215\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.693182110786438 \t Validate_Accuracy: 0.5215\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6931790113449097 \t Validate_Accuracy: 0.521\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6931779384613037 \t Validate_Accuracy: 0.522\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6931779980659485 \t Validate_Accuracy: 0.5225\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.6931760907173157 \t Validate_Accuracy: 0.524\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.6931748390197754 \t Validate_Accuracy: 0.5255\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.6931735277175903 \t Validate_Accuracy: 0.525\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.693173348903656 \t Validate_Accuracy: 0.525\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.6931710243225098 \t Validate_Accuracy: 0.526\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.6931709051132202 \t Validate_Accuracy: 0.526\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.6931692361831665 \t Validate_Accuracy: 0.5235\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.6931688785552979 \t Validate_Accuracy: 0.5225\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.6931690573692322 \t Validate_Accuracy: 0.5215\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.6931678056716919 \t Validate_Accuracy: 0.5205\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.6931667923927307 \t Validate_Accuracy: 0.5215\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.6931666135787964 \t Validate_Accuracy: 0.5215\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.6931650638580322 \t Validate_Accuracy: 0.522\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.6931649446487427 \t Validate_Accuracy: 0.523\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.6931642293930054 \t Validate_Accuracy: 0.5235\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.6931643486022949 \t Validate_Accuracy: 0.5245\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.6931617856025696 \t Validate_Accuracy: 0.5245\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.6931630373001099 \t Validate_Accuracy: 0.523\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.6931615471839905 \t Validate_Accuracy: 0.523\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.6931623220443726 \t Validate_Accuracy: 0.5255\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.6931607127189636 \t Validate_Accuracy: 0.525\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.6931610107421875 \t Validate_Accuracy: 0.5235\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.6931595206260681 \t Validate_Accuracy: 0.5255\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.6931594014167786 \t Validate_Accuracy: 0.5255\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.6931597590446472 \t Validate_Accuracy: 0.5245\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.6931579113006592 \t Validate_Accuracy: 0.5255\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.6931591033935547 \t Validate_Accuracy: 0.5265\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.6931569576263428 \t Validate_Accuracy: 0.5265\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.6931580901145935 \t Validate_Accuracy: 0.528\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.6931578516960144 \t Validate_Accuracy: 0.5285\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.6931569576263428 \t Validate_Accuracy: 0.5265\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.693156898021698 \t Validate_Accuracy: 0.528\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.6931568384170532 \t Validate_Accuracy: 0.5315\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.6931569576263428 \t Validate_Accuracy: 0.533\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.6931554079055786 \t Validate_Accuracy: 0.5325\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.6931557655334473 \t Validate_Accuracy: 0.535\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.6931557655334473 \t Validate_Accuracy: 0.531\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.69315505027771 \t Validate_Accuracy: 0.5315\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.6931544542312622 \t Validate_Accuracy: 0.532\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.6931555271148682 \t Validate_Accuracy: 0.5335\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.693154513835907 \t Validate_Accuracy: 0.534\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.6931547522544861 \t Validate_Accuracy: 0.538\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.6931540966033936 \t Validate_Accuracy: 0.54\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.6931543350219727 \t Validate_Accuracy: 0.539\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.6931548118591309 \t Validate_Accuracy: 0.535\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.6931535005569458 \t Validate_Accuracy: 0.535\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.6931528449058533 \t Validate_Accuracy: 0.5395\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.6931525468826294 \t Validate_Accuracy: 0.5345\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.534\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.6931523680686951 \t Validate_Accuracy: 0.533\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.693153440952301 \t Validate_Accuracy: 0.5325\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.6931520700454712 \t Validate_Accuracy: 0.533\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.6931526064872742 \t Validate_Accuracy: 0.5335\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.6931518316268921 \t Validate_Accuracy: 0.531\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.6931511759757996 \t Validate_Accuracy: 0.5275\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.6931523084640503 \t Validate_Accuracy: 0.527\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.5215\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.6931519508361816 \t Validate_Accuracy: 0.52\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.693150520324707 \t Validate_Accuracy: 0.516\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.6931511759757996 \t Validate_Accuracy: 0.519\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.6931506395339966 \t Validate_Accuracy: 0.5165\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.6931513547897339 \t Validate_Accuracy: 0.5135\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.6931504607200623 \t Validate_Accuracy: 0.5125\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.6931509375572205 \t Validate_Accuracy: 0.5145\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.6931519508361816 \t Validate_Accuracy: 0.5135\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.6931508183479309 \t Validate_Accuracy: 0.5115\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.6931494474411011 \t Validate_Accuracy: 0.51\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.6931498646736145 \t Validate_Accuracy: 0.504\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.6931493878364563 \t Validate_Accuracy: 0.5035\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.6931493878364563 \t Validate_Accuracy: 0.505\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.6931495070457458 \t Validate_Accuracy: 0.505\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.6931494474411011 \t Validate_Accuracy: 0.502\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.693150520324707 \t Validate_Accuracy: 0.5\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.6931505799293518 \t Validate_Accuracy: 0.5\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.6931506395339966 \t Validate_Accuracy: 0.5005\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.6931495666503906 \t Validate_Accuracy: 0.5005\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.6931496262550354 \t Validate_Accuracy: 0.5\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.693149983882904 \t Validate_Accuracy: 0.499\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.693149983882904 \t Validate_Accuracy: 0.499\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.6931489706039429 \t Validate_Accuracy: 0.5005\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.6931490302085876 \t Validate_Accuracy: 0.4995\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.6931486129760742 \t Validate_Accuracy: 0.502\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.6931489109992981 \t Validate_Accuracy: 0.5025\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.6931490898132324 \t Validate_Accuracy: 0.5025\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.6931489706039429 \t Validate_Accuracy: 0.5025\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.6931486129760742 \t Validate_Accuracy: 0.5025\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.6931498646736145 \t Validate_Accuracy: 0.503\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.6931495666503906 \t Validate_Accuracy: 0.503\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.6931487321853638 \t Validate_Accuracy: 0.503\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.6931501030921936 \t Validate_Accuracy: 0.503\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.6931483149528503 \t Validate_Accuracy: 0.503\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.6931489109992981 \t Validate_Accuracy: 0.503\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.6931482553482056 \t Validate_Accuracy: 0.503\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.693149209022522 \t Validate_Accuracy: 0.503\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.6931474208831787 \t Validate_Accuracy: 0.503\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.6931485533714294 \t Validate_Accuracy: 0.503\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.6931478977203369 \t Validate_Accuracy: 0.503\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.6931475400924683 \t Validate_Accuracy: 0.503\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.6931477189064026 \t Validate_Accuracy: 0.503\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.6931479573249817 \t Validate_Accuracy: 0.503\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.6931480169296265 \t Validate_Accuracy: 0.503\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.6931477785110474 \t Validate_Accuracy: 0.503\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.6931479573249817 \t Validate_Accuracy: 0.503\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.6931487321853638 \t Validate_Accuracy: 0.503\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.6931483149528503 \t Validate_Accuracy: 0.503\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.6931479573249817 \t Validate_Accuracy: 0.503\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.6931476593017578 \t Validate_Accuracy: 0.503\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.6931490898132324 \t Validate_Accuracy: 0.503\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.6931476593017578 \t Validate_Accuracy: 0.503\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.6931487321853638 \t Validate_Accuracy: 0.503\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.6931478977203369 \t Validate_Accuracy: 0.503\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.6931482553482056 \t Validate_Accuracy: 0.503\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.6931470632553101 \t Validate_Accuracy: 0.503\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.6931471824645996 \t Validate_Accuracy: 0.503\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.6931465864181519 \t Validate_Accuracy: 0.503\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.6931466460227966 \t Validate_Accuracy: 0.503\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.6931458711624146 \t Validate_Accuracy: 0.503\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.6931456327438354 \t Validate_Accuracy: 0.503\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.6931447982788086 \t Validate_Accuracy: 0.503\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.6931465268135071 \t Validate_Accuracy: 0.503\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.6931456923484802 \t Validate_Accuracy: 0.503\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.6931444406509399 \t Validate_Accuracy: 0.503\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.6931443810462952 \t Validate_Accuracy: 0.503\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.693145751953125 \t Validate_Accuracy: 0.503\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.6931448578834534 \t Validate_Accuracy: 0.503\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.6931448578834534 \t Validate_Accuracy: 0.503\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.693147599697113 \t Validate_Accuracy: 0.503\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.693145751953125 \t Validate_Accuracy: 0.503\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.693144679069519 \t Validate_Accuracy: 0.503\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.6931458115577698 \t Validate_Accuracy: 0.503\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.6931465268135071 \t Validate_Accuracy: 0.503\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.6931456923484802 \t Validate_Accuracy: 0.503\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.6931458115577698 \t Validate_Accuracy: 0.503\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.6931458115577698 \t Validate_Accuracy: 0.503\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.6931453943252563 \t Validate_Accuracy: 0.503\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.6931449174880981 \t Validate_Accuracy: 0.503\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.6931456923484802 \t Validate_Accuracy: 0.503\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.6931450366973877 \t Validate_Accuracy: 0.503\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.6931450366973877 \t Validate_Accuracy: 0.503\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.6931451559066772 \t Validate_Accuracy: 0.503\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.0003,  0.0845,  0.0207],\n","          [ 0.0376, -0.0063, -0.0308],\n","          [ 0.0065,  0.0065, -0.0003]]]])\n","conv1.bias tensor([-3.3533e-05])\n","first_linear.weight tensor([[ 6.3141e-05,  2.3030e-05, -7.5204e-03, -2.6768e-05,  1.3561e-03,\n","          1.3481e-05, -2.0150e-04,  5.3776e-02,  6.8708e-04],\n","        [ 1.9984e-02,  5.7837e-04,  6.9365e-02,  4.9104e-04,  1.8832e-03,\n","         -1.1634e-04,  7.1968e-02,  2.4918e-05,  4.5023e-06],\n","        [-1.4473e-03, -2.1611e-04, -6.7558e-02, -3.0872e-02, -1.8603e-02,\n","         -1.9537e-04, -5.5985e-02,  1.3849e-04,  6.8495e-02],\n","        [-2.5993e-02,  2.4965e-02, -9.9943e-05,  5.2885e-02, -8.8584e-03,\n","          5.3473e-05,  5.8273e-04, -5.5552e-03, -7.8298e-06],\n","        [ 1.7859e-02, -5.3229e-04,  8.2141e-05,  1.3661e-05,  8.0960e-02,\n","         -1.9271e-05,  6.5065e-03,  6.3667e-05, -3.3035e-02],\n","        [ 2.5963e-04,  1.2345e-02, -1.7607e-04,  6.0410e-04,  2.5171e-03,\n","          3.1734e-02,  2.6437e-03,  2.3096e-02, -2.8107e-05],\n","        [ 4.4680e-04, -1.3591e-02, -2.1100e-02,  3.4025e-03,  3.3989e-05,\n","          7.4388e-02, -3.5752e-02, -1.0020e-04,  2.2462e-02],\n","        [ 6.0129e-02, -2.5284e-05,  9.6735e-02, -4.9760e-02, -6.9400e-06,\n","         -1.0077e-02,  1.0547e-04, -7.7310e-02,  1.4862e-05],\n","        [ 2.9164e-02,  1.9957e-05, -4.1191e-02, -2.3764e-04,  2.5673e-02,\n","         -4.1376e-02, -1.9304e-06,  1.7525e-03,  4.2824e-04],\n","        [-4.3593e-04,  6.4058e-04,  6.9204e-02,  4.6026e-06, -2.3137e-05,\n","          8.8718e-02, -6.3804e-02, -3.1731e-02, -7.3361e-02]])\n","first_linear.bias tensor([-6.2451e-03, -4.2742e-02,  8.4356e-05, -9.7676e-02, -1.7033e-02,\n","         5.1384e-04, -4.3851e-05,  1.6573e-03,  7.6856e-02,  1.0263e-03])\n","linear_hidden.0.weight tensor([[ 9.8051e-06,  2.9453e-03,  7.2822e-02, -1.0234e-03, -9.4021e-05,\n","          1.4143e-02, -2.1614e-04,  1.5917e-03, -2.8493e-03, -5.1217e-02],\n","        [-9.2715e-03,  5.9996e-06,  3.0246e-06,  7.1776e-02, -2.8722e-02,\n","         -4.1751e-02,  1.8556e-02,  4.2317e-03,  1.1189e-05, -4.2077e-02],\n","        [-1.4141e-05, -2.9635e-04, -4.3119e-02, -2.2595e-05, -5.2558e-04,\n","         -6.5243e-02,  3.6417e-02,  1.2088e-04, -3.5301e-03, -8.1259e-02],\n","        [ 1.9093e-02,  8.3684e-03,  7.9869e-05, -8.9522e-02,  4.8387e-03,\n","          3.3788e-02,  9.2870e-06,  2.2753e-04,  2.2282e-04,  1.0416e-02],\n","        [-3.4143e-07, -9.0737e-03, -4.2757e-02,  3.9635e-05, -2.7631e-07,\n","          3.6881e-02, -2.5771e-02, -2.8502e-02, -1.0635e-03,  4.5085e-02],\n","        [ 7.3900e-05,  1.8318e-02,  7.1225e-02, -5.7713e-03, -1.7273e-04,\n","         -6.8987e-02, -3.5032e-02, -1.5278e-04, -2.4112e-04,  2.3757e-02],\n","        [-1.0812e-04,  2.3664e-03,  2.0669e-02, -3.4787e-03,  2.6436e-05,\n","          5.2881e-02,  7.0318e-02, -7.6127e-02, -1.2792e-02,  5.5486e-05],\n","        [ 2.1148e-04, -8.2078e-02,  5.6781e-02,  3.3921e-02,  8.6518e-06,\n","         -1.1708e-03, -1.9554e-02,  4.0834e-05,  8.7578e-02,  7.7038e-02],\n","        [-3.2185e-02,  1.7728e-04, -5.9309e-02, -1.8056e-02,  5.5852e-02,\n","          3.0011e-02, -1.0601e-04, -5.6619e-05, -1.1953e-03,  9.0845e-03],\n","        [-3.2924e-02, -1.3390e-06,  1.0065e-05,  4.2083e-04, -1.9130e-05,\n","          4.2946e-04, -4.6491e-02, -9.0159e-02,  2.2080e-02,  3.9729e-05]])\n","linear_hidden.0.bias tensor([-9.4179e-02,  1.8620e-02, -2.5013e-04, -7.4698e-05, -3.8852e-02,\n","         6.8964e-04, -1.1089e-04, -5.7561e-02, -3.9325e-02, -5.2559e-02])\n","linear_output.weight tensor([[-0.0446, -0.0004,  0.0391,  0.0027, -0.0002, -0.1054,  0.0178,  0.0025,\n","         -0.0314,  0.0001]])\n","linear_output.bias tensor([-0.0075])\n","Testing out: \n","batch_size:  2923\n","train_size:  2288\n","n_epochs:  170\n","lr:  0.010533547625077681\n","weight_decay:  0.001722509956227932\n","betas0:  0.9908169386666653\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1697,  0.3138, -0.0274],\n","          [-0.0169,  0.2259, -0.2780],\n","          [-0.1570,  0.2605,  0.0669]]]])\n","conv1.bias tensor([0.0687])\n","first_linear.weight tensor([[ 0.0399,  0.1409, -0.0302,  0.3316, -0.2283, -0.2738,  0.1666,  0.0470,\n","         -0.2664],\n","        [ 0.0353,  0.1894,  0.0690, -0.1217, -0.2038,  0.0817,  0.1156,  0.1011,\n","          0.2865],\n","        [-0.2935, -0.1630, -0.1222, -0.1323, -0.1062,  0.0503, -0.1673,  0.0937,\n","          0.1135],\n","        [ 0.1554,  0.2183, -0.2444, -0.1062, -0.1366, -0.1083, -0.2410, -0.2976,\n","         -0.1041],\n","        [-0.2123,  0.3035, -0.1286,  0.1151, -0.2262, -0.0352, -0.0531,  0.1113,\n","          0.1721],\n","        [ 0.3326, -0.0646, -0.2560, -0.1272,  0.2619,  0.0604, -0.2959, -0.2921,\n","          0.0530],\n","        [ 0.0065,  0.2279, -0.0671, -0.0606, -0.1578, -0.1113,  0.2440, -0.1573,\n","         -0.2881],\n","        [-0.2233,  0.2352, -0.0991, -0.0039, -0.1928, -0.2573, -0.1551, -0.2501,\n","         -0.1581],\n","        [-0.0808,  0.0954, -0.1667,  0.3137,  0.0113,  0.2215,  0.1129,  0.2841,\n","          0.3121],\n","        [-0.2470, -0.2677,  0.2215,  0.1771, -0.1380,  0.0444, -0.1195, -0.0320,\n","          0.0507]])\n","first_linear.bias tensor([-0.0877, -0.2145,  0.1527,  0.0913,  0.0455, -0.1411,  0.0953,  0.1775,\n","        -0.3128, -0.1245])\n","linear_hidden.0.weight tensor([[-0.1775, -0.0550,  0.0449,  0.2263, -0.0564, -0.2837, -0.1793, -0.2687,\n","         -0.0799, -0.0014],\n","        [-0.2859,  0.1474,  0.2480, -0.0376, -0.2988, -0.1026, -0.3051,  0.0806,\n","          0.1137, -0.0362],\n","        [ 0.1533,  0.0327, -0.0175,  0.2149, -0.1101,  0.0451,  0.0362, -0.0006,\n","         -0.1713, -0.2351],\n","        [ 0.1794, -0.2489, -0.0313, -0.0835,  0.2430, -0.1744, -0.0325, -0.1562,\n","          0.0216, -0.3060],\n","        [-0.1173, -0.1491,  0.0276, -0.1797, -0.1546,  0.2922,  0.0090, -0.1298,\n","          0.2994, -0.0279],\n","        [ 0.1754,  0.2157, -0.2753,  0.1596,  0.2329,  0.1185,  0.0363,  0.2548,\n","          0.0866, -0.1774],\n","        [-0.1315,  0.1358,  0.0917, -0.0776, -0.1958, -0.0446, -0.1987, -0.2638,\n","         -0.3044,  0.0181],\n","        [ 0.1558, -0.0316,  0.1791, -0.2215,  0.1333, -0.1970, -0.0187, -0.2500,\n","         -0.1941, -0.0645],\n","        [-0.1587,  0.0779, -0.2565, -0.1389, -0.2033,  0.0305, -0.1530,  0.2913,\n","         -0.1875,  0.0703],\n","        [ 0.2276, -0.1620, -0.0039,  0.2760,  0.2777, -0.1496, -0.1670,  0.2175,\n","          0.1463,  0.2391]])\n","linear_hidden.0.bias tensor([-0.0209,  0.3093, -0.2200,  0.0606, -0.1139,  0.1865,  0.2394,  0.0334,\n","        -0.3047, -0.1036])\n","linear_output.weight tensor([[ 0.2869,  0.1662,  0.0780, -0.0167,  0.1251,  0.0545,  0.0348, -0.1085,\n","         -0.0757,  0.2540]])\n","linear_output.bias tensor([-0.0895])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6931774616241455 \t Validate_Accuracy: 0.494\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6927019953727722 \t Validate_Accuracy: 0.506\n","epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2288])) that is different to the input size (torch.Size([2288, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3 \t Train Loss: 0.6925312280654907 \t Validate_Accuracy: 0.512\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6923137903213501 \t Validate_Accuracy: 0.519\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6920631527900696 \t Validate_Accuracy: 0.509\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6917945742607117 \t Validate_Accuracy: 0.4755\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6914491057395935 \t Validate_Accuracy: 0.463\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.690963625907898 \t Validate_Accuracy: 0.467\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.690316915512085 \t Validate_Accuracy: 0.5005\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6895083785057068 \t Validate_Accuracy: 0.5275\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6885343194007874 \t Validate_Accuracy: 0.5305\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6873735189437866 \t Validate_Accuracy: 0.529\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6859807968139648 \t Validate_Accuracy: 0.5325\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6843144297599792 \t Validate_Accuracy: 0.5415\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6823562979698181 \t Validate_Accuracy: 0.574\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6801072955131531 \t Validate_Accuracy: 0.6115\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.677595317363739 \t Validate_Accuracy: 0.6215\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.674849271774292 \t Validate_Accuracy: 0.5955\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6718369126319885 \t Validate_Accuracy: 0.5925\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6684332489967346 \t Validate_Accuracy: 0.623\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6645272970199585 \t Validate_Accuracy: 0.684\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6601195335388184 \t Validate_Accuracy: 0.7045\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6552594304084778 \t Validate_Accuracy: 0.7165\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6499178409576416 \t Validate_Accuracy: 0.723\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6439869999885559 \t Validate_Accuracy: 0.73\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6374195218086243 \t Validate_Accuracy: 0.7405\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6302130222320557 \t Validate_Accuracy: 0.7555\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.622351348400116 \t Validate_Accuracy: 0.772\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.61380934715271 \t Validate_Accuracy: 0.7805\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6046090722084045 \t Validate_Accuracy: 0.7875\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.5947974920272827 \t Validate_Accuracy: 0.797\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.5843797326087952 \t Validate_Accuracy: 0.804\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.5733174681663513 \t Validate_Accuracy: 0.803\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.5616287589073181 \t Validate_Accuracy: 0.805\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.5494193434715271 \t Validate_Accuracy: 0.8055\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.536843478679657 \t Validate_Accuracy: 0.8065\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.5241417288780212 \t Validate_Accuracy: 0.81\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.511623740196228 \t Validate_Accuracy: 0.81\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.4995465576648712 \t Validate_Accuracy: 0.8155\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.488101601600647 \t Validate_Accuracy: 0.815\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.4774259030818939 \t Validate_Accuracy: 0.818\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.46762794256210327 \t Validate_Accuracy: 0.821\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.45879897475242615 \t Validate_Accuracy: 0.822\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.4509386122226715 \t Validate_Accuracy: 0.8225\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.44399672746658325 \t Validate_Accuracy: 0.8215\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.4378557503223419 \t Validate_Accuracy: 0.8225\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.432270884513855 \t Validate_Accuracy: 0.823\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.42694714665412903 \t Validate_Accuracy: 0.824\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.4216824769973755 \t Validate_Accuracy: 0.8235\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.41639769077301025 \t Validate_Accuracy: 0.825\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.411067396402359 \t Validate_Accuracy: 0.828\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.4057259261608124 \t Validate_Accuracy: 0.8315\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.4004492163658142 \t Validate_Accuracy: 0.833\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.395277202129364 \t Validate_Accuracy: 0.8325\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.39012303948402405 \t Validate_Accuracy: 0.8365\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.38479727506637573 \t Validate_Accuracy: 0.8355\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.37909215688705444 \t Validate_Accuracy: 0.8365\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.372963011264801 \t Validate_Accuracy: 0.839\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.3666074275970459 \t Validate_Accuracy: 0.838\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.3602634072303772 \t Validate_Accuracy: 0.8405\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.354026198387146 \t Validate_Accuracy: 0.8445\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.3479292690753937 \t Validate_Accuracy: 0.8465\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.34200912714004517 \t Validate_Accuracy: 0.8465\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.3362705707550049 \t Validate_Accuracy: 0.848\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.33073365688323975 \t Validate_Accuracy: 0.848\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.32549935579299927 \t Validate_Accuracy: 0.8465\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3207365870475769 \t Validate_Accuracy: 0.8485\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.3166266977787018 \t Validate_Accuracy: 0.85\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.3132464289665222 \t Validate_Accuracy: 0.851\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.31053653359413147 \t Validate_Accuracy: 0.8485\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.30830666422843933 \t Validate_Accuracy: 0.85\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.30629923939704895 \t Validate_Accuracy: 0.8535\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.3042953610420227 \t Validate_Accuracy: 0.8575\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.3021869361400604 \t Validate_Accuracy: 0.857\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.29998141527175903 \t Validate_Accuracy: 0.855\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.29772573709487915 \t Validate_Accuracy: 0.8555\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.29547175765037537 \t Validate_Accuracy: 0.856\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.2932662069797516 \t Validate_Accuracy: 0.856\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2911495268344879 \t Validate_Accuracy: 0.857\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2891361117362976 \t Validate_Accuracy: 0.861\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2872297763824463 \t Validate_Accuracy: 0.8605\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.285433828830719 \t Validate_Accuracy: 0.862\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2837347090244293 \t Validate_Accuracy: 0.861\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.28209760785102844 \t Validate_Accuracy: 0.861\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.28047749400138855 \t Validate_Accuracy: 0.862\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.27884700894355774 \t Validate_Accuracy: 0.864\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.27719905972480774 \t Validate_Accuracy: 0.866\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.275547593832016 \t Validate_Accuracy: 0.8665\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.27390557527542114 \t Validate_Accuracy: 0.868\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2722727954387665 \t Validate_Accuracy: 0.87\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.27063122391700745 \t Validate_Accuracy: 0.872\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2689755856990814 \t Validate_Accuracy: 0.874\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.2673136293888092 \t Validate_Accuracy: 0.8735\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.26566964387893677 \t Validate_Accuracy: 0.8735\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.264056533575058 \t Validate_Accuracy: 0.874\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.26247426867485046 \t Validate_Accuracy: 0.8725\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.26091334223747253 \t Validate_Accuracy: 0.8735\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2593699097633362 \t Validate_Accuracy: 0.873\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.25784197449684143 \t Validate_Accuracy: 0.876\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2563314735889435 \t Validate_Accuracy: 0.8755\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2548436224460602 \t Validate_Accuracy: 0.877\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.253391295671463 \t Validate_Accuracy: 0.8785\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.2519884705543518 \t Validate_Accuracy: 0.877\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2506466507911682 \t Validate_Accuracy: 0.876\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.24937620759010315 \t Validate_Accuracy: 0.875\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.24819138646125793 \t Validate_Accuracy: 0.8765\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2470991462469101 \t Validate_Accuracy: 0.8765\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.24609674513339996 \t Validate_Accuracy: 0.8765\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2451690435409546 \t Validate_Accuracy: 0.878\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2443043440580368 \t Validate_Accuracy: 0.878\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2434992790222168 \t Validate_Accuracy: 0.877\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.24275420606136322 \t Validate_Accuracy: 0.8785\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.24206630885601044 \t Validate_Accuracy: 0.8785\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.24142955243587494 \t Validate_Accuracy: 0.8775\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.24083752930164337 \t Validate_Accuracy: 0.8765\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.24028491973876953 \t Validate_Accuracy: 0.8775\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2397684007883072 \t Validate_Accuracy: 0.8775\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.239286407828331 \t Validate_Accuracy: 0.8775\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.23883739113807678 \t Validate_Accuracy: 0.8775\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.23841992020606995 \t Validate_Accuracy: 0.877\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.2380313277244568 \t Validate_Accuracy: 0.8765\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.23766714334487915 \t Validate_Accuracy: 0.8775\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.23732370138168335 \t Validate_Accuracy: 0.878\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.23699849843978882 \t Validate_Accuracy: 0.8785\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.23669007420539856 \t Validate_Accuracy: 0.8775\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.23639436066150665 \t Validate_Accuracy: 0.878\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.23610839247703552 \t Validate_Accuracy: 0.878\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.23583202064037323 \t Validate_Accuracy: 0.879\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23556582629680634 \t Validate_Accuracy: 0.879\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.23531237244606018 \t Validate_Accuracy: 0.879\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.23507247865200043 \t Validate_Accuracy: 0.879\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.23484812676906586 \t Validate_Accuracy: 0.88\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.23463767766952515 \t Validate_Accuracy: 0.8795\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.23443974554538727 \t Validate_Accuracy: 0.8795\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.23425231873989105 \t Validate_Accuracy: 0.8795\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.23407486081123352 \t Validate_Accuracy: 0.8805\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.23390866816043854 \t Validate_Accuracy: 0.881\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.233751580119133 \t Validate_Accuracy: 0.881\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.23360493779182434 \t Validate_Accuracy: 0.8805\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.23346270620822906 \t Validate_Accuracy: 0.8795\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.23332960903644562 \t Validate_Accuracy: 0.881\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.2331969290971756 \t Validate_Accuracy: 0.881\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.23308947682380676 \t Validate_Accuracy: 0.8825\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.232999786734581 \t Validate_Accuracy: 0.88\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2330540269613266 \t Validate_Accuracy: 0.883\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.2332703322172165 \t Validate_Accuracy: 0.88\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.2333400398492813 \t Validate_Accuracy: 0.882\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.23269183933734894 \t Validate_Accuracy: 0.8815\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.23233531415462494 \t Validate_Accuracy: 0.879\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.23263707756996155 \t Validate_Accuracy: 0.8825\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2323688119649887 \t Validate_Accuracy: 0.8815\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.23193548619747162 \t Validate_Accuracy: 0.88\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2321099489927292 \t Validate_Accuracy: 0.8815\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.23192106187343597 \t Validate_Accuracy: 0.8805\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.23156020045280457 \t Validate_Accuracy: 0.88\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.2316661924123764 \t Validate_Accuracy: 0.8805\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.23146769404411316 \t Validate_Accuracy: 0.88\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.23119117319583893 \t Validate_Accuracy: 0.8805\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.23125004768371582 \t Validate_Accuracy: 0.881\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.2310366928577423 \t Validate_Accuracy: 0.8805\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.23082351684570312 \t Validate_Accuracy: 0.8805\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.23083709180355072 \t Validate_Accuracy: 0.88\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.23062346875667572 \t Validate_Accuracy: 0.88\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.23044897615909576 \t Validate_Accuracy: 0.8805\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.2304251343011856 \t Validate_Accuracy: 0.88\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.23022423684597015 \t Validate_Accuracy: 0.88\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.2300684154033661 \t Validate_Accuracy: 0.881\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.23001717031002045 \t Validate_Accuracy: 0.88\n","epoch 169\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:47:16] ax.service.managed_loop: Running optimization trial 3...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 169 \t Train Loss: 0.2298341542482376 \t Validate_Accuracy: 0.8795\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.2296798825263977 \t Validate_Accuracy: 0.881\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1890,  0.0956,  0.2264],\n","          [ 0.1039, -0.0134,  0.0944],\n","          [ 0.1851,  0.0921,  0.2125]]]])\n","conv1.bias tensor([-0.0414])\n","first_linear.weight tensor([[-0.3583,  0.1417,  0.2936,  0.9192, -0.7272, -0.1892, -0.7389,  1.0965,\n","         -0.2528],\n","        [-0.7322,  0.5618, -0.0779, -0.0534,  0.1706, -0.3218,  0.5711, -0.7204,\n","          0.3582],\n","        [-0.0121,  0.0459,  0.0506,  0.1026,  0.0691,  0.1288, -0.0073, -0.0836,\n","          0.0907],\n","        [-0.1399, -0.0709,  0.1203, -0.3317, -0.3297, -0.3038, -0.2158, -0.2080,\n","         -0.2104],\n","        [ 0.0597, -0.2875,  0.0448, -0.2814, -0.0887,  0.0862, -0.4288, -0.5867,\n","          0.0502],\n","        [ 0.2537,  0.3526,  0.2002,  0.1719,  0.1959,  0.0047,  0.0601,  0.1691,\n","         -0.1834],\n","        [ 0.3898,  0.3368, -0.4466, -0.7184, -0.2766,  0.8051,  0.4622,  0.5386,\n","         -0.7740],\n","        [-0.5553,  0.6650, -0.7344,  0.8432, -0.6501,  0.8243, -0.1603, -0.0940,\n","         -0.2032],\n","        [ 0.0228,  0.2026,  0.3396, -0.1371,  0.2489,  0.3322,  0.0767,  0.1310,\n","          0.2665],\n","        [-0.1509, -0.2751, -0.1246, -0.1997, -0.1744, -0.1943,  0.0372,  0.2207,\n","         -0.0131]])\n","first_linear.bias tensor([-1.1523, -0.8510,  0.5635, -0.6111,  0.2636, -0.5134, -0.9852,  1.1455,\n","        -0.8661, -0.5906])\n","linear_hidden.0.weight tensor([[-0.5821, -0.4465, -0.3325,  0.5482, -0.3162,  0.3438, -0.4808,  0.5638,\n","          0.4843,  0.4560],\n","        [-0.5739, -0.4463, -0.3056,  0.5198, -0.2949,  0.3115, -0.4886,  0.5672,\n","          0.5347,  0.4646],\n","        [-0.5467, -0.4340, -0.2985,  0.3686, -0.2702,  0.2937, -0.4144,  0.4972,\n","          0.3643,  0.3587],\n","        [ 0.5295,  0.4129,  0.2891, -0.4246,  0.2737, -0.2891,  0.4229, -0.5094,\n","         -0.4456, -0.4040],\n","        [-0.2100, -0.0798, -0.1226, -0.0091, -0.1457,  0.2147, -0.2508,  0.1244,\n","          0.4841,  0.0200],\n","        [-0.5523, -0.4300, -0.2976,  0.4970, -0.2877,  0.2986, -0.4649,  0.5448,\n","          0.5171,  0.4316],\n","        [ 0.5260,  0.4116,  0.2899, -0.4136,  0.2710, -0.2863,  0.4176, -0.5049,\n","         -0.4377, -0.4036],\n","        [ 0.5809,  0.4497,  0.3156, -0.5294,  0.2960, -0.3074,  0.4948, -0.5662,\n","         -0.5853, -0.4657],\n","        [ 0.5510,  0.4289,  0.2900, -0.4979,  0.2839, -0.2938,  0.4673, -0.5471,\n","         -0.5424, -0.4429],\n","        [-0.6301, -0.4887, -0.3278,  0.5681, -0.3139,  0.3303, -0.5734,  0.6069,\n","          0.5653,  0.4735]])\n","linear_hidden.0.bias tensor([-0.2080, -0.2279, -0.3329,  0.2389, -0.4281, -0.2198,  0.2407,  0.1982,\n","         0.2089, -0.3040])\n","linear_output.weight tensor([[ 1.1483,  1.0846,  1.1064, -1.0196,  0.6313,  1.0489, -1.0152, -1.0935,\n","         -1.0236,  1.1950]])\n","linear_output.bias tensor([0.1157])\n","Testing out: \n","batch_size:  2222\n","train_size:  2995\n","n_epochs:  148\n","lr:  0.044331880259845954\n","weight_decay:  0.0007907805398493902\n","betas0:  0.9961852384100632\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0940, -0.2900, -0.2480],\n","          [ 0.0864,  0.0505, -0.0592],\n","          [-0.3101,  0.2759, -0.1581]]]])\n","conv1.bias tensor([-0.0031])\n","first_linear.weight tensor([[-0.2183, -0.0832,  0.2355,  0.0478,  0.2293, -0.1965, -0.1213, -0.3074,\n","         -0.1922],\n","        [ 0.2740,  0.1020, -0.1038,  0.0590,  0.3172,  0.2089,  0.2545, -0.2054,\n","         -0.1794],\n","        [-0.2512, -0.0870,  0.3286,  0.1415, -0.1882,  0.2411, -0.3315,  0.2052,\n","          0.0592],\n","        [-0.0891,  0.0815, -0.1051, -0.1166, -0.0698,  0.0341, -0.3304, -0.2669,\n","         -0.0786],\n","        [ 0.0856, -0.1014,  0.3098, -0.0762,  0.0185,  0.3283,  0.0123, -0.0757,\n","         -0.1736],\n","        [-0.0933, -0.1593, -0.2898,  0.2140,  0.2224,  0.0414,  0.2512,  0.0623,\n","         -0.2051],\n","        [-0.2069, -0.0349,  0.3260,  0.3202, -0.0214,  0.3202,  0.2220,  0.2865,\n","         -0.2244],\n","        [ 0.1464,  0.2615, -0.1675, -0.2800,  0.1606,  0.2081,  0.1051,  0.2572,\n","         -0.0915],\n","        [ 0.1021,  0.2618, -0.0494,  0.1732, -0.2561,  0.3122, -0.3281, -0.3027,\n","         -0.1339],\n","        [ 0.3084, -0.1671,  0.0744,  0.1351,  0.0504, -0.0936,  0.1326, -0.2264,\n","          0.1667]])\n","first_linear.bias tensor([ 0.3243, -0.0552, -0.2158,  0.0871,  0.1816,  0.2629,  0.2406, -0.2088,\n","        -0.1191,  0.3285])\n","linear_hidden.0.weight tensor([[-0.2153,  0.2267,  0.2186, -0.2972, -0.1608, -0.2985, -0.2499,  0.1731,\n","         -0.2064, -0.0595],\n","        [-0.0360,  0.2972,  0.2803,  0.1253, -0.0222, -0.1254, -0.2334,  0.0751,\n","          0.1993,  0.2291],\n","        [ 0.1092, -0.0787, -0.0630,  0.1475,  0.2672, -0.0680, -0.0124, -0.2460,\n","         -0.1348, -0.3132],\n","        [ 0.1788, -0.1033, -0.0790, -0.1576, -0.0117, -0.1327, -0.2464,  0.1063,\n","         -0.0432, -0.2410],\n","        [-0.1060,  0.0888,  0.1841,  0.1524, -0.0183, -0.1077,  0.0351,  0.1406,\n","          0.1707, -0.1546],\n","        [-0.1133, -0.2935,  0.1308,  0.1514,  0.2291, -0.1203, -0.0070,  0.2151,\n","          0.2974,  0.1641],\n","        [ 0.0844, -0.0736,  0.2627, -0.3157,  0.2617, -0.0165,  0.1312,  0.1022,\n","         -0.3015,  0.0270],\n","        [-0.2454, -0.1343, -0.2384, -0.3127,  0.0927, -0.3015,  0.0679,  0.2777,\n","         -0.0229,  0.2316],\n","        [-0.1552, -0.1871,  0.2315,  0.1545, -0.1703,  0.0109,  0.2607, -0.1794,\n","          0.1034,  0.2206],\n","        [-0.1933,  0.0100,  0.0032,  0.1714,  0.1246, -0.2401, -0.0554, -0.0953,\n","         -0.0472, -0.0797]])\n","linear_hidden.0.bias tensor([-0.1019, -0.2921, -0.1642,  0.0213, -0.0996,  0.0696, -0.0479,  0.2332,\n","         0.1730,  0.0758])\n","linear_output.weight tensor([[ 0.2898, -0.0367,  0.1958,  0.2194, -0.2022,  0.2786,  0.2032,  0.0887,\n","          0.1670,  0.0026]])\n","linear_output.bias tensor([-0.2193])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6964486837387085 \t Validate_Accuracy: 0.5325\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2222])) that is different to the input size (torch.Size([2222, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([773])) that is different to the input size (torch.Size([773, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6924384534358978 \t Validate_Accuracy: 0.5015\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6871255934238434 \t Validate_Accuracy: 0.59\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6752354502677917 \t Validate_Accuracy: 0.667\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6584599614143372 \t Validate_Accuracy: 0.6585\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6295489072799683 \t Validate_Accuracy: 0.7225\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.5873101055622101 \t Validate_Accuracy: 0.7565\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5465005040168762 \t Validate_Accuracy: 0.783\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5024086236953735 \t Validate_Accuracy: 0.79\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.4566221535205841 \t Validate_Accuracy: 0.801\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.4166407436132431 \t Validate_Accuracy: 0.821\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.38885578513145447 \t Validate_Accuracy: 0.8335\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.3721844255924225 \t Validate_Accuracy: 0.8335\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.34987665712833405 \t Validate_Accuracy: 0.8435\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.3286677598953247 \t Validate_Accuracy: 0.843\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.32383105158805847 \t Validate_Accuracy: 0.8575\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3120874911546707 \t Validate_Accuracy: 0.858\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.29731595516204834 \t Validate_Accuracy: 0.853\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.2911135405302048 \t Validate_Accuracy: 0.8605\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.2724856436252594 \t Validate_Accuracy: 0.864\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.28539037704467773 \t Validate_Accuracy: 0.8675\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.2774944603443146 \t Validate_Accuracy: 0.865\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.26030437648296356 \t Validate_Accuracy: 0.866\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.2581281363964081 \t Validate_Accuracy: 0.868\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.25960519909858704 \t Validate_Accuracy: 0.8625\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.2482449933886528 \t Validate_Accuracy: 0.8685\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.2548590302467346 \t Validate_Accuracy: 0.875\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.25383729487657547 \t Validate_Accuracy: 0.874\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.24368178099393845 \t Validate_Accuracy: 0.874\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2410902976989746 \t Validate_Accuracy: 0.869\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.23854009062051773 \t Validate_Accuracy: 0.868\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.23640133440494537 \t Validate_Accuracy: 0.877\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.2411952242255211 \t Validate_Accuracy: 0.8765\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.23952357470989227 \t Validate_Accuracy: 0.874\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.22875458002090454 \t Validate_Accuracy: 0.8745\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.23937330394983292 \t Validate_Accuracy: 0.878\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.2543392702937126 \t Validate_Accuracy: 0.87\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.249372698366642 \t Validate_Accuracy: 0.8785\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.2385951206088066 \t Validate_Accuracy: 0.877\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.24097229540348053 \t Validate_Accuracy: 0.8785\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.23257745802402496 \t Validate_Accuracy: 0.88\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.23117252439260483 \t Validate_Accuracy: 0.8815\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.22667016834020615 \t Validate_Accuracy: 0.875\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.23398840427398682 \t Validate_Accuracy: 0.878\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.22023729234933853 \t Validate_Accuracy: 0.876\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2211332693696022 \t Validate_Accuracy: 0.876\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.22105810046195984 \t Validate_Accuracy: 0.8805\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.22201362252235413 \t Validate_Accuracy: 0.882\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.21862781792879105 \t Validate_Accuracy: 0.8855\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.22460700571537018 \t Validate_Accuracy: 0.879\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2239452451467514 \t Validate_Accuracy: 0.8795\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.235650435090065 \t Validate_Accuracy: 0.884\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.236369788646698 \t Validate_Accuracy: 0.885\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.22039416432380676 \t Validate_Accuracy: 0.89\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.22417176514863968 \t Validate_Accuracy: 0.88\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.22098936140537262 \t Validate_Accuracy: 0.881\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.22645455598831177 \t Validate_Accuracy: 0.877\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.23007967323064804 \t Validate_Accuracy: 0.885\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.22795553505420685 \t Validate_Accuracy: 0.8805\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.23296960443258286 \t Validate_Accuracy: 0.8855\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.21912183612585068 \t Validate_Accuracy: 0.879\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.21881744265556335 \t Validate_Accuracy: 0.8815\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.21642424911260605 \t Validate_Accuracy: 0.877\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.21767394244670868 \t Validate_Accuracy: 0.8835\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.22730548679828644 \t Validate_Accuracy: 0.878\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.21989226341247559 \t Validate_Accuracy: 0.879\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.2135234698653221 \t Validate_Accuracy: 0.877\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.23181437700986862 \t Validate_Accuracy: 0.8845\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.21833401173353195 \t Validate_Accuracy: 0.885\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.21732815355062485 \t Validate_Accuracy: 0.8845\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.21061444282531738 \t Validate_Accuracy: 0.8815\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.21973247081041336 \t Validate_Accuracy: 0.881\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.21933532506227493 \t Validate_Accuracy: 0.8805\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.21753783524036407 \t Validate_Accuracy: 0.88\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2306220456957817 \t Validate_Accuracy: 0.885\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2136566862463951 \t Validate_Accuracy: 0.8825\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.21912992745637894 \t Validate_Accuracy: 0.882\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.21704794466495514 \t Validate_Accuracy: 0.8865\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2229400798678398 \t Validate_Accuracy: 0.879\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.21392561495304108 \t Validate_Accuracy: 0.883\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.22339320927858353 \t Validate_Accuracy: 0.8805\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.2203790321946144 \t Validate_Accuracy: 0.881\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2262173518538475 \t Validate_Accuracy: 0.881\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.2138797715306282 \t Validate_Accuracy: 0.878\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.20943481475114822 \t Validate_Accuracy: 0.875\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.2220865786075592 \t Validate_Accuracy: 0.8785\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.22191311419010162 \t Validate_Accuracy: 0.8835\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.20454611629247665 \t Validate_Accuracy: 0.8825\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2098793089389801 \t Validate_Accuracy: 0.8815\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.21010107547044754 \t Validate_Accuracy: 0.8795\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.20976080745458603 \t Validate_Accuracy: 0.886\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.20773570984601974 \t Validate_Accuracy: 0.8855\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.20585603266954422 \t Validate_Accuracy: 0.8855\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.21102116256952286 \t Validate_Accuracy: 0.8855\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.20887994021177292 \t Validate_Accuracy: 0.8845\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.20742420107126236 \t Validate_Accuracy: 0.8825\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.209100142121315 \t Validate_Accuracy: 0.8885\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.20419161766767502 \t Validate_Accuracy: 0.885\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.2109088972210884 \t Validate_Accuracy: 0.881\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.20740947127342224 \t Validate_Accuracy: 0.888\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.20416872203350067 \t Validate_Accuracy: 0.8855\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.20571131259202957 \t Validate_Accuracy: 0.8835\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.1993117779493332 \t Validate_Accuracy: 0.8855\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2173885479569435 \t Validate_Accuracy: 0.8925\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.201689213514328 \t Validate_Accuracy: 0.8925\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.20093884319067 \t Validate_Accuracy: 0.8885\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.20097052305936813 \t Validate_Accuracy: 0.8885\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.20773542672395706 \t Validate_Accuracy: 0.8915\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.20700766891241074 \t Validate_Accuracy: 0.879\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.20770808309316635 \t Validate_Accuracy: 0.889\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.205914668738842 \t Validate_Accuracy: 0.8895\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.20494332909584045 \t Validate_Accuracy: 0.8835\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.20738564431667328 \t Validate_Accuracy: 0.887\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.20527295768260956 \t Validate_Accuracy: 0.885\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.20204099267721176 \t Validate_Accuracy: 0.887\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.19523286819458008 \t Validate_Accuracy: 0.8875\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.19570796936750412 \t Validate_Accuracy: 0.878\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.22273491322994232 \t Validate_Accuracy: 0.8915\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.1939309909939766 \t Validate_Accuracy: 0.8805\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.20443905889987946 \t Validate_Accuracy: 0.885\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.20261459797620773 \t Validate_Accuracy: 0.8795\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21990887820720673 \t Validate_Accuracy: 0.8815\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.21803908050060272 \t Validate_Accuracy: 0.878\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.21952664852142334 \t Validate_Accuracy: 0.8885\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.21728824824094772 \t Validate_Accuracy: 0.888\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2032494693994522 \t Validate_Accuracy: 0.8865\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.20865848660469055 \t Validate_Accuracy: 0.885\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.22206426411867142 \t Validate_Accuracy: 0.8855\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.21393733471632004 \t Validate_Accuracy: 0.8885\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.20076052844524384 \t Validate_Accuracy: 0.8935\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.20405790954828262 \t Validate_Accuracy: 0.8865\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.2022516056895256 \t Validate_Accuracy: 0.8855\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.21345394849777222 \t Validate_Accuracy: 0.8875\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.19605228304862976 \t Validate_Accuracy: 0.891\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.20371628552675247 \t Validate_Accuracy: 0.8905\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.20670408755540848 \t Validate_Accuracy: 0.883\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.210835799574852 \t Validate_Accuracy: 0.8915\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.1960696578025818 \t Validate_Accuracy: 0.8845\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.19320764392614365 \t Validate_Accuracy: 0.8885\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.2006923034787178 \t Validate_Accuracy: 0.888\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2025637924671173 \t Validate_Accuracy: 0.8935\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.18793929368257523 \t Validate_Accuracy: 0.891\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.19141093641519547 \t Validate_Accuracy: 0.8925\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.1964675709605217 \t Validate_Accuracy: 0.89\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2015531063079834 \t Validate_Accuracy: 0.8855\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.1917014718055725 \t Validate_Accuracy: 0.8865\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.19481933116912842 \t Validate_Accuracy: 0.8875\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.19168910384178162 \t Validate_Accuracy: 0.8915\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1818, -0.0648, -0.2128],\n","          [-0.0828,  0.0168, -0.0468],\n","          [-0.1610, -0.0460, -0.1818]]]])\n","conv1.bias tensor([0.0384])\n","first_linear.weight tensor([[ 1.3631e-01, -3.5348e-01, -3.0982e-01, -1.7387e-01, -2.0987e-01,\n","         -5.1452e-01, -2.7911e-01, -1.2520e-03, -3.7606e-01],\n","        [ 6.8370e-01, -8.8666e-01,  5.1046e-01, -5.9898e-01,  8.9143e-01,\n","         -4.2300e-01, -2.9528e-01,  4.9210e-01, -2.2266e-01],\n","        [ 8.8785e-02, -4.4833e-01, -4.3117e-02,  7.8685e-01, -1.0929e-01,\n","          1.0658e+00, -2.1187e-01,  3.2731e-02, -2.8944e-01],\n","        [-3.8225e-01, -1.7306e-01, -3.3698e-01, -4.5040e-02, -2.5137e-01,\n","         -2.5227e-01, -2.4492e-01,  6.7336e-03, -1.0676e-01],\n","        [ 7.0412e-01, -1.2867e-01, -4.9376e-01, -9.3903e-01, -3.7881e-02,\n","          1.2472e+00,  4.2234e-01,  3.6968e-01, -8.7025e-01],\n","        [ 3.2484e-01,  3.6207e-02,  1.9493e-01,  2.5966e-02,  1.9563e-01,\n","          1.8266e-01,  2.3924e-01,  2.8798e-01,  3.2119e-01],\n","        [-2.2284e-01,  1.0777e-01,  5.6912e-02,  1.0296e+00, -3.3306e-01,\n","         -6.0537e-02, -1.3142e+00,  1.3758e+00, -5.1636e-01],\n","        [ 1.3545e-01,  9.5474e-01,  1.5064e-03, -8.6432e-01,  3.6723e-01,\n","          3.3976e-02, -1.0974e-01,  3.7525e-01, -1.1189e-01],\n","        [ 1.8063e-01,  4.4438e-01, -1.4226e-01,  1.8018e-01, -1.2903e+00,\n","          4.9876e-01, -2.0451e-01,  8.8727e-01, -3.6281e-01],\n","        [ 7.4509e-01, -9.5135e-01,  9.3589e-01, -1.2587e-01, -7.5504e-01,\n","         -5.2943e-01,  5.1428e-03,  3.8690e-01,  3.5266e-01]])\n","first_linear.bias tensor([ 0.6248, -0.8992, -0.6438,  1.2571,  0.9927,  1.1468,  1.1047, -0.1006,\n","        -0.7700,  1.1021])\n","linear_hidden.0.weight tensor([[-0.1267,  0.1606,  0.1214, -0.3638, -0.1975, -0.4169, -0.1719,  0.0188,\n","          0.1631, -0.1699],\n","        [ 0.1613,  0.2818,  0.4030,  0.7960, -0.5108,  0.4400, -0.5516, -0.0494,\n","          0.7313, -0.4509],\n","        [ 0.4576, -0.4308, -0.9468, -0.1160, -0.1178, -0.3272,  0.4304, -0.8470,\n","         -0.2016,  0.0231],\n","        [-0.1710, -0.4791, -0.2791, -0.2983,  0.3367, -0.4587,  0.3486, -0.0084,\n","         -0.4816,  0.1427],\n","        [-0.2662, -0.5993, -0.0128,  0.6949, -0.8311,  0.2343, -0.3739, -0.0562,\n","          1.2059, -0.7931],\n","        [-0.5043, -1.0039, -0.3180,  0.0523,  0.9904, -0.5910,  0.6098,  0.1713,\n","          0.3899, -0.2231],\n","        [-0.2175, -0.7427, -0.0358, -0.8525,  0.7064, -0.5987,  0.7206, -0.2704,\n","         -0.9047,  0.5825],\n","        [-0.2323, -0.6221, -1.0839, -0.5322,  0.4626, -0.3313,  0.2628,  1.1092,\n","         -0.5518,  0.6237],\n","        [-0.1403, -0.9487,  0.9015,  0.0941,  0.2474, -0.4273,  0.8876, -1.1180,\n","         -0.2626,  0.6669],\n","        [ 0.5889,  0.0237, -0.1932,  0.4949, -0.0518,  0.2821, -0.2092, -0.3883,\n","          0.2562, -0.2307]])\n","linear_hidden.0.bias tensor([-0.6308,  0.6410, -1.0973, -0.5630,  0.2778, -0.2692, -0.8067, -0.3314,\n","        -0.0907,  0.7012])\n","linear_output.weight tensor([[ 1.3391, -1.4275,  1.6605,  1.1082, -1.5312,  1.4579,  2.1108,  1.8328,\n","          1.7085, -1.2670]])\n","linear_output.bias tensor([-0.9475])\n","Testing out: \n","batch_size:  2264\n","train_size:  3191\n","n_epochs:  203\n","lr:  0.02481608773444175\n","weight_decay:  0.00042484457288295897\n","betas0:  0.9196572522079607\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0937,  0.1456,  0.3113],\n","          [-0.1155, -0.0120,  0.2456],\n","          [ 0.2636, -0.0967, -0.2220]]]])\n","conv1.bias tensor([-0.0185])\n","first_linear.weight tensor([[ 0.3269, -0.3141, -0.1344,  0.2100, -0.1869,  0.0191, -0.3155,  0.1197,\n","          0.0649],\n","        [-0.1055, -0.3274,  0.2167, -0.0319, -0.0573,  0.2767,  0.2116, -0.0567,\n","         -0.1850],\n","        [ 0.3102,  0.2770, -0.0160,  0.2667, -0.1009,  0.3261,  0.0243,  0.0276,\n","         -0.0624],\n","        [ 0.2511, -0.0929,  0.1224, -0.1767,  0.1260, -0.2261, -0.1011,  0.1906,\n","          0.1476],\n","        [-0.0843, -0.2910,  0.0893,  0.1304,  0.3327, -0.1223, -0.1721,  0.1543,\n","         -0.2871],\n","        [-0.2664,  0.2777, -0.0731,  0.1111, -0.0536,  0.2522,  0.0858,  0.2153,\n","          0.2265],\n","        [-0.0518,  0.1021, -0.0385,  0.2168, -0.2181, -0.0538, -0.3326,  0.3027,\n","         -0.0770],\n","        [ 0.2953, -0.2615, -0.1356, -0.0020, -0.1428,  0.0616, -0.3123, -0.1670,\n","          0.1564],\n","        [ 0.0364,  0.2593, -0.0782, -0.2853, -0.2969, -0.1886,  0.2161,  0.2439,\n","          0.2041],\n","        [-0.0645,  0.1465, -0.3178,  0.1921,  0.0775, -0.2227,  0.0117, -0.0116,\n","         -0.3195]])\n","first_linear.bias tensor([ 0.2039, -0.3146, -0.0469,  0.0494, -0.2413,  0.1971,  0.0359,  0.2181,\n","         0.2665,  0.1211])\n","linear_hidden.0.weight tensor([[-0.1994,  0.0424, -0.2787,  0.1389,  0.1041,  0.2685,  0.2925,  0.0205,\n","         -0.0356, -0.1286],\n","        [ 0.0601,  0.3152,  0.2515,  0.0113,  0.1170,  0.0973,  0.0492, -0.1838,\n","          0.2244,  0.0867],\n","        [-0.0449, -0.0240,  0.1188, -0.1368, -0.1395,  0.2013, -0.1762, -0.2026,\n","          0.2743, -0.0981],\n","        [ 0.2002, -0.0187, -0.2638, -0.2742,  0.1919,  0.1008, -0.0290,  0.2606,\n","          0.2137, -0.0528],\n","        [-0.0428,  0.0295,  0.0462, -0.0428,  0.1926,  0.1306, -0.1018, -0.2826,\n","         -0.0033, -0.1105],\n","        [-0.1291,  0.0007, -0.2478, -0.3125, -0.2206,  0.0503,  0.2460, -0.2925,\n","         -0.0523,  0.2628],\n","        [-0.2141, -0.1149,  0.2658,  0.2409,  0.0741, -0.3120, -0.1285,  0.1075,\n","         -0.2343, -0.3147],\n","        [-0.3117, -0.1118, -0.0610,  0.0399, -0.0101,  0.1789, -0.0664,  0.0468,\n","          0.0797,  0.2852],\n","        [ 0.2365,  0.0185,  0.0515,  0.0490,  0.1416,  0.1745, -0.2602,  0.0820,\n","         -0.3122, -0.1686],\n","        [ 0.0039, -0.1539,  0.2637,  0.2246,  0.2621, -0.1528,  0.3023,  0.2772,\n","          0.1794,  0.2477]])\n","linear_hidden.0.bias tensor([ 0.2633, -0.2349, -0.2211,  0.2129, -0.1173, -0.1980,  0.1776, -0.2027,\n","        -0.1127,  0.0413])\n","linear_output.weight tensor([[-0.3132, -0.0905,  0.1276, -0.0365, -0.0748, -0.2050, -0.1695, -0.1286,\n","          0.2779, -0.0490]])\n","linear_output.bias tensor([-0.0290])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6947386860847473 \t Validate_Accuracy: 0.5615\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2264])) that is different to the input size (torch.Size([2264, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([927])) that is different to the input size (torch.Size([927, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6913417279720306 \t Validate_Accuracy: 0.512\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6903115510940552 \t Validate_Accuracy: 0.5235\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6870336532592773 \t Validate_Accuracy: 0.574\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6829624474048615 \t Validate_Accuracy: 0.599\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6751400232315063 \t Validate_Accuracy: 0.598\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.663835197687149 \t Validate_Accuracy: 0.6605\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.640969306230545 \t Validate_Accuracy: 0.7025\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6174843907356262 \t Validate_Accuracy: 0.7275\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.5831946730613708 \t Validate_Accuracy: 0.7735\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.5370382964611053 \t Validate_Accuracy: 0.8205\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4897811710834503 \t Validate_Accuracy: 0.8385\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.4442640244960785 \t Validate_Accuracy: 0.8455\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.39919690787792206 \t Validate_Accuracy: 0.8475\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.3697376847267151 \t Validate_Accuracy: 0.847\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3567616641521454 \t Validate_Accuracy: 0.849\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.34048087894916534 \t Validate_Accuracy: 0.853\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3334212154150009 \t Validate_Accuracy: 0.8525\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.310920849442482 \t Validate_Accuracy: 0.8535\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.29962053894996643 \t Validate_Accuracy: 0.859\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.2883616089820862 \t Validate_Accuracy: 0.862\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.2713227719068527 \t Validate_Accuracy: 0.864\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.26248450577259064 \t Validate_Accuracy: 0.8685\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.24673306196928024 \t Validate_Accuracy: 0.8675\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.24810538440942764 \t Validate_Accuracy: 0.8775\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.2414638251066208 \t Validate_Accuracy: 0.8825\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.24006417393684387 \t Validate_Accuracy: 0.875\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.2385607287287712 \t Validate_Accuracy: 0.8795\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.22669754922389984 \t Validate_Accuracy: 0.884\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2248045578598976 \t Validate_Accuracy: 0.885\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.22856760770082474 \t Validate_Accuracy: 0.885\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.2257574424147606 \t Validate_Accuracy: 0.8905\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.22175383567810059 \t Validate_Accuracy: 0.8875\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.217584528028965 \t Validate_Accuracy: 0.887\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.2194160297513008 \t Validate_Accuracy: 0.8865\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.22432861477136612 \t Validate_Accuracy: 0.89\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.21585191786289215 \t Validate_Accuracy: 0.8845\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.22133325785398483 \t Validate_Accuracy: 0.8885\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.21545543521642685 \t Validate_Accuracy: 0.882\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.21636588126420975 \t Validate_Accuracy: 0.885\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2153630033135414 \t Validate_Accuracy: 0.885\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.20975887030363083 \t Validate_Accuracy: 0.888\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2042999193072319 \t Validate_Accuracy: 0.8845\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2135791853070259 \t Validate_Accuracy: 0.8875\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2170908823609352 \t Validate_Accuracy: 0.887\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.21093923598527908 \t Validate_Accuracy: 0.8845\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.20301194489002228 \t Validate_Accuracy: 0.883\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.21242797374725342 \t Validate_Accuracy: 0.8845\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.20516851544380188 \t Validate_Accuracy: 0.8865\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.21447353810071945 \t Validate_Accuracy: 0.8855\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2081897333264351 \t Validate_Accuracy: 0.887\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.20649321377277374 \t Validate_Accuracy: 0.8845\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.20334121584892273 \t Validate_Accuracy: 0.8825\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.20696308463811874 \t Validate_Accuracy: 0.888\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.20850348472595215 \t Validate_Accuracy: 0.888\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.20745734870433807 \t Validate_Accuracy: 0.888\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.20565100759267807 \t Validate_Accuracy: 0.887\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.20243629813194275 \t Validate_Accuracy: 0.8885\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.21320483088493347 \t Validate_Accuracy: 0.8895\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2052670568227768 \t Validate_Accuracy: 0.887\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.21088393777608871 \t Validate_Accuracy: 0.885\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.21040961891412735 \t Validate_Accuracy: 0.885\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.20648078620433807 \t Validate_Accuracy: 0.8905\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.19943980127573013 \t Validate_Accuracy: 0.8865\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.21041995286941528 \t Validate_Accuracy: 0.886\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.20310164988040924 \t Validate_Accuracy: 0.887\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.20221051573753357 \t Validate_Accuracy: 0.8855\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.20637041330337524 \t Validate_Accuracy: 0.8865\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.21219339221715927 \t Validate_Accuracy: 0.8885\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.20677166432142258 \t Validate_Accuracy: 0.8885\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.19178029894828796 \t Validate_Accuracy: 0.8845\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.20592651516199112 \t Validate_Accuracy: 0.885\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2012861743569374 \t Validate_Accuracy: 0.8855\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.20467643439769745 \t Validate_Accuracy: 0.8855\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.20835226774215698 \t Validate_Accuracy: 0.8915\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.1997549682855606 \t Validate_Accuracy: 0.8865\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.19763104617595673 \t Validate_Accuracy: 0.8895\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.1973348930478096 \t Validate_Accuracy: 0.89\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2026968076825142 \t Validate_Accuracy: 0.8885\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.19683516025543213 \t Validate_Accuracy: 0.8925\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.19611463695764542 \t Validate_Accuracy: 0.885\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.19724448025226593 \t Validate_Accuracy: 0.889\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.20131974667310715 \t Validate_Accuracy: 0.89\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.1929984986782074 \t Validate_Accuracy: 0.8915\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.19434454292058945 \t Validate_Accuracy: 0.8925\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.19582413882017136 \t Validate_Accuracy: 0.89\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.19380954653024673 \t Validate_Accuracy: 0.8885\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.19780321419239044 \t Validate_Accuracy: 0.887\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.20244649797677994 \t Validate_Accuracy: 0.891\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.18970556557178497 \t Validate_Accuracy: 0.893\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.19648635387420654 \t Validate_Accuracy: 0.89\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.19020743668079376 \t Validate_Accuracy: 0.892\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.19489546865224838 \t Validate_Accuracy: 0.889\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.1903863102197647 \t Validate_Accuracy: 0.892\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.1954205259680748 \t Validate_Accuracy: 0.8935\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.19651531428098679 \t Validate_Accuracy: 0.8945\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.1874481588602066 \t Validate_Accuracy: 0.888\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.19644229859113693 \t Validate_Accuracy: 0.891\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.1907351240515709 \t Validate_Accuracy: 0.888\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.1892852932214737 \t Validate_Accuracy: 0.8915\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.1922219768166542 \t Validate_Accuracy: 0.8925\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.18835963308811188 \t Validate_Accuracy: 0.8915\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.19372370839118958 \t Validate_Accuracy: 0.8975\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.19107354432344437 \t Validate_Accuracy: 0.8985\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.1868293583393097 \t Validate_Accuracy: 0.8985\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.18636055290699005 \t Validate_Accuracy: 0.894\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.19180172681808472 \t Validate_Accuracy: 0.8965\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.18474312126636505 \t Validate_Accuracy: 0.8945\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.1897963583469391 \t Validate_Accuracy: 0.897\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.1813691481947899 \t Validate_Accuracy: 0.8985\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.18788960576057434 \t Validate_Accuracy: 0.8995\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.1795470416545868 \t Validate_Accuracy: 0.8985\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.19001854956150055 \t Validate_Accuracy: 0.897\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.18876339495182037 \t Validate_Accuracy: 0.9005\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.1907733455300331 \t Validate_Accuracy: 0.8975\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.18208006024360657 \t Validate_Accuracy: 0.899\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.19058044254779816 \t Validate_Accuracy: 0.9\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.18246876448392868 \t Validate_Accuracy: 0.898\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.18961917608976364 \t Validate_Accuracy: 0.9\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.18168765306472778 \t Validate_Accuracy: 0.899\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.18835212290287018 \t Validate_Accuracy: 0.8915\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.1896256059408188 \t Validate_Accuracy: 0.8975\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.18454011529684067 \t Validate_Accuracy: 0.8985\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.18311707675457 \t Validate_Accuracy: 0.9005\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.18717936426401138 \t Validate_Accuracy: 0.901\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.19066251814365387 \t Validate_Accuracy: 0.898\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.1835361197590828 \t Validate_Accuracy: 0.899\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.19405879080295563 \t Validate_Accuracy: 0.8965\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.1845593824982643 \t Validate_Accuracy: 0.8975\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.17596766352653503 \t Validate_Accuracy: 0.904\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.18736890703439713 \t Validate_Accuracy: 0.903\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.1841193586587906 \t Validate_Accuracy: 0.8865\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.17926404625177383 \t Validate_Accuracy: 0.8985\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.18571806699037552 \t Validate_Accuracy: 0.895\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.17953336983919144 \t Validate_Accuracy: 0.895\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.1831938773393631 \t Validate_Accuracy: 0.894\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.18572358042001724 \t Validate_Accuracy: 0.8985\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.18564312160015106 \t Validate_Accuracy: 0.888\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.1962265968322754 \t Validate_Accuracy: 0.8855\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.20877721160650253 \t Validate_Accuracy: 0.8885\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2015286162495613 \t Validate_Accuracy: 0.888\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.2070566862821579 \t Validate_Accuracy: 0.897\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.18264367431402206 \t Validate_Accuracy: 0.896\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.19000539928674698 \t Validate_Accuracy: 0.8865\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.19806933403015137 \t Validate_Accuracy: 0.9005\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.19631189107894897 \t Validate_Accuracy: 0.899\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.19547154009342194 \t Validate_Accuracy: 0.891\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.18369948863983154 \t Validate_Accuracy: 0.8915\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.19742178916931152 \t Validate_Accuracy: 0.89\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.18516279757022858 \t Validate_Accuracy: 0.8895\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.18405689299106598 \t Validate_Accuracy: 0.8935\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.18350381404161453 \t Validate_Accuracy: 0.896\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.19046101719141006 \t Validate_Accuracy: 0.8895\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.1853088140487671 \t Validate_Accuracy: 0.898\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.1903885379433632 \t Validate_Accuracy: 0.892\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.17961431294679642 \t Validate_Accuracy: 0.9005\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.17635363340377808 \t Validate_Accuracy: 0.9005\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.18009667843580246 \t Validate_Accuracy: 0.893\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.17875966429710388 \t Validate_Accuracy: 0.9005\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.18440166115760803 \t Validate_Accuracy: 0.895\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.18585991114377975 \t Validate_Accuracy: 0.8975\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.18214398622512817 \t Validate_Accuracy: 0.901\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.18393834680318832 \t Validate_Accuracy: 0.8925\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.1829933300614357 \t Validate_Accuracy: 0.898\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.18767542392015457 \t Validate_Accuracy: 0.9005\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.17810341715812683 \t Validate_Accuracy: 0.8985\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.17761237919330597 \t Validate_Accuracy: 0.897\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.17925281822681427 \t Validate_Accuracy: 0.9\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.17985190451145172 \t Validate_Accuracy: 0.8975\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.1745157167315483 \t Validate_Accuracy: 0.898\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.17699932307004929 \t Validate_Accuracy: 0.8995\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.1810087487101555 \t Validate_Accuracy: 0.8995\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.17642360925674438 \t Validate_Accuracy: 0.8995\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.18592599034309387 \t Validate_Accuracy: 0.9025\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.18461866676807404 \t Validate_Accuracy: 0.898\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.180923193693161 \t Validate_Accuracy: 0.898\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.18039079755544662 \t Validate_Accuracy: 0.9005\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.1831350401043892 \t Validate_Accuracy: 0.9015\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.18310609459877014 \t Validate_Accuracy: 0.8965\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.1870652586221695 \t Validate_Accuracy: 0.8995\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.1836089789867401 \t Validate_Accuracy: 0.901\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.18545135855674744 \t Validate_Accuracy: 0.9005\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.18064017593860626 \t Validate_Accuracy: 0.8995\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.18044216185808182 \t Validate_Accuracy: 0.899\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.18740936368703842 \t Validate_Accuracy: 0.902\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.18765466660261154 \t Validate_Accuracy: 0.898\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.18147920817136765 \t Validate_Accuracy: 0.8965\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.18383877724409103 \t Validate_Accuracy: 0.903\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.1825145184993744 \t Validate_Accuracy: 0.896\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.18587534874677658 \t Validate_Accuracy: 0.9035\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.1848142370581627 \t Validate_Accuracy: 0.899\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.1911112740635872 \t Validate_Accuracy: 0.9005\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.18616099655628204 \t Validate_Accuracy: 0.9\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.19332482665777206 \t Validate_Accuracy: 0.897\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.18408985435962677 \t Validate_Accuracy: 0.9\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.18843938410282135 \t Validate_Accuracy: 0.8915\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.18461577594280243 \t Validate_Accuracy: 0.8995\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.18235792964696884 \t Validate_Accuracy: 0.897\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.18075546622276306 \t Validate_Accuracy: 0.899\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.17788636684417725 \t Validate_Accuracy: 0.902\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.17849598079919815 \t Validate_Accuracy: 0.9005\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.18102572858333588 \t Validate_Accuracy: 0.9005\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.17877843976020813 \t Validate_Accuracy: 0.8985\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1455,  0.0494,  0.1752],\n","          [ 0.0696, -0.0171,  0.0448],\n","          [ 0.1342,  0.0599,  0.1589]]]])\n","conv1.bias tensor([-0.0581])\n","first_linear.weight tensor([[-1.4016e-03, -8.5503e-01,  2.3977e-01,  1.4955e+00, -6.9192e-01,\n","          2.0544e-01, -5.6021e-01,  1.7847e-01,  6.4346e-02],\n","        [ 5.6004e-01, -1.3635e-02, -5.1644e-01, -2.4937e-01, -5.7726e-01,\n","          1.5595e+00,  2.4086e-01,  5.1484e-01, -9.1293e-01],\n","        [ 1.7116e-01,  6.4583e-02,  2.0327e-01, -1.0082e-01,  7.0347e-02,\n","          9.1717e-02,  1.1629e-01,  1.1402e-01,  3.8797e-02],\n","        [ 7.1705e-01, -7.9954e-01,  5.3528e-01, -6.9281e-01, -3.6680e-02,\n","         -3.9122e-01,  1.6491e-02, -1.1475e-01,  8.6868e-01],\n","        [ 1.6066e-02, -1.0140e+00,  8.9894e-01, -3.3794e-02,  8.5329e-01,\n","         -5.2163e-01,  2.4361e-01, -1.3076e-02, -5.7071e-01],\n","        [ 4.0272e-01,  3.9630e-01,  3.2298e-02, -1.5298e-01,  2.7296e-01,\n","          8.9595e-01,  4.4354e-01,  1.9315e-01,  1.7255e-01],\n","        [-2.3340e-01,  3.5650e-01,  1.4905e-01,  7.9714e-01, -6.5977e-01,\n","         -3.1318e-01, -1.3025e+00,  1.4039e+00, -4.7055e-01],\n","        [ 7.1240e-01, -8.4041e-01, -2.2478e-01, -4.2872e-01,  1.3336e+00,\n","          1.4664e-01, -6.4018e-01,  1.5558e-01, -3.0694e-01],\n","        [ 6.5299e-01, -2.8264e-01,  4.4774e-01, -7.5512e-01, -5.7780e-01,\n","         -5.4182e-02, -5.8642e-02,  7.5768e-01, -4.1572e-01],\n","        [-6.9758e-01,  4.1212e-01,  1.3722e-01, -4.3602e-01, -3.7005e-01,\n","         -5.7289e-01,  6.5290e-02,  2.5908e-01, -5.2717e-01]])\n","first_linear.bias tensor([ 0.9473, -1.0239, -1.3008, -0.6712, -0.8939, -0.6269, -1.0689,  0.8899,\n","         0.8522, -0.2532])\n","linear_hidden.0.weight tensor([[-0.7913,  0.5655, -1.1367,  0.4445,  0.6025, -0.3006,  0.6161, -0.4582,\n","         -0.6055, -0.5149],\n","        [-0.0961,  0.9580, -0.5304, -0.4983,  0.1859, -0.1787,  0.7866, -0.5304,\n","         -0.5730, -0.2013],\n","        [-0.1577,  0.0749,  0.5188,  0.1636,  0.1008,  0.1721,  0.2120, -0.1541,\n","         -0.1567,  0.0478],\n","        [ 0.1585, -0.0944, -0.4880, -0.1978, -0.1007, -0.1537, -0.2166,  0.1774,\n","          0.1634, -0.0908],\n","        [-0.7162,  0.6397, -0.9747,  0.3508,  0.6980, -0.3595,  0.1751, -0.3378,\n","         -0.5027, -0.5763],\n","        [-0.3493,  0.5273, -0.4979, -0.7797, -0.2849,  0.0903,  0.6639, -0.6284,\n","         -0.8201, -0.0227],\n","        [-0.8657,  0.5324, -0.6603,  0.7970,  0.7397, -0.7416,  0.4914,  0.3273,\n","         -0.5165, -0.5622],\n","        [-0.7408,  0.7027, -0.6599, -0.7100, -0.2591,  0.0429,  0.0279, -0.5632,\n","         -0.3798,  0.2558],\n","        [ 0.6667, -0.3999,  1.0121, -0.7653, -0.4157,  0.7081, -0.8005,  0.7983,\n","          0.4082,  0.4960],\n","        [ 0.5814, -0.4696,  0.9799, -0.5960, -0.4815,  0.4636, -0.5802,  0.6249,\n","          0.5195,  0.4144]])\n","linear_hidden.0.bias tensor([ 1.1253,  0.4712, -0.6337,  0.5824,  0.8829,  0.0185,  0.7362,  0.1272,\n","        -0.9498, -0.9737])\n","linear_output.weight tensor([[-2.0260, -1.1309,  1.1372, -1.0467, -1.8061, -1.1587, -1.7616, -1.0089,\n","          2.0058,  1.8578]])\n","linear_output.bias tensor([-0.8992])\n","Testing out: \n","batch_size:  1897\n","train_size:  1935\n","n_epochs:  135\n","lr:  0.02114014687020053\n","weight_decay:  0.0029532526094495004\n","betas0:  0.9997363959529095\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.0379,  0.2476,  0.1271],\n","          [-0.1693, -0.1483, -0.0304],\n","          [ 0.2323,  0.0201,  0.0141]]]])\n","conv1.bias tensor([-0.2833])\n","first_linear.weight tensor([[ 0.0283, -0.0234,  0.3025,  0.2069,  0.0063,  0.2034,  0.1505, -0.2274,\n","          0.1381],\n","        [ 0.1803, -0.3063,  0.1145,  0.1430, -0.2750,  0.0367, -0.1479, -0.1950,\n","         -0.0438],\n","        [-0.0228,  0.2542,  0.1222,  0.3248, -0.0424,  0.2026, -0.1290,  0.1815,\n","         -0.3301],\n","        [ 0.0125,  0.2521, -0.2808,  0.1223,  0.1265, -0.0033, -0.0645,  0.0563,\n","         -0.1607],\n","        [ 0.0654,  0.1580, -0.2640,  0.0618, -0.1320,  0.1725,  0.2852,  0.0480,\n","         -0.2906],\n","        [ 0.2022,  0.2590,  0.1550, -0.0669, -0.3198,  0.2897, -0.0976, -0.0883,\n","         -0.2358],\n","        [ 0.1722, -0.1521,  0.0253,  0.1630,  0.3060,  0.1536,  0.2093, -0.2848,\n","          0.1406],\n","        [-0.2452,  0.0922,  0.1217,  0.1335, -0.2522, -0.1645,  0.3234, -0.0532,\n","          0.1901],\n","        [ 0.1868, -0.2977, -0.2054,  0.2934, -0.0678,  0.2496, -0.1218, -0.3108,\n","          0.0195],\n","        [ 0.0433,  0.3101,  0.1076,  0.2258,  0.0924,  0.1117,  0.0609,  0.1639,\n","         -0.0791]])\n","first_linear.bias tensor([ 0.1183, -0.0251, -0.2915, -0.0464, -0.2954,  0.0310, -0.3287, -0.2059,\n","        -0.1657,  0.0134])\n","linear_hidden.0.weight tensor([[ 0.0767,  0.2943, -0.0229, -0.2087, -0.2093,  0.0591,  0.0143,  0.1107,\n","          0.3048,  0.0515],\n","        [-0.0773, -0.2178,  0.0612, -0.1301,  0.3150,  0.1005,  0.0540, -0.2182,\n","         -0.2857, -0.1330],\n","        [ 0.0346, -0.2224, -0.2598, -0.2887,  0.1519, -0.2332, -0.2366, -0.0137,\n","         -0.1407, -0.2083],\n","        [-0.3044,  0.0740,  0.0405,  0.1707,  0.0500, -0.2672,  0.2191,  0.2839,\n","         -0.1150,  0.0258],\n","        [ 0.2179,  0.1149, -0.2414,  0.0077, -0.2881,  0.0822,  0.2357, -0.2871,\n","          0.2416, -0.2405],\n","        [ 0.2456,  0.2086, -0.1351, -0.1581, -0.2856, -0.0135,  0.1937,  0.1730,\n","          0.2509,  0.2573],\n","        [-0.2393,  0.0209,  0.2190,  0.1867, -0.1461,  0.1361,  0.1510, -0.2665,\n","         -0.2968,  0.1884],\n","        [-0.2799,  0.2108,  0.1729,  0.0969,  0.2699,  0.2785,  0.1761,  0.0172,\n","          0.1175,  0.1461],\n","        [-0.0099, -0.1823,  0.1449,  0.0189, -0.1293, -0.2807, -0.2328, -0.2555,\n","         -0.1728, -0.1756],\n","        [-0.2049, -0.2188,  0.0368,  0.0710, -0.1012,  0.0385,  0.2122,  0.1676,\n","          0.0875,  0.0322]])\n","linear_hidden.0.bias tensor([-0.2098, -0.0902, -0.2737, -0.0507,  0.0322,  0.2342,  0.0474,  0.1372,\n","        -0.2574,  0.1335])\n","linear_output.weight tensor([[ 0.2812,  0.1977,  0.2291,  0.1909, -0.2892,  0.0886, -0.1582, -0.3010,\n","          0.2635, -0.2286]])\n","linear_output.bias tensor([-0.2246])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7064525783061981 \t Validate_Accuracy: 0.4945\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6993853747844696 \t Validate_Accuracy: 0.477\n","epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1897])) that is different to the input size (torch.Size([1897, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3 \t Train Loss: 0.6982326209545135 \t Validate_Accuracy: 0.523\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6907370090484619 \t Validate_Accuracy: 0.503\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.70530766248703 \t Validate_Accuracy: 0.503\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6925854086875916 \t Validate_Accuracy: 0.503\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6915650367736816 \t Validate_Accuracy: 0.5015\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6912851333618164 \t Validate_Accuracy: 0.4995\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6893599927425385 \t Validate_Accuracy: 0.502\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6977558732032776 \t Validate_Accuracy: 0.5025\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.7003382444381714 \t Validate_Accuracy: 0.488\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6911355555057526 \t Validate_Accuracy: 0.4835\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6932529509067535 \t Validate_Accuracy: 0.5715\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6981471478939056 \t Validate_Accuracy: 0.5695\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6928704082965851 \t Validate_Accuracy: 0.557\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6929983198642731 \t Validate_Accuracy: 0.5025\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6918376386165619 \t Validate_Accuracy: 0.5205\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6932549178600311 \t Validate_Accuracy: 0.502\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6933711171150208 \t Validate_Accuracy: 0.503\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6884325742721558 \t Validate_Accuracy: 0.503\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.695764809846878 \t Validate_Accuracy: 0.503\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6904460489749908 \t Validate_Accuracy: 0.503\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6984424889087677 \t Validate_Accuracy: 0.503\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6835587322711945 \t Validate_Accuracy: 0.5065\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.69969242811203 \t Validate_Accuracy: 0.5255\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6973948776721954 \t Validate_Accuracy: 0.562\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6945204138755798 \t Validate_Accuracy: 0.592\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.696934700012207 \t Validate_Accuracy: 0.587\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.691977471113205 \t Validate_Accuracy: 0.491\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6921224892139435 \t Validate_Accuracy: 0.529\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6927477717399597 \t Validate_Accuracy: 0.519\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6942141950130463 \t Validate_Accuracy: 0.5135\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6964668035507202 \t Validate_Accuracy: 0.5265\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6932432651519775 \t Validate_Accuracy: 0.5765\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6933316290378571 \t Validate_Accuracy: 0.4375\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6923989355564117 \t Validate_Accuracy: 0.503\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6949776411056519 \t Validate_Accuracy: 0.503\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.694675624370575 \t Validate_Accuracy: 0.503\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6930858790874481 \t Validate_Accuracy: 0.503\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.6928587853908539 \t Validate_Accuracy: 0.503\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6937941014766693 \t Validate_Accuracy: 0.503\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6932263076305389 \t Validate_Accuracy: 0.496\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6923007369041443 \t Validate_Accuracy: 0.483\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6943854987621307 \t Validate_Accuracy: 0.4585\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6933887600898743 \t Validate_Accuracy: 0.4945\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6923834681510925 \t Validate_Accuracy: 0.5575\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6911873817443848 \t Validate_Accuracy: 0.501\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6915414333343506 \t Validate_Accuracy: 0.497\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.6853399872779846 \t Validate_Accuracy: 0.497\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6953989863395691 \t Validate_Accuracy: 0.497\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6943115890026093 \t Validate_Accuracy: 0.497\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.6894605755805969 \t Validate_Accuracy: 0.497\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6942180693149567 \t Validate_Accuracy: 0.497\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6927144825458527 \t Validate_Accuracy: 0.497\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6949291527271271 \t Validate_Accuracy: 0.497\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.7063108384609222 \t Validate_Accuracy: 0.498\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.6924489736557007 \t Validate_Accuracy: 0.5315\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6934535205364227 \t Validate_Accuracy: 0.5955\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.6932230889797211 \t Validate_Accuracy: 0.4745\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6915911734104156 \t Validate_Accuracy: 0.418\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.6921245753765106 \t Validate_Accuracy: 0.4775\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6924411356449127 \t Validate_Accuracy: 0.503\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6938548982143402 \t Validate_Accuracy: 0.503\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6946047246456146 \t Validate_Accuracy: 0.503\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.6912747323513031 \t Validate_Accuracy: 0.501\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.6920290589332581 \t Validate_Accuracy: 0.4745\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6926320791244507 \t Validate_Accuracy: 0.4485\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6929015815258026 \t Validate_Accuracy: 0.428\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6922654807567596 \t Validate_Accuracy: 0.439\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.6935549378395081 \t Validate_Accuracy: 0.4995\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.6907704770565033 \t Validate_Accuracy: 0.5165\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.6927191317081451 \t Validate_Accuracy: 0.555\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.6907149851322174 \t Validate_Accuracy: 0.5635\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.6913917064666748 \t Validate_Accuracy: 0.554\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.6924548149108887 \t Validate_Accuracy: 0.5535\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.6924190521240234 \t Validate_Accuracy: 0.5715\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.6906368732452393 \t Validate_Accuracy: 0.577\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.6915095746517181 \t Validate_Accuracy: 0.5835\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.6947153508663177 \t Validate_Accuracy: 0.585\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.6905289888381958 \t Validate_Accuracy: 0.532\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.690809816122055 \t Validate_Accuracy: 0.495\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.6954841911792755 \t Validate_Accuracy: 0.435\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.6885032057762146 \t Validate_Accuracy: 0.462\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.6915536224842072 \t Validate_Accuracy: 0.4795\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.6859310269355774 \t Validate_Accuracy: 0.503\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.6857997477054596 \t Validate_Accuracy: 0.503\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.6948375999927521 \t Validate_Accuracy: 0.503\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.6906983256340027 \t Validate_Accuracy: 0.503\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.6920473277568817 \t Validate_Accuracy: 0.503\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.6979093849658966 \t Validate_Accuracy: 0.4815\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.6879361569881439 \t Validate_Accuracy: 0.449\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.6967342495918274 \t Validate_Accuracy: 0.494\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.6863293647766113 \t Validate_Accuracy: 0.583\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.6916529536247253 \t Validate_Accuracy: 0.577\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.6910561621189117 \t Validate_Accuracy: 0.5715\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.6948052942752838 \t Validate_Accuracy: 0.5755\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.6856098771095276 \t Validate_Accuracy: 0.5885\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.6790399551391602 \t Validate_Accuracy: 0.588\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.6840433776378632 \t Validate_Accuracy: 0.5425\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.6875891983509064 \t Validate_Accuracy: 0.531\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.687461256980896 \t Validate_Accuracy: 0.5285\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.6949110925197601 \t Validate_Accuracy: 0.53\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.6835081577301025 \t Validate_Accuracy: 0.5845\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.6677097082138062 \t Validate_Accuracy: 0.518\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.6729931831359863 \t Validate_Accuracy: 0.5235\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.6783351302146912 \t Validate_Accuracy: 0.6015\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.6892027258872986 \t Validate_Accuracy: 0.6\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.6902869939804077 \t Validate_Accuracy: 0.564\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.6760986149311066 \t Validate_Accuracy: 0.5905\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.6822682023048401 \t Validate_Accuracy: 0.6065\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.6760263442993164 \t Validate_Accuracy: 0.6055\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.6756382584571838 \t Validate_Accuracy: 0.5925\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.650892585515976 \t Validate_Accuracy: 0.5045\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.6819842457771301 \t Validate_Accuracy: 0.555\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.6591623723506927 \t Validate_Accuracy: 0.594\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.6695414483547211 \t Validate_Accuracy: 0.5835\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.6577364504337311 \t Validate_Accuracy: 0.4945\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.6463286876678467 \t Validate_Accuracy: 0.5435\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.6869126260280609 \t Validate_Accuracy: 0.57\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.6519500315189362 \t Validate_Accuracy: 0.5475\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.6738960146903992 \t Validate_Accuracy: 0.597\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.6419981718063354 \t Validate_Accuracy: 0.598\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.6631055474281311 \t Validate_Accuracy: 0.57\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.6163608133792877 \t Validate_Accuracy: 0.612\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.6309454143047333 \t Validate_Accuracy: 0.6025\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.5760982930660248 \t Validate_Accuracy: 0.688\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.561546117067337 \t Validate_Accuracy: 0.7805\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.5179354548454285 \t Validate_Accuracy: 0.79\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.4773646146059036 \t Validate_Accuracy: 0.767\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.4896206557750702 \t Validate_Accuracy: 0.7895\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.5471680611371994 \t Validate_Accuracy: 0.7965\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.4884234815835953 \t Validate_Accuracy: 0.7945\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.4016639143228531 \t Validate_Accuracy: 0.8105\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.5311551988124847 \t Validate_Accuracy: 0.8125\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.48287972807884216 \t Validate_Accuracy: 0.801\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.4723,  0.0228,  0.3777],\n","          [ 0.1304, -0.0723,  0.1535],\n","          [ 0.3377, -0.0743,  0.4337]]]])\n","conv1.bias tensor([-0.0832])\n","first_linear.weight tensor([[-0.0359, -0.0862,  0.1621, -0.1620, -0.3028, -0.1621, -0.2253,  0.3945,\n","          0.1561],\n","        [ 0.3345,  0.0123,  0.5300,  0.6530,  0.4217, -0.0295, -0.1682,  0.4090,\n","          0.1893],\n","        [ 0.0150,  0.0632, -0.1609, -0.0331,  0.3380,  0.2024,  0.0208, -0.1783,\n","         -0.0123],\n","        [-0.0820, -0.2681,  0.1097, -0.1855,  0.0101, -0.0487, -0.1161,  0.1287,\n","          0.1217],\n","        [-0.0383,  0.1573, -0.1878,  0.2550,  0.2755,  0.1297, -0.1192, -0.1042,\n","          0.0347],\n","        [ 0.2372,  0.0311,  0.2900,  0.0911,  0.4660,  0.3294, -0.0285,  0.3346,\n","          0.2995],\n","        [-0.1083,  0.8879, -0.5347, -0.8360,  0.3063,  0.2434,  0.9506, -0.4136,\n","         -0.3275],\n","        [-0.0299,  0.3208, -0.3872, -0.7212, -0.1017, -0.0039,  0.1025, -0.3862,\n","         -0.0145],\n","        [-0.4360, -0.6276, -0.2477, -0.4110, -0.5068, -0.2650, -0.3442, -0.2982,\n","         -0.1296],\n","        [ 0.3327,  0.7374,  0.0901,  0.3762,  0.6368,  0.3216,  0.4124,  0.4826,\n","          0.2008]])\n","first_linear.bias tensor([ 0.2965,  0.7644, -0.3716,  0.2247, -0.4926,  0.0756, -1.3778, -0.1131,\n","         1.2529, -1.5952])\n","linear_hidden.0.weight tensor([[ 1.6892e-02, -5.7953e-01,  9.3427e-02,  8.4695e-04,  3.7348e-03,\n","         -1.6644e-02, -4.5953e-01,  2.2780e-01, -3.6159e-01,  5.2675e-01],\n","        [ 1.9657e-03, -5.7264e-01,  1.1750e-01,  1.4889e-02,  2.4679e-02,\n","         -3.4662e-02, -4.7150e-01,  1.9326e-01, -4.0372e-01,  5.5856e-01],\n","        [-4.1607e-03, -4.7954e-01,  1.2176e-01,  8.1686e-02,  2.2615e-02,\n","          1.0439e-01, -3.4714e-01,  1.5349e-01, -2.6309e-01,  3.3560e-01],\n","        [-1.0190e-02, -4.0534e-01,  1.2423e-01,  1.0750e-01,  2.8972e-02,\n","          1.4333e-01, -2.8551e-01,  1.3400e-01, -2.2481e-01,  2.7191e-01],\n","        [-1.6670e-02,  5.4474e-01,  1.3440e-03, -1.0710e-01,  9.7406e-02,\n","          6.8561e-02,  7.7252e-01, -3.5483e-01,  5.0021e-01, -7.5996e-01],\n","        [ 6.8338e-02, -6.0007e-01,  1.4010e-02,  2.9534e-02, -7.0089e-02,\n","         -3.4515e-02, -6.2877e-01,  3.5373e-01, -3.5005e-01,  5.8174e-01],\n","        [ 9.1584e-03,  1.4852e-01, -6.4832e-02, -6.5368e-02, -5.6683e-02,\n","         -1.3182e-01,  1.0986e-01, -4.0412e-02,  7.1681e-02, -8.2966e-02],\n","        [-3.1766e-01, -7.3915e-02,  2.4607e-01, -2.3519e-01,  1.9558e-01,\n","         -4.8310e-02,  1.4257e+00,  3.9211e-02,  1.3665e-01, -8.2510e-02],\n","        [ 1.8799e-02,  2.1649e-01, -6.4938e-02, -6.3878e-02, -5.2711e-02,\n","         -1.2537e-01,  1.1543e-01, -5.8086e-02,  1.1046e-01, -1.1386e-01],\n","        [-4.8433e-02,  5.6774e-01,  1.1347e-03, -8.9576e-02,  8.8604e-02,\n","          3.8573e-02,  7.3911e-01, -3.8105e-01,  4.3148e-01, -6.5964e-01]])\n","linear_hidden.0.bias tensor([ 0.0048,  0.0380,  0.1302,  0.1459,  0.1242,  0.0087, -0.0654, -0.1766,\n","        -0.0527,  0.0204])\n","linear_output.weight tensor([[ 0.5125,  0.5019,  0.2986,  0.2108, -0.8100,  0.5311, -0.0181, -1.1634,\n","         -0.0360, -0.7046]])\n","linear_output.bias tensor([-0.1465])\n","Testing out: \n","batch_size:  3008\n","train_size:  3290\n","n_epochs:  86\n","lr:  0.1369523922173857\n","weight_decay:  0.0006156034734584197\n","betas0:  0.9997174459457547\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0602, -0.0961, -0.0748],\n","          [ 0.3264, -0.2430, -0.2030],\n","          [-0.0153,  0.1749,  0.1125]]]])\n","conv1.bias tensor([-0.1425])\n","first_linear.weight tensor([[-0.0023, -0.0724,  0.1468,  0.0463,  0.3219,  0.1219, -0.1924, -0.1179,\n","         -0.0305],\n","        [ 0.2143, -0.2497,  0.2978, -0.3167,  0.2094,  0.0506, -0.2446, -0.0259,\n","          0.3053],\n","        [-0.1487,  0.2335, -0.2811,  0.1298, -0.2909, -0.1313, -0.1854,  0.1938,\n","         -0.1154],\n","        [-0.2979, -0.3131, -0.0548,  0.2171, -0.1856, -0.1669, -0.2698, -0.2945,\n","          0.0344],\n","        [ 0.0327, -0.0261,  0.1681,  0.1299, -0.2052,  0.3113,  0.3298,  0.1669,\n","         -0.1975],\n","        [ 0.1767,  0.1146, -0.2169, -0.0544, -0.1498, -0.2320,  0.0848, -0.3207,\n","         -0.3139],\n","        [ 0.1640, -0.2480, -0.2313, -0.0535,  0.0054,  0.2426, -0.2252, -0.1290,\n","         -0.2151],\n","        [-0.3040, -0.1954,  0.1938,  0.0664, -0.0817,  0.0391,  0.0700,  0.0865,\n","          0.1521],\n","        [-0.0209, -0.1531, -0.0140,  0.1017, -0.1616,  0.2210,  0.2941, -0.1675,\n","         -0.1139],\n","        [ 0.1434, -0.0842,  0.1544, -0.1750, -0.2142,  0.2830, -0.1464,  0.0196,\n","          0.1945]])\n","first_linear.bias tensor([ 0.2417,  0.0100,  0.0867, -0.0294, -0.3269,  0.3134, -0.2601,  0.0775,\n","         0.1726,  0.2745])\n","linear_hidden.0.weight tensor([[-0.0502,  0.1781, -0.2259,  0.0961,  0.2612,  0.1453, -0.2316,  0.0726,\n","          0.2274,  0.1972],\n","        [ 0.3023, -0.1376,  0.1370,  0.1616,  0.2697, -0.1690,  0.0061, -0.1671,\n","         -0.0180,  0.0183],\n","        [-0.0023, -0.1872, -0.1503,  0.0578, -0.1018, -0.1630, -0.0403, -0.2655,\n","          0.0293, -0.1922],\n","        [-0.0099,  0.0440,  0.2606, -0.2449,  0.3032,  0.0073,  0.1918,  0.1271,\n","          0.0032, -0.0425],\n","        [ 0.0293,  0.0275,  0.0230,  0.0934, -0.2375, -0.1000,  0.2824,  0.1574,\n","         -0.0499,  0.2704],\n","        [ 0.0804, -0.2590,  0.1432, -0.2283,  0.2156, -0.2667, -0.2159,  0.2794,\n","          0.2300,  0.3091],\n","        [ 0.1919, -0.0557, -0.2641, -0.0517,  0.1271,  0.1569, -0.2904,  0.2848,\n","          0.2397,  0.1042],\n","        [-0.1487, -0.1230, -0.1316, -0.0321,  0.1086, -0.0627,  0.1199,  0.1584,\n","          0.0554, -0.0153],\n","        [-0.1660, -0.2698, -0.2183,  0.2866,  0.2005, -0.1663, -0.1645, -0.1427,\n","          0.2305, -0.1687],\n","        [-0.2618, -0.3008,  0.1634,  0.1062, -0.2762,  0.2161,  0.0547,  0.0940,\n","         -0.2114, -0.1948]])\n","linear_hidden.0.bias tensor([-0.2412,  0.1701, -0.2658, -0.0650, -0.2244, -0.1972,  0.0463, -0.0310,\n","         0.3054, -0.1326])\n","linear_output.weight tensor([[-0.1409, -0.3152,  0.2119,  0.2065, -0.1394,  0.2556,  0.0098,  0.1480,\n","         -0.0767,  0.1435]])\n","linear_output.bias tensor([0.2118])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([3008])) that is different to the input size (torch.Size([3008, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([282])) that is different to the input size (torch.Size([282, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.722647488117218 \t Validate_Accuracy: 0.457\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.7020787596702576 \t Validate_Accuracy: 0.5595\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6838515400886536 \t Validate_Accuracy: 0.7385\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6439616084098816 \t Validate_Accuracy: 0.6305\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.5943869650363922 \t Validate_Accuracy: 0.7855\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.5597705841064453 \t Validate_Accuracy: 0.7885\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.49048852920532227 \t Validate_Accuracy: 0.7925\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.4641948491334915 \t Validate_Accuracy: 0.8085\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.4625054597854614 \t Validate_Accuracy: 0.8095\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.4640205502510071 \t Validate_Accuracy: 0.806\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.42287661135196686 \t Validate_Accuracy: 0.817\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.3925122916698456 \t Validate_Accuracy: 0.82\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.4104124754667282 \t Validate_Accuracy: 0.8235\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.4102536141872406 \t Validate_Accuracy: 0.822\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.42914149165153503 \t Validate_Accuracy: 0.826\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.4194846600294113 \t Validate_Accuracy: 0.8185\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.4053007513284683 \t Validate_Accuracy: 0.823\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.4040912985801697 \t Validate_Accuracy: 0.815\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.42041172087192535 \t Validate_Accuracy: 0.8165\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.4022633284330368 \t Validate_Accuracy: 0.8215\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.37497030198574066 \t Validate_Accuracy: 0.8245\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3785010725259781 \t Validate_Accuracy: 0.827\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3651638776063919 \t Validate_Accuracy: 0.8235\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3561396300792694 \t Validate_Accuracy: 0.825\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.39699290692806244 \t Validate_Accuracy: 0.831\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3603534698486328 \t Validate_Accuracy: 0.825\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3751482665538788 \t Validate_Accuracy: 0.8155\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3843056261539459 \t Validate_Accuracy: 0.815\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.361875057220459 \t Validate_Accuracy: 0.8205\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.36078397929668427 \t Validate_Accuracy: 0.8255\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.3901684880256653 \t Validate_Accuracy: 0.8345\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.36290499567985535 \t Validate_Accuracy: 0.8325\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.373394176363945 \t Validate_Accuracy: 0.825\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3760967254638672 \t Validate_Accuracy: 0.8285\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.34550538659095764 \t Validate_Accuracy: 0.835\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3255559951066971 \t Validate_Accuracy: 0.831\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.34282156825065613 \t Validate_Accuracy: 0.82\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3876361548900604 \t Validate_Accuracy: 0.832\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.35485950112342834 \t Validate_Accuracy: 0.8375\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.33625219762325287 \t Validate_Accuracy: 0.8325\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.309257373213768 \t Validate_Accuracy: 0.851\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.31632591784000397 \t Validate_Accuracy: 0.8425\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.3039422631263733 \t Validate_Accuracy: 0.8295\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.33026522397994995 \t Validate_Accuracy: 0.8335\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.3189022094011307 \t Validate_Accuracy: 0.846\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.3299233615398407 \t Validate_Accuracy: 0.8335\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2885419577360153 \t Validate_Accuracy: 0.8445\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.3140569478273392 \t Validate_Accuracy: 0.8335\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.3339664787054062 \t Validate_Accuracy: 0.839\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.29741762578487396 \t Validate_Accuracy: 0.853\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.30050888657569885 \t Validate_Accuracy: 0.8645\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2811623364686966 \t Validate_Accuracy: 0.87\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.3097849190235138 \t Validate_Accuracy: 0.873\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.27973201870918274 \t Validate_Accuracy: 0.864\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2764594107866287 \t Validate_Accuracy: 0.8665\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.271746501326561 \t Validate_Accuracy: 0.866\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2795349061489105 \t Validate_Accuracy: 0.866\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.28373706340789795 \t Validate_Accuracy: 0.8565\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.2657966911792755 \t Validate_Accuracy: 0.8565\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2947074621915817 \t Validate_Accuracy: 0.8555\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.30005697906017303 \t Validate_Accuracy: 0.854\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.27797001600265503 \t Validate_Accuracy: 0.8405\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.34263038635253906 \t Validate_Accuracy: 0.8345\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.3254610300064087 \t Validate_Accuracy: 0.8545\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.3373924791812897 \t Validate_Accuracy: 0.841\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.30620092153549194 \t Validate_Accuracy: 0.8495\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3533332645893097 \t Validate_Accuracy: 0.865\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.2946663349866867 \t Validate_Accuracy: 0.8465\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.3093068301677704 \t Validate_Accuracy: 0.863\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.308042049407959 \t Validate_Accuracy: 0.859\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.26530317962169647 \t Validate_Accuracy: 0.8375\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.3070118725299835 \t Validate_Accuracy: 0.8525\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.3127266466617584 \t Validate_Accuracy: 0.85\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.3176572769880295 \t Validate_Accuracy: 0.858\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.28675776720046997 \t Validate_Accuracy: 0.867\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.26520703732967377 \t Validate_Accuracy: 0.865\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.2634740322828293 \t Validate_Accuracy: 0.867\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.26396898925304413 \t Validate_Accuracy: 0.871\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2570280209183693 \t Validate_Accuracy: 0.873\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.23905660957098007 \t Validate_Accuracy: 0.863\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2665238678455353 \t Validate_Accuracy: 0.8675\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.25919967889785767 \t Validate_Accuracy: 0.873\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2695147171616554 \t Validate_Accuracy: 0.8755\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.24976062774658203 \t Validate_Accuracy: 0.8745\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.24730084836483002 \t Validate_Accuracy: 0.863\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.28052347898483276 \t Validate_Accuracy: 0.8685\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.3072, -0.0506, -0.3921],\n","          [-0.0880, -0.0666, -0.0916],\n","          [-0.2132, -0.0882, -0.1601]]]])\n","conv1.bias tensor([0.0696])\n","first_linear.weight tensor([[ 0.7127, -1.8200,  0.8654, -0.3027, -0.0588, -0.2959, -0.9893,  0.1145,\n","         -0.5211],\n","        [-0.5327, -1.0631, -1.0272, -0.2827, -0.3056,  0.6078,  0.2982, -0.3406,\n","         -0.1712],\n","        [-0.3298,  0.3997,  0.6675,  0.5999,  0.6675,  0.6293,  0.2204,  0.4392,\n","          0.5201],\n","        [ 0.5788,  0.1127, -0.1196, -2.5993,  0.4636, -0.1046,  0.7911, -0.0287,\n","         -0.3816],\n","        [ 0.6378,  0.4792, -0.4858,  0.1093,  0.2412,  1.5628,  0.8677, -0.8036,\n","         -0.1261],\n","        [ 0.6999, -0.2519, -0.3395, -1.5101, -0.6459,  1.0809,  0.3892, -0.3071,\n","         -1.4413],\n","        [-0.3241,  0.5722, -0.5063,  1.0435, -1.3953,  1.1237, -0.7407,  1.3856,\n","         -1.0985],\n","        [ 0.4883,  0.6771,  0.7499, -0.0199,  0.4248,  0.5158,  0.0302, -0.1627,\n","          0.1359],\n","        [ 0.4020,  0.1939,  0.4276, -0.0431, -0.3034, -0.4099, -0.8143,  1.8172,\n","         -0.4766],\n","        [-1.2782,  1.5833, -0.6884, -0.0327, -1.3191,  0.1646, -0.5086, -0.8391,\n","         -0.3412]])\n","first_linear.bias tensor([ 0.9462,  1.3937, -1.7537,  1.6941,  0.0140, -0.6723,  1.7673,  1.6309,\n","        -1.1235,  0.2930])\n","linear_hidden.0.weight tensor([[-0.1776, -0.5805,  0.8232, -0.4077,  0.5221,  0.0050,  0.1661, -0.4782,\n","          0.0019, -0.6437],\n","        [-0.3718,  0.5731, -0.5076, -1.0862,  1.2742,  1.0875, -1.5199,  0.1522,\n","         -0.0356,  0.1647],\n","        [ 0.9050, -0.3886,  0.5535,  0.6840, -0.1268, -0.3568,  1.0884, -0.9245,\n","         -0.9263,  0.2325],\n","        [ 0.2408,  0.0376,  0.0996,  0.0440, -0.0488, -0.0398,  0.2378, -0.1663,\n","         -0.0473, -0.0971],\n","        [-1.2674, -0.3120,  0.2219,  0.5606,  0.0967, -0.4967,  0.4144, -0.7654,\n","          0.4169, -1.4751],\n","        [ 0.4222,  0.0488,  0.0810, -1.1299, -0.3151,  0.9013, -0.6499,  0.8630,\n","          1.2963, -0.6819],\n","        [-0.2412,  0.3718, -0.7251, -0.3406,  0.7639, -0.9168, -0.0651,  1.1211,\n","          0.5896, -0.2521],\n","        [-0.0057,  0.3384, -0.4421,  0.0372, -0.1145, -0.1528,  0.1571,  0.5451,\n","         -0.1981, -0.0316],\n","        [ 0.5386,  0.5397,  0.4198,  1.1277,  1.4128,  0.9094,  0.9439, -0.2305,\n","         -0.1289, -0.3753],\n","        [ 0.6037,  0.0967, -0.5079, -0.0736, -0.1925,  0.1623, -0.1388,  0.4452,\n","         -0.0750,  0.7502]])\n","linear_hidden.0.bias tensor([-0.9070,  0.2784, -0.9977, -0.1492, -0.8625,  0.0940,  1.1736,  0.3278,\n","        -0.1719,  0.5137])\n","linear_output.weight tensor([[ 1.0006, -1.6159,  1.4873,  0.0958,  1.0912, -1.2988, -1.5842, -0.2525,\n","          1.2845, -0.4098]])\n","linear_output.bias tensor([0.4810])\n","Testing out: \n","batch_size:  2958\n","train_size:  1872\n","n_epochs:  132\n","lr:  0.05393723379042198\n","weight_decay:  0.006291315803189283\n","betas0:  0.9626806284829563\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2841, -0.0784,  0.1480],\n","          [ 0.0670,  0.3191,  0.2918],\n","          [ 0.1106,  0.0568,  0.0141]]]])\n","conv1.bias tensor([0.1326])\n","first_linear.weight tensor([[ 0.1180,  0.1096,  0.1700,  0.2532, -0.1628, -0.2065, -0.3160, -0.1749,\n","          0.0869],\n","        [ 0.2547,  0.3186, -0.0336, -0.3308,  0.1728, -0.1718,  0.1056,  0.1064,\n","         -0.2239],\n","        [-0.0137,  0.2607,  0.0421, -0.0575,  0.2271,  0.2247, -0.0581, -0.2112,\n","         -0.1971],\n","        [ 0.2487, -0.1487,  0.2465,  0.3245,  0.2881, -0.1220, -0.2877, -0.0276,\n","          0.2589],\n","        [-0.1106,  0.2172, -0.2628, -0.2169, -0.2209,  0.3228,  0.2870,  0.3091,\n","          0.1455],\n","        [-0.2002,  0.2217,  0.3267,  0.1960,  0.1493, -0.0345, -0.3124,  0.3296,\n","          0.0522],\n","        [ 0.1538, -0.3011,  0.2501,  0.1689, -0.0246, -0.2030,  0.0466, -0.0934,\n","          0.0281],\n","        [-0.2012, -0.0912,  0.1101, -0.1270, -0.2570,  0.1835, -0.0497, -0.3228,\n","         -0.2250],\n","        [-0.0036, -0.1327,  0.0405,  0.0767, -0.2333, -0.2943, -0.1230, -0.1209,\n","          0.2503],\n","        [ 0.0821, -0.0863, -0.1510,  0.1070,  0.1199,  0.1633,  0.2323, -0.2401,\n","          0.1762]])\n","first_linear.bias tensor([-0.2529, -0.2733, -0.1429,  0.0222, -0.0086, -0.2950, -0.2773,  0.3089,\n","         0.2485, -0.3184])\n","linear_hidden.0.weight tensor([[-0.1757,  0.2890, -0.0329,  0.2986, -0.2468,  0.1783,  0.2980, -0.2980,\n","          0.2486,  0.0795],\n","        [-0.1951,  0.2422, -0.2336, -0.1543, -0.0742,  0.3042, -0.1181, -0.1837,\n","          0.1112,  0.0048],\n","        [ 0.1641,  0.1616,  0.2977, -0.2448,  0.2515,  0.1747,  0.0438, -0.0062,\n","         -0.2619, -0.1446],\n","        [-0.1402, -0.1098, -0.1956,  0.1328,  0.1066,  0.2398, -0.1988,  0.0545,\n","         -0.1375, -0.0673],\n","        [ 0.1247, -0.2385,  0.1156, -0.3160, -0.1007, -0.2324,  0.2561,  0.1593,\n","          0.2170,  0.1357],\n","        [-0.0537, -0.0146,  0.1707, -0.1100,  0.2026, -0.1744, -0.0605,  0.2814,\n","          0.1203,  0.2477],\n","        [-0.2734,  0.1778,  0.0610,  0.2394,  0.0860,  0.2417, -0.0664,  0.2579,\n","          0.1006, -0.0813],\n","        [ 0.1221,  0.0137,  0.1853,  0.2717, -0.0540, -0.1192,  0.1056,  0.2701,\n","         -0.0066, -0.0576],\n","        [-0.1864,  0.2517, -0.1740,  0.1679, -0.3088, -0.0581,  0.2886, -0.1310,\n","         -0.2679,  0.2557],\n","        [ 0.1689,  0.0835, -0.0099,  0.3088,  0.1917, -0.2091, -0.0890, -0.2925,\n","          0.3140, -0.3083]])\n","linear_hidden.0.bias tensor([ 2.4200e-01, -1.0610e-02, -1.3736e-03,  2.5645e-01,  1.4281e-01,\n","         2.5040e-04,  3.7807e-02, -1.7650e-01, -9.2604e-02, -2.5394e-01])\n","linear_output.weight tensor([[-0.0908, -0.0869,  0.0276,  0.1122,  0.1934,  0.1362,  0.1488,  0.2659,\n","          0.1701,  0.2248]])\n","linear_output.bias tensor([-0.0025])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6945192813873291 \t Validate_Accuracy: 0.497\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6948503851890564 \t Validate_Accuracy: 0.504\n","epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1872])) that is different to the input size (torch.Size([1872, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3 \t Train Loss: 0.6931778192520142 \t Validate_Accuracy: 0.503\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6935097575187683 \t Validate_Accuracy: 0.503\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.693374514579773 \t Validate_Accuracy: 0.503\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6931518912315369 \t Validate_Accuracy: 0.497\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6931872963905334 \t Validate_Accuracy: 0.497\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.693253219127655 \t Validate_Accuracy: 0.497\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6932234168052673 \t Validate_Accuracy: 0.528\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6931880712509155 \t Validate_Accuracy: 0.503\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6931976675987244 \t Validate_Accuracy: 0.503\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6931992173194885 \t Validate_Accuracy: 0.503\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6931681632995605 \t Validate_Accuracy: 0.503\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6931375861167908 \t Validate_Accuracy: 0.5055\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6931324005126953 \t Validate_Accuracy: 0.497\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6931449174880981 \t Validate_Accuracy: 0.497\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6931541562080383 \t Validate_Accuracy: 0.497\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6931540369987488 \t Validate_Accuracy: 0.497\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6931484937667847 \t Validate_Accuracy: 0.497\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6931478977203369 \t Validate_Accuracy: 0.503\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6931475400924683 \t Validate_Accuracy: 0.503\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6931493878364563 \t Validate_Accuracy: 0.503\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6931501030921936 \t Validate_Accuracy: 0.503\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6931501030921936 \t Validate_Accuracy: 0.494\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6931490898132324 \t Validate_Accuracy: 0.497\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6931474208831787 \t Validate_Accuracy: 0.497\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6931456327438354 \t Validate_Accuracy: 0.497\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6931521892547607 \t Validate_Accuracy: 0.497\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6931495070457458 \t Validate_Accuracy: 0.503\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6931411623954773 \t Validate_Accuracy: 0.503\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6931532025337219 \t Validate_Accuracy: 0.503\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6931536197662354 \t Validate_Accuracy: 0.503\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6931414604187012 \t Validate_Accuracy: 0.503\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6931524872779846 \t Validate_Accuracy: 0.503\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6931530833244324 \t Validate_Accuracy: 0.503\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6931434869766235 \t Validate_Accuracy: 0.497\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6931535005569458 \t Validate_Accuracy: 0.497\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.6931541562080383 \t Validate_Accuracy: 0.497\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.693141520023346 \t Validate_Accuracy: 0.497\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.6931411623954773 \t Validate_Accuracy: 0.497\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.693140983581543 \t Validate_Accuracy: 0.497\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6931529641151428 \t Validate_Accuracy: 0.503\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6931541562080383 \t Validate_Accuracy: 0.503\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6931515336036682 \t Validate_Accuracy: 0.503\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6931425333023071 \t Validate_Accuracy: 0.503\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6931396126747131 \t Validate_Accuracy: 0.503\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6931514739990234 \t Validate_Accuracy: 0.503\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6931400895118713 \t Validate_Accuracy: 0.503\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.693152129650116 \t Validate_Accuracy: 0.497\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6931520700454712 \t Validate_Accuracy: 0.497\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6931527853012085 \t Validate_Accuracy: 0.497\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.6931520104408264 \t Validate_Accuracy: 0.497\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6931425929069519 \t Validate_Accuracy: 0.497\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6931527853012085 \t Validate_Accuracy: 0.497\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6931394934654236 \t Validate_Accuracy: 0.503\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.503\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.6931527853012085 \t Validate_Accuracy: 0.503\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.503\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.6931553483009338 \t Validate_Accuracy: 0.503\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6931520700454712 \t Validate_Accuracy: 0.497\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.6931526064872742 \t Validate_Accuracy: 0.497\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.497\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6931429505348206 \t Validate_Accuracy: 0.497\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6931394934654236 \t Validate_Accuracy: 0.497\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.6931393146514893 \t Validate_Accuracy: 0.497\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.6931530833244324 \t Validate_Accuracy: 0.503\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6931395530700684 \t Validate_Accuracy: 0.503\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6931514143943787 \t Validate_Accuracy: 0.503\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6931394934654236 \t Validate_Accuracy: 0.503\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.693152129650116 \t Validate_Accuracy: 0.503\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.6931517124176025 \t Validate_Accuracy: 0.497\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.6931526064872742 \t Validate_Accuracy: 0.497\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.6931532621383667 \t Validate_Accuracy: 0.497\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.6931517124176025 \t Validate_Accuracy: 0.497\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.6931518912315369 \t Validate_Accuracy: 0.497\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.693152904510498 \t Validate_Accuracy: 0.503\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.6931391954421997 \t Validate_Accuracy: 0.503\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.693139374256134 \t Validate_Accuracy: 0.503\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.693139910697937 \t Validate_Accuracy: 0.503\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.6931397914886475 \t Validate_Accuracy: 0.503\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.6931520700454712 \t Validate_Accuracy: 0.503\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.693139374256134 \t Validate_Accuracy: 0.497\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.6931532621383667 \t Validate_Accuracy: 0.497\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.6931421756744385 \t Validate_Accuracy: 0.497\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.6931425929069519 \t Validate_Accuracy: 0.497\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.6931519508361816 \t Validate_Accuracy: 0.497\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.693139374256134 \t Validate_Accuracy: 0.503\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.503\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.6931531429290771 \t Validate_Accuracy: 0.503\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.6931527256965637 \t Validate_Accuracy: 0.503\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.6931524276733398 \t Validate_Accuracy: 0.503\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.6931422352790833 \t Validate_Accuracy: 0.497\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.6931526064872742 \t Validate_Accuracy: 0.497\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.6931519508361816 \t Validate_Accuracy: 0.497\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.6931518912315369 \t Validate_Accuracy: 0.497\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.6931553483009338 \t Validate_Accuracy: 0.497\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.6931394934654236 \t Validate_Accuracy: 0.503\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.6931399703025818 \t Validate_Accuracy: 0.503\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.6931554079055786 \t Validate_Accuracy: 0.503\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.6931522488594055 \t Validate_Accuracy: 0.503\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.6931527256965637 \t Validate_Accuracy: 0.503\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.6931403279304504 \t Validate_Accuracy: 0.497\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.6931399703025818 \t Validate_Accuracy: 0.497\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.693152666091919 \t Validate_Accuracy: 0.497\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.6931535005569458 \t Validate_Accuracy: 0.497\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.6931394338607788 \t Validate_Accuracy: 0.497\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.6931391954421997 \t Validate_Accuracy: 0.503\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.6931424736976624 \t Validate_Accuracy: 0.503\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.6931391954421997 \t Validate_Accuracy: 0.503\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.6931397914886475 \t Validate_Accuracy: 0.503\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.6931390762329102 \t Validate_Accuracy: 0.503\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.6931514143943787 \t Validate_Accuracy: 0.497\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.6931423544883728 \t Validate_Accuracy: 0.497\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.6931399703025818 \t Validate_Accuracy: 0.497\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.6931394934654236 \t Validate_Accuracy: 0.497\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.6931390166282654 \t Validate_Accuracy: 0.503\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.6931392550468445 \t Validate_Accuracy: 0.503\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.6931391954421997 \t Validate_Accuracy: 0.503\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.6931399703025818 \t Validate_Accuracy: 0.503\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.6931427717208862 \t Validate_Accuracy: 0.503\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.6931391954421997 \t Validate_Accuracy: 0.497\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.6931393146514893 \t Validate_Accuracy: 0.497\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.6931391954421997 \t Validate_Accuracy: 0.497\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.693142294883728 \t Validate_Accuracy: 0.497\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.693138837814331 \t Validate_Accuracy: 0.497\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.6931388974189758 \t Validate_Accuracy: 0.503\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.6931394934654236 \t Validate_Accuracy: 0.503\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.6931396126747131 \t Validate_Accuracy: 0.503\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.6931394338607788 \t Validate_Accuracy: 0.503\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.6931397318840027 \t Validate_Accuracy: 0.497\n","epoch 131\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:48:38] ax.service.managed_loop: Running optimization trial 4...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 131 \t Train Loss: 0.6931514143943787 \t Validate_Accuracy: 0.497\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.6931402683258057 \t Validate_Accuracy: 0.497\n","model parameters! \n","\n","conv1.weight tensor([[[[ 1.0225e-04, -2.3852e-04,  1.1042e-04],\n","          [-1.1696e-05, -1.6007e-04, -7.0560e-05],\n","          [ 3.0371e-05,  6.5589e-05, -1.0628e-04]]]])\n","conv1.bias tensor([1.0584e-05])\n","first_linear.weight tensor([[ 5.7359e-05, -7.3786e-05, -4.7194e-05, -1.8388e-04,  1.5524e-04,\n","         -1.7461e-04,  2.6416e-04,  9.2422e-05,  2.2417e-05],\n","        [-2.1082e-04,  1.2942e-04, -1.7003e-05,  3.0321e-04, -1.3096e-04,\n","         -8.9947e-05,  2.5285e-05,  7.8906e-05, -1.6496e-04],\n","        [ 9.1873e-05, -1.2702e-04,  8.5634e-05,  7.2277e-05, -1.8014e-04,\n","         -1.4082e-04,  4.9030e-05,  1.8410e-04, -1.6441e-04],\n","        [-2.0205e-04, -8.5576e-05, -1.7687e-04, -2.2022e-05, -5.6651e-05,\n","          5.6666e-05, -9.8068e-05,  2.4152e-05, -7.0768e-05],\n","        [-5.8225e-05, -8.5229e-05,  1.2526e-04,  1.6176e-05, -5.5740e-05,\n","         -2.4571e-04,  2.2406e-06,  1.4971e-05,  2.7933e-05],\n","        [-9.8546e-05,  1.2493e-04,  2.8551e-04, -7.6225e-05,  1.6560e-04,\n","         -1.3932e-04, -2.0095e-04,  2.6247e-04, -1.3528e-04],\n","        [ 1.1210e-04,  2.1583e-04,  1.8951e-04, -1.1984e-04, -5.2509e-05,\n","          1.3668e-04, -3.3243e-05,  1.1017e-04, -6.5549e-05],\n","        [ 5.8809e-07, -5.8226e-05,  1.1507e-04,  1.2046e-04,  1.7113e-04,\n","          2.7151e-04, -2.5549e-05, -2.2578e-04,  1.3995e-04],\n","        [ 5.4615e-05,  2.9042e-05, -8.0184e-05,  9.9756e-05, -3.0093e-05,\n","          8.6602e-05,  4.1646e-05, -6.7542e-05,  8.9827e-05],\n","        [ 1.3324e-05, -1.1471e-04,  1.0492e-04, -2.6123e-05,  2.5415e-05,\n","          1.2617e-04, -3.2544e-05,  2.1483e-04,  5.5660e-06]])\n","first_linear.bias tensor([-1.3455e-04,  5.3532e-05,  7.7376e-05, -6.5568e-05, -3.8332e-05,\n","         2.9295e-04,  1.2152e-04, -5.3498e-05,  2.4646e-04, -2.1086e-04])\n","linear_hidden.0.weight tensor([[-9.6267e-06,  1.5033e-04,  3.9622e-05, -7.5815e-05,  1.7091e-04,\n","         -1.5811e-04,  2.4418e-04,  4.2427e-05, -1.7278e-04,  5.9676e-05],\n","        [ 1.3153e-04,  1.7102e-06, -1.2183e-04, -6.9406e-05, -2.0015e-05,\n","         -1.3178e-04,  7.9387e-05, -1.6437e-04,  3.9254e-05,  8.0998e-05],\n","        [ 1.4516e-04,  1.3316e-04,  1.7871e-04,  1.3162e-04, -1.9259e-04,\n","          1.5942e-04,  2.2949e-05,  3.4348e-06,  8.9435e-05, -1.7355e-05],\n","        [-3.7558e-05, -7.1693e-05,  1.5701e-04, -4.5913e-05, -1.1566e-04,\n","          9.7793e-05, -1.6085e-04, -5.3593e-05, -6.7517e-06, -3.1739e-05],\n","        [ 4.8319e-05,  1.9228e-04,  6.2476e-05, -1.5457e-04,  2.7373e-05,\n","          1.1607e-04, -2.0776e-04,  1.3526e-04,  2.2086e-04, -1.6042e-06],\n","        [ 4.8692e-05,  9.5984e-05, -9.5184e-05, -8.6405e-05,  9.5429e-05,\n","         -1.4250e-04,  5.3334e-05, -1.5849e-04,  6.9939e-05, -2.2159e-04],\n","        [-9.8259e-05, -1.1966e-04,  2.7282e-05,  3.0038e-05, -7.7913e-05,\n","         -1.4170e-04,  1.0347e-04,  1.4688e-04,  1.1988e-04, -2.1552e-05],\n","        [ 3.5568e-05, -3.7680e-05,  9.3685e-05,  1.6401e-04, -2.1653e-05,\n","         -9.6044e-05, -7.4082e-05, -2.1145e-04, -8.2065e-05,  6.3260e-05],\n","        [-5.4944e-05,  4.9462e-05,  7.1392e-05,  7.7617e-05,  2.5014e-04,\n","         -7.8663e-05, -2.0707e-04,  5.9185e-05, -9.3169e-05, -1.6259e-04],\n","        [ 3.1152e-05, -3.9093e-07, -3.2593e-05, -2.4305e-04,  1.4473e-04,\n","         -2.8813e-05, -1.4347e-04,  2.6198e-04,  2.8364e-05,  2.2140e-04]])\n","linear_hidden.0.bias tensor([ 2.3544e-04, -1.2933e-04, -8.6871e-05, -1.1739e-04,  1.8945e-04,\n","         5.6177e-05, -4.8615e-05, -8.0462e-05, -8.5389e-05, -1.9183e-04])\n","linear_output.weight tensor([[-6.5636e-05, -1.2022e-04, -4.8270e-06, -1.2366e-04, -5.5518e-05,\n","         -2.0618e-04,  2.4119e-04, -3.1005e-04,  1.7200e-04,  1.6836e-04]])\n","linear_output.bias tensor([1.7762e-05])\n","Testing out: \n","batch_size:  2332\n","train_size:  3144\n","n_epochs:  184\n","lr:  0.01395967126017705\n","weight_decay:  0.00032914304319312286\n","betas0:  0.994856471841974\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0744, -0.0175,  0.0505],\n","          [ 0.2986, -0.0832, -0.1906],\n","          [ 0.3259,  0.2577, -0.1955]]]])\n","conv1.bias tensor([-0.2019])\n","first_linear.weight tensor([[-1.2727e-01,  1.9963e-01,  1.5619e-01,  2.9486e-01,  1.3444e-01,\n","         -2.2708e-01,  1.5174e-01, -2.9807e-01, -8.5336e-02],\n","        [-7.6384e-03, -3.2644e-01, -6.7691e-02, -1.4460e-01,  2.4727e-01,\n","         -1.7336e-01,  2.6018e-01, -2.8970e-01, -8.7821e-03],\n","        [ 2.8940e-01,  5.0895e-03, -3.1979e-01,  2.4909e-01, -7.7347e-02,\n","         -2.4247e-01, -7.0220e-02,  1.0204e-01,  2.1293e-02],\n","        [-3.6958e-02,  4.3141e-02, -2.0758e-01, -2.3700e-01,  7.8449e-02,\n","         -1.0063e-01, -9.8599e-02,  2.4008e-01, -3.0903e-01],\n","        [-1.4144e-01,  6.9185e-02, -1.0376e-01,  5.3506e-02,  1.7762e-01,\n","          1.4759e-01, -6.3141e-02, -1.8833e-01,  8.7107e-02],\n","        [-2.0143e-04, -1.3055e-01, -2.5747e-01, -2.6370e-01,  1.1098e-01,\n","          3.0727e-01,  2.1584e-01, -2.9499e-01,  2.5002e-01],\n","        [-8.7477e-02,  7.8069e-02, -1.9267e-01,  3.1857e-01,  9.6906e-02,\n","          4.7910e-02, -8.7053e-02, -8.6177e-02,  2.1350e-01],\n","        [-7.6819e-02, -2.6432e-01, -1.4325e-02, -3.1930e-02,  1.9103e-01,\n","          2.2575e-01, -2.7529e-01,  2.9617e-01,  1.2083e-01],\n","        [-2.9229e-01, -2.2717e-01, -1.0275e-01,  2.7898e-01, -1.8502e-01,\n","          1.3892e-01,  2.9179e-01, -3.2108e-01, -1.3490e-01],\n","        [ 2.8652e-01, -1.0380e-01,  2.3891e-01,  2.8524e-01,  4.7061e-02,\n","          6.9714e-02,  2.1708e-01,  2.8556e-01, -7.2238e-02]])\n","first_linear.bias tensor([-0.0677, -0.1258, -0.2410, -0.2852,  0.0953, -0.3019,  0.2795, -0.2241,\n","        -0.2897,  0.0341])\n","linear_hidden.0.weight tensor([[ 0.0468,  0.2668, -0.2354, -0.2175,  0.1455, -0.3000,  0.2026, -0.1900,\n","          0.0108, -0.2111],\n","        [ 0.1038, -0.1928,  0.1432, -0.2549, -0.1170,  0.1107, -0.0926, -0.2639,\n","         -0.0552,  0.2052],\n","        [ 0.1500,  0.0108, -0.3032,  0.0501,  0.1097, -0.1232,  0.1848, -0.0531,\n","          0.1600,  0.2364],\n","        [ 0.2552,  0.0106, -0.1984,  0.1642,  0.1102,  0.0092,  0.1228, -0.0155,\n","          0.2289,  0.0251],\n","        [-0.2223, -0.0368, -0.0960,  0.3067, -0.0339,  0.2820,  0.2461, -0.0867,\n","         -0.0743, -0.2968],\n","        [ 0.1291,  0.0553, -0.1153, -0.1401, -0.1875,  0.0797,  0.0716, -0.0081,\n","          0.2720,  0.0686],\n","        [-0.0572, -0.2093,  0.0473,  0.1212, -0.0163,  0.2579, -0.2833, -0.1470,\n","         -0.2536,  0.1690],\n","        [-0.1049,  0.2873, -0.2935,  0.0915, -0.0923, -0.2129,  0.0879,  0.0285,\n","         -0.3068, -0.1403],\n","        [-0.2972, -0.2633,  0.3033, -0.2543,  0.2107, -0.0793,  0.0764,  0.0658,\n","          0.2144,  0.0222],\n","        [ 0.2410, -0.0951,  0.1759, -0.0619, -0.1944, -0.0009, -0.1669,  0.0224,\n","          0.1791,  0.0080]])\n","linear_hidden.0.bias tensor([-0.0299, -0.1309, -0.2024, -0.1746, -0.2660, -0.3095,  0.2288,  0.1702,\n","         0.0159, -0.1225])\n","linear_output.weight tensor([[ 0.2298, -0.0911, -0.2082, -0.0927,  0.1406,  0.2809, -0.2769,  0.0280,\n","          0.2128,  0.2868]])\n","linear_output.bias tensor([0.0515])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6939114928245544 \t Validate_Accuracy: 0.5125\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2332])) that is different to the input size (torch.Size([2332, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([812])) that is different to the input size (torch.Size([812, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.692741721868515 \t Validate_Accuracy: 0.498\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6922445297241211 \t Validate_Accuracy: 0.4995\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6925562620162964 \t Validate_Accuracy: 0.4985\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6914000809192657 \t Validate_Accuracy: 0.5095\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6922825574874878 \t Validate_Accuracy: 0.5235\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6907959580421448 \t Validate_Accuracy: 0.5555\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.689902275800705 \t Validate_Accuracy: 0.54\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6898767352104187 \t Validate_Accuracy: 0.52\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6886282563209534 \t Validate_Accuracy: 0.5475\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6856730282306671 \t Validate_Accuracy: 0.5695\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6834085285663605 \t Validate_Accuracy: 0.58\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6770150363445282 \t Validate_Accuracy: 0.5905\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6742397844791412 \t Validate_Accuracy: 0.6075\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6652101874351501 \t Validate_Accuracy: 0.629\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6514351665973663 \t Validate_Accuracy: 0.6675\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6368012428283691 \t Validate_Accuracy: 0.6955\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6174343228340149 \t Validate_Accuracy: 0.72\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.5907326340675354 \t Validate_Accuracy: 0.7375\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.5658741891384125 \t Validate_Accuracy: 0.7645\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.5304392874240875 \t Validate_Accuracy: 0.8035\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.5001290738582611 \t Validate_Accuracy: 0.8225\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.47256046533584595 \t Validate_Accuracy: 0.8355\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.4441552758216858 \t Validate_Accuracy: 0.8385\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.4284832179546356 \t Validate_Accuracy: 0.8375\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3960947096347809 \t Validate_Accuracy: 0.8355\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.38344767689704895 \t Validate_Accuracy: 0.839\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.37793760001659393 \t Validate_Accuracy: 0.84\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3577556014060974 \t Validate_Accuracy: 0.843\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3548606187105179 \t Validate_Accuracy: 0.85\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.34853067994117737 \t Validate_Accuracy: 0.851\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.344860702753067 \t Validate_Accuracy: 0.851\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3240993618965149 \t Validate_Accuracy: 0.8575\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3154798895120621 \t Validate_Accuracy: 0.865\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.3145197182893753 \t Validate_Accuracy: 0.866\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3030017465353012 \t Validate_Accuracy: 0.866\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.2943215072154999 \t Validate_Accuracy: 0.8625\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3004053831100464 \t Validate_Accuracy: 0.8675\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.292495533823967 \t Validate_Accuracy: 0.87\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.28920187056064606 \t Validate_Accuracy: 0.875\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.274894580245018 \t Validate_Accuracy: 0.879\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2745809108018875 \t Validate_Accuracy: 0.879\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2795591205358505 \t Validate_Accuracy: 0.8765\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.26221826672554016 \t Validate_Accuracy: 0.8765\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.26089558005332947 \t Validate_Accuracy: 0.878\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.25566959381103516 \t Validate_Accuracy: 0.8765\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2636149525642395 \t Validate_Accuracy: 0.8745\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.2617092579603195 \t Validate_Accuracy: 0.873\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.25169871002435684 \t Validate_Accuracy: 0.877\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.24740534275770187 \t Validate_Accuracy: 0.8795\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.24824195355176926 \t Validate_Accuracy: 0.8755\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.23791727423667908 \t Validate_Accuracy: 0.8735\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2477174922823906 \t Validate_Accuracy: 0.874\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.24754779040813446 \t Validate_Accuracy: 0.8715\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2443019524216652 \t Validate_Accuracy: 0.8755\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.24259315431118011 \t Validate_Accuracy: 0.878\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.24420954287052155 \t Validate_Accuracy: 0.876\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.24189462512731552 \t Validate_Accuracy: 0.879\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.23833492398262024 \t Validate_Accuracy: 0.876\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.22992272675037384 \t Validate_Accuracy: 0.877\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.22953610867261887 \t Validate_Accuracy: 0.8795\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.23078597337007523 \t Validate_Accuracy: 0.8795\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.22844034433364868 \t Validate_Accuracy: 0.879\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.23170065879821777 \t Validate_Accuracy: 0.8775\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.22613617777824402 \t Validate_Accuracy: 0.8775\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2234051749110222 \t Validate_Accuracy: 0.879\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.23124146461486816 \t Validate_Accuracy: 0.883\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.21643586456775665 \t Validate_Accuracy: 0.8815\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.22361724078655243 \t Validate_Accuracy: 0.879\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2222788855433464 \t Validate_Accuracy: 0.88\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.22855423390865326 \t Validate_Accuracy: 0.881\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.22907275706529617 \t Validate_Accuracy: 0.8815\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.21740464121103287 \t Validate_Accuracy: 0.8775\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2280810922384262 \t Validate_Accuracy: 0.8785\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.20926696807146072 \t Validate_Accuracy: 0.882\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.22898558527231216 \t Validate_Accuracy: 0.8795\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.21306657791137695 \t Validate_Accuracy: 0.88\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.21148722618818283 \t Validate_Accuracy: 0.883\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2085690125823021 \t Validate_Accuracy: 0.8795\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.21537572145462036 \t Validate_Accuracy: 0.885\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.21519393473863602 \t Validate_Accuracy: 0.892\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.21929090470075607 \t Validate_Accuracy: 0.888\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.20849015563726425 \t Validate_Accuracy: 0.8855\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.20992794632911682 \t Validate_Accuracy: 0.882\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.21183936297893524 \t Validate_Accuracy: 0.8815\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.21513675898313522 \t Validate_Accuracy: 0.88\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.21447120606899261 \t Validate_Accuracy: 0.8805\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.20556799322366714 \t Validate_Accuracy: 0.8875\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.20482122153043747 \t Validate_Accuracy: 0.885\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2097335234284401 \t Validate_Accuracy: 0.8885\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.22110670804977417 \t Validate_Accuracy: 0.888\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2047087326645851 \t Validate_Accuracy: 0.8855\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.20414573699235916 \t Validate_Accuracy: 0.8865\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.21266289055347443 \t Validate_Accuracy: 0.886\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.2084849774837494 \t Validate_Accuracy: 0.8885\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.20044326782226562 \t Validate_Accuracy: 0.886\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.2100580781698227 \t Validate_Accuracy: 0.8835\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.21440517902374268 \t Validate_Accuracy: 0.884\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.20885635912418365 \t Validate_Accuracy: 0.8865\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.20892764627933502 \t Validate_Accuracy: 0.883\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.21099522709846497 \t Validate_Accuracy: 0.8835\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.20708243548870087 \t Validate_Accuracy: 0.8845\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.21345480531454086 \t Validate_Accuracy: 0.8885\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.21056073904037476 \t Validate_Accuracy: 0.8885\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.20687250047922134 \t Validate_Accuracy: 0.8855\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.20928259193897247 \t Validate_Accuracy: 0.884\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.20303688943386078 \t Validate_Accuracy: 0.8845\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.20631732791662216 \t Validate_Accuracy: 0.885\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.19990695267915726 \t Validate_Accuracy: 0.8835\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.20545939356088638 \t Validate_Accuracy: 0.888\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2112421616911888 \t Validate_Accuracy: 0.8855\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.20057683438062668 \t Validate_Accuracy: 0.8835\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2119314819574356 \t Validate_Accuracy: 0.8865\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.1969229355454445 \t Validate_Accuracy: 0.8865\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.21380072087049484 \t Validate_Accuracy: 0.8855\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.1967838704586029 \t Validate_Accuracy: 0.8835\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.20689218491315842 \t Validate_Accuracy: 0.889\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.20245027542114258 \t Validate_Accuracy: 0.8825\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.19063913077116013 \t Validate_Accuracy: 0.8875\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.20170582085847855 \t Validate_Accuracy: 0.892\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.20515578985214233 \t Validate_Accuracy: 0.884\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.19985643029212952 \t Validate_Accuracy: 0.8835\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.20117588341236115 \t Validate_Accuracy: 0.8905\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.19786430895328522 \t Validate_Accuracy: 0.889\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.19799672812223434 \t Validate_Accuracy: 0.889\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.19570456445217133 \t Validate_Accuracy: 0.8905\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.1950923427939415 \t Validate_Accuracy: 0.886\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.2032056376338005 \t Validate_Accuracy: 0.888\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.1991763710975647 \t Validate_Accuracy: 0.891\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.1987927183508873 \t Validate_Accuracy: 0.89\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.19811195135116577 \t Validate_Accuracy: 0.8875\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.19709479808807373 \t Validate_Accuracy: 0.888\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.19892307370901108 \t Validate_Accuracy: 0.891\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.1956595927476883 \t Validate_Accuracy: 0.889\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.20479001849889755 \t Validate_Accuracy: 0.886\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.1934029757976532 \t Validate_Accuracy: 0.8855\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.1937423050403595 \t Validate_Accuracy: 0.886\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.1960655152797699 \t Validate_Accuracy: 0.8845\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.193009153008461 \t Validate_Accuracy: 0.892\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.19156227260828018 \t Validate_Accuracy: 0.8875\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.19149981439113617 \t Validate_Accuracy: 0.8905\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.1863151639699936 \t Validate_Accuracy: 0.8885\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.1981724426150322 \t Validate_Accuracy: 0.892\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.20449969917535782 \t Validate_Accuracy: 0.892\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.19635052978992462 \t Validate_Accuracy: 0.8865\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.2055828645825386 \t Validate_Accuracy: 0.893\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.20336397737264633 \t Validate_Accuracy: 0.893\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.19621482491493225 \t Validate_Accuracy: 0.8935\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.19352203607559204 \t Validate_Accuracy: 0.891\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.19107292592525482 \t Validate_Accuracy: 0.8945\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.18904069066047668 \t Validate_Accuracy: 0.893\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.19313358515501022 \t Validate_Accuracy: 0.892\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.1957051381468773 \t Validate_Accuracy: 0.8925\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.20335479080677032 \t Validate_Accuracy: 0.891\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.19395160675048828 \t Validate_Accuracy: 0.893\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.20537914335727692 \t Validate_Accuracy: 0.8925\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.18871712684631348 \t Validate_Accuracy: 0.891\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.1867884173989296 \t Validate_Accuracy: 0.8925\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.19292724132537842 \t Validate_Accuracy: 0.892\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.19360525161027908 \t Validate_Accuracy: 0.892\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2051243633031845 \t Validate_Accuracy: 0.892\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.1944851651787758 \t Validate_Accuracy: 0.8935\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.2000514268875122 \t Validate_Accuracy: 0.8915\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.19177749007940292 \t Validate_Accuracy: 0.893\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.191927969455719 \t Validate_Accuracy: 0.8915\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.1898442506790161 \t Validate_Accuracy: 0.891\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.19967692345380783 \t Validate_Accuracy: 0.8945\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.18727882206439972 \t Validate_Accuracy: 0.894\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.1947939768433571 \t Validate_Accuracy: 0.8945\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.18590980023145676 \t Validate_Accuracy: 0.8905\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.20757192373275757 \t Validate_Accuracy: 0.892\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.20661894977092743 \t Validate_Accuracy: 0.894\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.19979041814804077 \t Validate_Accuracy: 0.8895\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.19754600524902344 \t Validate_Accuracy: 0.891\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.20609165728092194 \t Validate_Accuracy: 0.8915\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.19376034289598465 \t Validate_Accuracy: 0.8915\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.19884028285741806 \t Validate_Accuracy: 0.8915\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.19892387092113495 \t Validate_Accuracy: 0.892\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.2046773061156273 \t Validate_Accuracy: 0.8945\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.19108214229345322 \t Validate_Accuracy: 0.896\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.19428981095552444 \t Validate_Accuracy: 0.892\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.199566550552845 \t Validate_Accuracy: 0.895\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.19431186467409134 \t Validate_Accuracy: 0.896\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.20005490630865097 \t Validate_Accuracy: 0.894\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1435,  0.0508,  0.1716],\n","          [ 0.0627, -0.0057,  0.0446],\n","          [ 0.1392,  0.0479,  0.1634]]]])\n","conv1.bias tensor([-0.0790])\n","first_linear.weight tensor([[-3.9918e-01,  3.2825e-01,  6.5027e-01,  6.0230e-01, -4.5964e-01,\n","         -9.4669e-01,  2.6470e-01, -4.2796e-01,  7.5433e-01],\n","        [-2.4152e-01, -2.2260e-02, -2.8330e-01,  4.2888e-01,  1.6502e+00,\n","         -2.3126e-01, -1.9713e-01, -2.4128e-01,  1.9860e-01],\n","        [ 4.8334e-01, -2.8633e-01, -1.5193e-04, -1.0424e-01, -4.2831e-01,\n","         -7.1809e-02,  2.9816e-01, -4.5100e-02, -6.0831e-01],\n","        [-1.2794e-01,  6.0744e-01, -6.0378e-02,  2.8341e-01, -7.5095e-01,\n","          4.8345e-02, -7.9971e-01,  1.3318e+00, -5.7275e-01],\n","        [-1.1383e+00, -5.4907e-02,  3.8806e-01,  3.6533e-02,  2.4918e-01,\n","         -2.9598e-01,  1.8397e-01, -2.6250e-01, -4.4705e-01],\n","        [ 1.8416e-01,  5.0373e-01, -3.2334e-01, -1.5832e+00,  3.0413e-01,\n","          2.1218e-01,  1.1263e+00, -6.7006e-01, -1.3016e-01],\n","        [-8.6291e-01,  1.5062e+00, -8.2531e-01,  3.2552e-01, -2.5809e-01,\n","          6.2221e-03,  9.2507e-02, -2.8606e-01,  2.9167e-01],\n","        [ 2.4046e-01,  6.9195e-02,  1.5625e-01,  2.6217e-01,  2.7396e-01,\n","          2.4031e-01,  5.9143e-02,  1.3637e-01,  3.7259e-01],\n","        [-3.1630e-01,  6.5453e-02, -4.1527e-01,  9.4956e-01, -7.5712e-01,\n","          9.0758e-01,  7.6313e-02, -1.8677e-01, -1.6067e-01],\n","        [ 2.6511e-01,  6.2867e-01, -2.5956e-01,  3.7477e-01,  5.3599e-02,\n","          9.5222e-01,  3.7261e-01,  6.0954e-01, -2.5854e-01]])\n","first_linear.bias tensor([-0.7837, -0.5320, -0.2363, -0.9352,  0.3467, -0.9416,  0.9205, -1.0805,\n","        -0.8268, -0.2911])\n","linear_hidden.0.weight tensor([[ 0.5916,  0.5544, -0.7892, -0.9240,  0.4645, -0.8980,  0.4761,  0.5942,\n","         -0.7118, -1.0771],\n","        [ 0.5446,  0.4039, -0.3783,  0.5577, -0.2653,  0.5485, -0.6593, -1.2467,\n","          0.7023, -0.1196],\n","        [ 0.6051,  0.3137, -0.1726,  0.7570,  0.2780,  0.3735, -0.3854, -1.1580,\n","          0.9819, -0.0554],\n","        [ 0.5139,  0.5458, -0.2766,  0.2939,  0.2792,  0.7998, -0.2341, -1.0139,\n","          0.9313, -0.2862],\n","        [-0.2362, -0.8062,  0.1769, -0.0547,  0.8215, -0.2648,  0.4465,  0.6791,\n","         -0.4677, -0.4032],\n","        [ 0.1616,  0.2569, -0.0567, -0.0424, -0.3826,  0.1354, -0.4061,  0.5126,\n","          0.2329,  0.3838],\n","        [ 0.5580,  0.4385, -0.3795,  0.5945, -0.2685,  0.6540, -0.6478, -1.2140,\n","          0.6441, -0.0458],\n","        [-0.3523, -0.2903,  0.3799, -0.6371,  0.1999, -0.6163,  0.6439,  1.1030,\n","         -0.8198,  0.1364],\n","        [-0.7985, -0.7112,  0.5167, -0.7598,  0.4152, -0.7127,  1.0209,  0.8982,\n","          0.5027,  0.9610],\n","        [ 0.1661,  0.2360, -0.0073,  0.0193, -0.3274,  0.1769, -0.3855,  0.5708,\n","          0.2297,  0.3161]])\n","linear_hidden.0.bias tensor([-0.2969,  0.8960,  0.5201,  0.4371, -1.1382, -0.7788,  1.0598, -0.9046,\n","        -0.4924, -0.6687])\n","linear_output.weight tensor([[ 1.3919, -1.8029, -1.5448, -1.3558,  1.5216,  1.0169, -1.9642,  1.7147,\n","          1.7903,  0.9759]])\n","linear_output.bias tensor([-0.4539])\n","Testing out: \n","batch_size:  2205\n","train_size:  4011\n","n_epochs:  173\n","lr:  0.0783908008435748\n","weight_decay:  0.00016254911045483195\n","betas0:  0.9949411458871855\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.3185,  0.2732, -0.1150],\n","          [ 0.2512,  0.1065, -0.1770],\n","          [ 0.1941, -0.3085,  0.0767]]]])\n","conv1.bias tensor([0.3260])\n","first_linear.weight tensor([[-0.1865, -0.3216,  0.2752, -0.1374,  0.0473,  0.0929,  0.1557, -0.2644,\n","         -0.2761],\n","        [ 0.2225, -0.2883,  0.1827,  0.2300, -0.1641, -0.3258, -0.0383,  0.1208,\n","          0.0372],\n","        [-0.1398, -0.0631,  0.2029,  0.1630,  0.2975,  0.0081,  0.0929, -0.0512,\n","          0.3088],\n","        [-0.2607, -0.2150, -0.2292,  0.1220, -0.0517,  0.0054,  0.0269,  0.0458,\n","         -0.1537],\n","        [ 0.1448, -0.1161, -0.1422, -0.0971, -0.1775, -0.3044, -0.1384, -0.2877,\n","         -0.3222],\n","        [-0.3323, -0.2645,  0.2529, -0.3245, -0.2810, -0.0473,  0.0381, -0.2301,\n","          0.1119],\n","        [ 0.1977,  0.0685,  0.0032, -0.1127,  0.0018,  0.2726,  0.2147, -0.1133,\n","          0.1890],\n","        [ 0.0071,  0.1987, -0.1846,  0.3309,  0.1640,  0.2849,  0.1303,  0.0276,\n","         -0.1894],\n","        [-0.0922, -0.1554, -0.1818,  0.3210,  0.0802, -0.3137,  0.1117, -0.3025,\n","         -0.2317],\n","        [ 0.0074, -0.1260,  0.0408,  0.1488,  0.2246, -0.3258, -0.0161,  0.0077,\n","         -0.3314]])\n","first_linear.bias tensor([ 0.1839,  0.3142, -0.0904, -0.1103, -0.1076, -0.0527, -0.3058,  0.0924,\n","        -0.1877,  0.2419])\n","linear_hidden.0.weight tensor([[ 0.0576,  0.2807,  0.2473, -0.1394, -0.2244,  0.3047, -0.0759,  0.1123,\n","         -0.0709,  0.1766],\n","        [ 0.2111, -0.2732, -0.2360, -0.0625, -0.2812,  0.1448, -0.0702,  0.1893,\n","          0.2763, -0.0583],\n","        [-0.0048, -0.0334,  0.0869,  0.0074, -0.0131,  0.0424, -0.3096, -0.0313,\n","         -0.0816, -0.2056],\n","        [-0.1460, -0.0512, -0.1473,  0.1789, -0.2316, -0.1020,  0.1623,  0.0345,\n","          0.2496,  0.2347],\n","        [-0.0952,  0.2376, -0.0119,  0.2503, -0.0538,  0.2545,  0.0767, -0.1326,\n","          0.0588, -0.1461],\n","        [ 0.2110, -0.0147,  0.0218,  0.0979, -0.0483, -0.1548, -0.2590, -0.3050,\n","         -0.1559,  0.0433],\n","        [ 0.0274,  0.1199,  0.2350,  0.0499, -0.1969,  0.0584, -0.2381,  0.2861,\n","          0.2600,  0.2336],\n","        [ 0.1402,  0.2615,  0.1419, -0.2300, -0.1261, -0.0707, -0.0015, -0.1134,\n","         -0.1228,  0.2263],\n","        [ 0.3043,  0.1331, -0.3043,  0.2407, -0.1195, -0.2750, -0.2814, -0.0779,\n","         -0.1555, -0.0905],\n","        [ 0.2899,  0.1450,  0.2333, -0.1535,  0.3032, -0.0059, -0.0764,  0.1058,\n","          0.3119, -0.1491]])\n","linear_hidden.0.bias tensor([-0.1337, -0.2006, -0.1870,  0.2459,  0.2100, -0.2429, -0.2525,  0.2295,\n","        -0.0239, -0.1640])\n","linear_output.weight tensor([[ 0.2216, -0.1147,  0.1482,  0.1387,  0.1668,  0.1224, -0.1311,  0.0127,\n","          0.1344,  0.2595]])\n","linear_output.bias tensor([-0.0443])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7125791311264038 \t Validate_Accuracy: 0.478\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2205])) that is different to the input size (torch.Size([2205, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1806])) that is different to the input size (torch.Size([1806, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.7013376355171204 \t Validate_Accuracy: 0.5215\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6939294934272766 \t Validate_Accuracy: 0.602\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6930208802223206 \t Validate_Accuracy: 0.64\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6695746183395386 \t Validate_Accuracy: 0.623\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6305927336215973 \t Validate_Accuracy: 0.8\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.581356942653656 \t Validate_Accuracy: 0.794\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5588324964046478 \t Validate_Accuracy: 0.8145\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5361921191215515 \t Validate_Accuracy: 0.7965\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.5139469057321548 \t Validate_Accuracy: 0.7995\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.4890759587287903 \t Validate_Accuracy: 0.817\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.46256645023822784 \t Validate_Accuracy: 0.817\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.4482661187648773 \t Validate_Accuracy: 0.821\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.43730056285858154 \t Validate_Accuracy: 0.8175\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.4213298261165619 \t Validate_Accuracy: 0.822\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.4056951254606247 \t Validate_Accuracy: 0.824\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3904934227466583 \t Validate_Accuracy: 0.836\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3691290467977524 \t Validate_Accuracy: 0.828\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.35641729831695557 \t Validate_Accuracy: 0.837\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.34921061992645264 \t Validate_Accuracy: 0.8425\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.33896537125110626 \t Validate_Accuracy: 0.8375\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.33339183032512665 \t Validate_Accuracy: 0.8495\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.32510553300380707 \t Validate_Accuracy: 0.8475\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3242283910512924 \t Validate_Accuracy: 0.8485\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3203718960285187 \t Validate_Accuracy: 0.8505\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.31646837294101715 \t Validate_Accuracy: 0.858\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3138064444065094 \t Validate_Accuracy: 0.854\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3110801428556442 \t Validate_Accuracy: 0.852\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.30912502110004425 \t Validate_Accuracy: 0.853\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3091951906681061 \t Validate_Accuracy: 0.8585\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.30776312947273254 \t Validate_Accuracy: 0.858\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.3074706494808197 \t Validate_Accuracy: 0.8655\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3073964864015579 \t Validate_Accuracy: 0.862\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.31020957231521606 \t Validate_Accuracy: 0.8625\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.3067062348127365 \t Validate_Accuracy: 0.8615\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3010234236717224 \t Validate_Accuracy: 0.862\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.30232739448547363 \t Validate_Accuracy: 0.868\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.2972874343395233 \t Validate_Accuracy: 0.862\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.29531359672546387 \t Validate_Accuracy: 0.856\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.29622457921504974 \t Validate_Accuracy: 0.8595\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.29520030319690704 \t Validate_Accuracy: 0.8645\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2938053011894226 \t Validate_Accuracy: 0.859\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.29053567349910736 \t Validate_Accuracy: 0.8685\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2900604158639908 \t Validate_Accuracy: 0.866\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2896310091018677 \t Validate_Accuracy: 0.864\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2885432690382004 \t Validate_Accuracy: 0.867\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2888767570257187 \t Validate_Accuracy: 0.868\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.2877180874347687 \t Validate_Accuracy: 0.867\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.28787750005722046 \t Validate_Accuracy: 0.866\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.2834644764661789 \t Validate_Accuracy: 0.86\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2833349108695984 \t Validate_Accuracy: 0.867\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2799902558326721 \t Validate_Accuracy: 0.8665\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2817615419626236 \t Validate_Accuracy: 0.8695\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2827749401330948 \t Validate_Accuracy: 0.866\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.27954013645648956 \t Validate_Accuracy: 0.8695\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.27788831293582916 \t Validate_Accuracy: 0.867\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.276584267616272 \t Validate_Accuracy: 0.8685\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.27727922797203064 \t Validate_Accuracy: 0.8695\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.27464453876018524 \t Validate_Accuracy: 0.8735\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.27470563352108 \t Validate_Accuracy: 0.8715\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.27021171152591705 \t Validate_Accuracy: 0.8695\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.270511731505394 \t Validate_Accuracy: 0.868\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.26605963706970215 \t Validate_Accuracy: 0.867\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.265082985162735 \t Validate_Accuracy: 0.868\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2644244581460953 \t Validate_Accuracy: 0.8695\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2648187130689621 \t Validate_Accuracy: 0.868\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.25939901173114777 \t Validate_Accuracy: 0.867\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.25696658343076706 \t Validate_Accuracy: 0.874\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2556849271059036 \t Validate_Accuracy: 0.8625\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2547103017568588 \t Validate_Accuracy: 0.873\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.25380705296993256 \t Validate_Accuracy: 0.866\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.25225459784269333 \t Validate_Accuracy: 0.865\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2508247494697571 \t Validate_Accuracy: 0.8675\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.25111496448516846 \t Validate_Accuracy: 0.8605\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2519695833325386 \t Validate_Accuracy: 0.871\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.24893818795681 \t Validate_Accuracy: 0.8715\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.24857860803604126 \t Validate_Accuracy: 0.868\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.24634944647550583 \t Validate_Accuracy: 0.875\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.24553976953029633 \t Validate_Accuracy: 0.8735\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2526356428861618 \t Validate_Accuracy: 0.8705\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.24344994872808456 \t Validate_Accuracy: 0.884\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.24163325875997543 \t Validate_Accuracy: 0.87\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.24098212271928787 \t Validate_Accuracy: 0.8715\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.23581386357545853 \t Validate_Accuracy: 0.8715\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2334621325135231 \t Validate_Accuracy: 0.879\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.23442988097667694 \t Validate_Accuracy: 0.8785\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.23853550106287003 \t Validate_Accuracy: 0.865\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.25361259281635284 \t Validate_Accuracy: 0.871\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2459578514099121 \t Validate_Accuracy: 0.881\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.23860767483711243 \t Validate_Accuracy: 0.855\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2643572688102722 \t Validate_Accuracy: 0.869\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2860569655895233 \t Validate_Accuracy: 0.8585\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.26155948638916016 \t Validate_Accuracy: 0.8615\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2654677703976631 \t Validate_Accuracy: 0.8625\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.25405527651309967 \t Validate_Accuracy: 0.869\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.25057370960712433 \t Validate_Accuracy: 0.8715\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.24628964811563492 \t Validate_Accuracy: 0.8755\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.24005037546157837 \t Validate_Accuracy: 0.8795\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.23450147360563278 \t Validate_Accuracy: 0.8785\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2358972653746605 \t Validate_Accuracy: 0.8795\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.22996792942285538 \t Validate_Accuracy: 0.8735\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2272263914346695 \t Validate_Accuracy: 0.8805\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.22387275099754333 \t Validate_Accuracy: 0.8815\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.22099708020687103 \t Validate_Accuracy: 0.878\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.22220437228679657 \t Validate_Accuracy: 0.8825\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.21949229389429092 \t Validate_Accuracy: 0.887\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.21698343008756638 \t Validate_Accuracy: 0.8835\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.21681686490774155 \t Validate_Accuracy: 0.877\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.22770379483699799 \t Validate_Accuracy: 0.8785\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.23127588629722595 \t Validate_Accuracy: 0.882\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2156314179301262 \t Validate_Accuracy: 0.885\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.22109317034482956 \t Validate_Accuracy: 0.8795\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.22769984602928162 \t Validate_Accuracy: 0.857\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2542177140712738 \t Validate_Accuracy: 0.879\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2354235053062439 \t Validate_Accuracy: 0.87\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.2500871419906616 \t Validate_Accuracy: 0.876\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.23784397542476654 \t Validate_Accuracy: 0.877\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.2312154397368431 \t Validate_Accuracy: 0.872\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.22880035638809204 \t Validate_Accuracy: 0.8715\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.2282877191901207 \t Validate_Accuracy: 0.881\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.22408627718687057 \t Validate_Accuracy: 0.8805\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21915119141340256 \t Validate_Accuracy: 0.88\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2242339849472046 \t Validate_Accuracy: 0.885\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.22197307646274567 \t Validate_Accuracy: 0.879\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.2176794782280922 \t Validate_Accuracy: 0.8885\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.22207118570804596 \t Validate_Accuracy: 0.881\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.22253084927797318 \t Validate_Accuracy: 0.8815\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.21863195300102234 \t Validate_Accuracy: 0.8885\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.22103644162416458 \t Validate_Accuracy: 0.8805\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.21369116753339767 \t Validate_Accuracy: 0.881\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.21429628878831863 \t Validate_Accuracy: 0.8845\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.21168726682662964 \t Validate_Accuracy: 0.886\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.20935645699501038 \t Validate_Accuracy: 0.885\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.20925405621528625 \t Validate_Accuracy: 0.887\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.20914749056100845 \t Validate_Accuracy: 0.881\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2157263681292534 \t Validate_Accuracy: 0.8775\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.21631788462400436 \t Validate_Accuracy: 0.882\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.222319595515728 \t Validate_Accuracy: 0.86\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.238533653318882 \t Validate_Accuracy: 0.8725\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.24255870282649994 \t Validate_Accuracy: 0.863\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.24076275527477264 \t Validate_Accuracy: 0.871\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.23812831938266754 \t Validate_Accuracy: 0.877\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.23375332355499268 \t Validate_Accuracy: 0.881\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.22556020319461823 \t Validate_Accuracy: 0.878\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2238835021853447 \t Validate_Accuracy: 0.879\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.23177282512187958 \t Validate_Accuracy: 0.881\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.22003750503063202 \t Validate_Accuracy: 0.8755\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.22232475131750107 \t Validate_Accuracy: 0.884\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.21751413494348526 \t Validate_Accuracy: 0.8815\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.21534334123134613 \t Validate_Accuracy: 0.877\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.21747156232595444 \t Validate_Accuracy: 0.8775\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.21209736913442612 \t Validate_Accuracy: 0.883\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2124445140361786 \t Validate_Accuracy: 0.8835\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.21113284677267075 \t Validate_Accuracy: 0.886\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.20866258442401886 \t Validate_Accuracy: 0.8845\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.20958954840898514 \t Validate_Accuracy: 0.8855\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.20979363471269608 \t Validate_Accuracy: 0.881\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.20540927350521088 \t Validate_Accuracy: 0.8855\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.205523319542408 \t Validate_Accuracy: 0.888\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.2084081843495369 \t Validate_Accuracy: 0.881\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2167770117521286 \t Validate_Accuracy: 0.8705\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.22142350673675537 \t Validate_Accuracy: 0.8815\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.2167922481894493 \t Validate_Accuracy: 0.884\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.2138282060623169 \t Validate_Accuracy: 0.885\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.2102130800485611 \t Validate_Accuracy: 0.889\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.20536769181489944 \t Validate_Accuracy: 0.882\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.20572949200868607 \t Validate_Accuracy: 0.8865\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.20613885670900345 \t Validate_Accuracy: 0.879\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.20551200211048126 \t Validate_Accuracy: 0.8875\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.21422763913869858 \t Validate_Accuracy: 0.884\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.21131888031959534 \t Validate_Accuracy: 0.8865\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.21178489923477173 \t Validate_Accuracy: 0.8855\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.2131425067782402 \t Validate_Accuracy: 0.884\n","model parameters! \n","\n","conv1.weight tensor([[[[0.1786, 0.0526, 0.1890],\n","          [0.0841, 0.0010, 0.0391],\n","          [0.1515, 0.0477, 0.1579]]]])\n","conv1.bias tensor([-0.0142])\n","first_linear.weight tensor([[ 0.1635,  0.1583,  0.3939,  0.0023,  0.4021,  0.5920,  0.2578,  0.1961,\n","          0.2562],\n","        [-0.2785, -0.2872,  0.0024, -0.6245, -0.3288,  0.5132, -0.5701, -1.1772,\n","         -0.3596],\n","        [ 0.5388,  0.5266, -0.0521,  0.2360,  0.6464,  1.0053,  0.5681, -0.0594,\n","          0.3251],\n","        [-0.1399,  0.4970, -0.5230,  1.0961, -1.1958,  1.4927, -0.2851,  0.6950,\n","         -0.7216],\n","        [-1.0762,  2.1459, -0.6712,  0.7295, -0.7499,  0.0049,  0.0344, -0.0247,\n","          0.1953],\n","        [-0.7679, -0.1549,  0.2775,  0.8566,  0.5913, -1.0370,  0.1034, -1.4171,\n","          1.1662],\n","        [-1.0452,  0.1976, -0.2851, -0.9386, -0.3229,  0.0218, -0.3190, -1.0442,\n","          0.5481],\n","        [-0.0509,  0.7913, -0.4732, -1.8062,  0.4193, -0.0073,  0.9830, -0.7852,\n","          0.0824],\n","        [ 0.3193, -0.1495, -0.5936,  0.0191,  0.4712,  1.3813,  1.3844, -1.3303,\n","          0.0050],\n","        [-0.3271, -0.4476, -0.4540, -0.1913, -0.3422, -0.3413,  0.1022,  0.4571,\n","         -0.1929]])\n","first_linear.bias tensor([ 1.7352,  1.0740, -1.1912,  0.9287, -1.3562,  1.2138, -0.9443,  0.9505,\n","        -0.8716,  1.6132])\n","linear_hidden.0.weight tensor([[ 0.7231,  0.9439, -0.5970, -1.3527,  0.6172, -0.9076,  0.9748, -1.0534,\n","          0.5138,  0.3027],\n","        [ 0.8689, -0.4391, -0.1983,  1.1631,  0.5952,  0.4832, -1.4590, -0.5588,\n","          0.9332,  0.5302],\n","        [-0.8724,  0.0455,  0.3799,  0.1915, -1.1616, -0.2984, -0.7537,  1.5620,\n","         -1.0698, -0.4202],\n","        [-1.8157, -0.7953, -0.2145,  1.1293, -0.9410,  1.2508,  0.0771,  0.2649,\n","         -0.8349, -0.7436],\n","        [ 0.4907,  0.7257, -1.0472, -0.6159,  0.4488, -0.6015, -0.2670,  0.2167,\n","          0.0775,  0.9381],\n","        [ 0.4332, -0.7624, -0.9646, -1.6243,  1.0871, -1.1041, -0.2744, -1.4808,\n","         -1.0624,  0.5608],\n","        [ 0.6631, -0.3314, -0.2886, -0.0025, -0.3208,  0.3365, -1.5878, -0.6112,\n","          1.0090,  0.6936],\n","        [-0.5278, -0.2698,  0.3254,  0.0257, -0.0899, -0.0824,  0.1922, -0.2500,\n","          0.0144, -0.3737],\n","        [ 0.5856,  0.3367, -0.3064, -0.0255,  0.0862, -0.1543, -0.0972,  0.2819,\n","         -0.2183,  0.4133],\n","        [-0.5290, -0.2910,  0.4155,  0.0624, -0.1254,  0.0117,  0.0883, -0.2306,\n","          0.0318, -0.4900]])\n","linear_hidden.0.bias tensor([ 0.9607,  0.6384, -0.2639,  0.3646,  0.8414,  0.1354,  0.3809, -0.6790,\n","         0.7337, -0.4629])\n","linear_output.weight tensor([[-1.6737, -1.0559,  1.4924,  2.6939, -1.3337, -2.2874, -0.7693,  0.8821,\n","         -1.0578,  0.6325]])\n","linear_output.bias tensor([1.3568])\n","Testing out: \n","batch_size:  2034\n","train_size:  3720\n","n_epochs:  115\n","lr:  0.02914498346362329\n","weight_decay:  0.0005406568046626602\n","betas0:  0.9997078094893725\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0005,  0.3311, -0.1240],\n","          [ 0.1141,  0.2740,  0.0479],\n","          [ 0.3134, -0.2187, -0.2294]]]])\n","conv1.bias tensor([-0.1793])\n","first_linear.weight tensor([[ 0.0928,  0.2587, -0.1263, -0.3301,  0.2670, -0.1470, -0.2814, -0.3019,\n","         -0.3059],\n","        [ 0.0932, -0.0520,  0.0731,  0.2533,  0.2903, -0.1891,  0.0198,  0.1038,\n","          0.2454],\n","        [ 0.2475, -0.0450, -0.0915, -0.1847, -0.1860, -0.2290, -0.2319,  0.2607,\n","          0.2981],\n","        [ 0.3288, -0.1705, -0.0461,  0.3231,  0.3267,  0.3000,  0.1726, -0.2131,\n","         -0.0836],\n","        [ 0.1959,  0.1421,  0.0937,  0.0783, -0.1638,  0.1324, -0.2241, -0.1184,\n","         -0.1454],\n","        [-0.1821,  0.1646,  0.3297, -0.0297, -0.1044,  0.2092, -0.0205, -0.2672,\n","          0.2012],\n","        [ 0.2615,  0.1161, -0.1071,  0.0523,  0.0354, -0.3244, -0.0033,  0.1683,\n","         -0.0093],\n","        [ 0.1643, -0.0649,  0.2351, -0.2963,  0.0415, -0.2162, -0.2625,  0.0707,\n","          0.1239],\n","        [-0.3033, -0.0145,  0.2060,  0.0013, -0.1037, -0.0089, -0.2061,  0.1224,\n","          0.0730],\n","        [ 0.0648, -0.0039,  0.0726,  0.0537,  0.1474,  0.0980, -0.2647,  0.2169,\n","         -0.0768]])\n","first_linear.bias tensor([ 0.1972, -0.3249, -0.1433,  0.1999, -0.1788, -0.0652, -0.0797,  0.2577,\n","         0.1875, -0.3047])\n","linear_hidden.0.weight tensor([[ 0.2573,  0.2334, -0.0412,  0.3156,  0.0457, -0.2879, -0.3144,  0.1452,\n","         -0.2302, -0.2016],\n","        [ 0.2010, -0.1586, -0.1492, -0.1990, -0.0754,  0.1607, -0.2504,  0.1200,\n","          0.3032,  0.2268],\n","        [ 0.0385, -0.1546,  0.0608,  0.0063,  0.1206,  0.1685, -0.0282,  0.0380,\n","          0.2385,  0.1640],\n","        [ 0.1482, -0.2067,  0.1585,  0.3118,  0.1630, -0.1584,  0.0878, -0.1163,\n","          0.1437,  0.2555],\n","        [-0.0289, -0.1127,  0.1050,  0.1819, -0.0609, -0.2924,  0.0891,  0.2241,\n","         -0.2237, -0.1126],\n","        [ 0.2923, -0.2371, -0.1642,  0.0213, -0.1025, -0.1694, -0.1429,  0.0773,\n","         -0.1946, -0.2910],\n","        [-0.0974, -0.1808, -0.1402,  0.0780, -0.1017, -0.0014,  0.2267, -0.1306,\n","         -0.1498, -0.1100],\n","        [-0.2743, -0.2168, -0.1992,  0.2477,  0.0626, -0.1236, -0.1525,  0.1299,\n","         -0.2053, -0.2175],\n","        [ 0.0143, -0.0017,  0.0302,  0.1235,  0.1040,  0.2801,  0.2171, -0.1860,\n","         -0.3078, -0.3021],\n","        [ 0.2987, -0.2206, -0.0632, -0.1146,  0.1139,  0.2768, -0.1804,  0.1641,\n","          0.2357,  0.0709]])\n","linear_hidden.0.bias tensor([ 0.0502, -0.2192,  0.0015,  0.0449,  0.0131,  0.2238, -0.0097, -0.2983,\n","         0.1329, -0.2499])\n","linear_output.weight tensor([[ 0.1284, -0.2526, -0.1903,  0.0971, -0.2929, -0.1003,  0.1364, -0.0401,\n","          0.0962,  0.0556]])\n","linear_output.bias tensor([-0.0340])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2034])) that is different to the input size (torch.Size([2034, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1686])) that is different to the input size (torch.Size([1686, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6939334273338318 \t Validate_Accuracy: 0.5045\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6930422186851501 \t Validate_Accuracy: 0.549\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6895143389701843 \t Validate_Accuracy: 0.5635\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6822512745857239 \t Validate_Accuracy: 0.613\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6720125675201416 \t Validate_Accuracy: 0.696\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6563305258750916 \t Validate_Accuracy: 0.731\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6294540166854858 \t Validate_Accuracy: 0.7805\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5963761806488037 \t Validate_Accuracy: 0.7945\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5550357699394226 \t Validate_Accuracy: 0.8045\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.514647051692009 \t Validate_Accuracy: 0.8155\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.4805714040994644 \t Validate_Accuracy: 0.8085\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.46248410642147064 \t Validate_Accuracy: 0.816\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.456659734249115 \t Validate_Accuracy: 0.806\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.4487292915582657 \t Validate_Accuracy: 0.819\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.43792857229709625 \t Validate_Accuracy: 0.806\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.4314843416213989 \t Validate_Accuracy: 0.8215\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.4244094341993332 \t Validate_Accuracy: 0.807\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.41124218702316284 \t Validate_Accuracy: 0.8245\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.397651270031929 \t Validate_Accuracy: 0.818\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.38633573055267334 \t Validate_Accuracy: 0.832\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.37902921438217163 \t Validate_Accuracy: 0.8305\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.37035730481147766 \t Validate_Accuracy: 0.84\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.36629152297973633 \t Validate_Accuracy: 0.831\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.35980646312236786 \t Validate_Accuracy: 0.835\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.35558246076107025 \t Validate_Accuracy: 0.8335\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.34551437199115753 \t Validate_Accuracy: 0.833\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.34136712551116943 \t Validate_Accuracy: 0.849\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.331946462392807 \t Validate_Accuracy: 0.8445\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3286066949367523 \t Validate_Accuracy: 0.8505\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.32343120872974396 \t Validate_Accuracy: 0.849\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.3175168037414551 \t Validate_Accuracy: 0.853\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.3175171762704849 \t Validate_Accuracy: 0.86\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.31007906794548035 \t Validate_Accuracy: 0.8495\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3063661903142929 \t Validate_Accuracy: 0.8605\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.30555351078510284 \t Validate_Accuracy: 0.8495\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.30213145911693573 \t Validate_Accuracy: 0.8545\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.29641208052635193 \t Validate_Accuracy: 0.855\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.2962707430124283 \t Validate_Accuracy: 0.858\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.2911050468683243 \t Validate_Accuracy: 0.8605\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.291624516248703 \t Validate_Accuracy: 0.856\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.28685787320137024 \t Validate_Accuracy: 0.8575\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.28560036420822144 \t Validate_Accuracy: 0.864\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2839839309453964 \t Validate_Accuracy: 0.866\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.28106163442134857 \t Validate_Accuracy: 0.864\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.28096000850200653 \t Validate_Accuracy: 0.865\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.276767373085022 \t Validate_Accuracy: 0.8675\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2784632444381714 \t Validate_Accuracy: 0.862\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.27677007019519806 \t Validate_Accuracy: 0.8645\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.2716372609138489 \t Validate_Accuracy: 0.8635\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.27002881467342377 \t Validate_Accuracy: 0.862\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2665608376264572 \t Validate_Accuracy: 0.864\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2648322582244873 \t Validate_Accuracy: 0.863\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.26750122010707855 \t Validate_Accuracy: 0.8645\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2608402371406555 \t Validate_Accuracy: 0.868\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.255985364317894 \t Validate_Accuracy: 0.863\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.257486492395401 \t Validate_Accuracy: 0.867\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.25273966789245605 \t Validate_Accuracy: 0.87\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2486419528722763 \t Validate_Accuracy: 0.8685\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.24557346105575562 \t Validate_Accuracy: 0.874\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.24107809364795685 \t Validate_Accuracy: 0.874\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.24158386141061783 \t Validate_Accuracy: 0.876\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.2371607944369316 \t Validate_Accuracy: 0.876\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.23503290116786957 \t Validate_Accuracy: 0.8745\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2356240302324295 \t Validate_Accuracy: 0.8785\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.23317058384418488 \t Validate_Accuracy: 0.88\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2309427186846733 \t Validate_Accuracy: 0.88\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.23074128478765488 \t Validate_Accuracy: 0.8825\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.23019234836101532 \t Validate_Accuracy: 0.8795\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.22955196350812912 \t Validate_Accuracy: 0.8785\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2299213707447052 \t Validate_Accuracy: 0.884\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.22696100920438766 \t Validate_Accuracy: 0.881\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.2250530645251274 \t Validate_Accuracy: 0.88\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.222459077835083 \t Validate_Accuracy: 0.8825\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.22210703045129776 \t Validate_Accuracy: 0.886\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2245941460132599 \t Validate_Accuracy: 0.885\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.22077368944883347 \t Validate_Accuracy: 0.8845\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.22356489300727844 \t Validate_Accuracy: 0.8815\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.22567622363567352 \t Validate_Accuracy: 0.8815\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.22543903440237045 \t Validate_Accuracy: 0.883\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.22490769624710083 \t Validate_Accuracy: 0.888\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.22162076085805893 \t Validate_Accuracy: 0.888\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.21968720853328705 \t Validate_Accuracy: 0.8865\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.22070614248514175 \t Validate_Accuracy: 0.882\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.22637668251991272 \t Validate_Accuracy: 0.888\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.22177748382091522 \t Validate_Accuracy: 0.885\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.21911947429180145 \t Validate_Accuracy: 0.8885\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2222357764840126 \t Validate_Accuracy: 0.889\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.21761681139469147 \t Validate_Accuracy: 0.887\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.21763072162866592 \t Validate_Accuracy: 0.8905\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.21973057836294174 \t Validate_Accuracy: 0.888\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2169579640030861 \t Validate_Accuracy: 0.89\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.21877558529376984 \t Validate_Accuracy: 0.8875\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.21631526947021484 \t Validate_Accuracy: 0.8905\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.21612819284200668 \t Validate_Accuracy: 0.888\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.21491775661706924 \t Validate_Accuracy: 0.89\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.21177200973033905 \t Validate_Accuracy: 0.886\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.21267299354076385 \t Validate_Accuracy: 0.887\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.21337225288152695 \t Validate_Accuracy: 0.889\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.21356862783432007 \t Validate_Accuracy: 0.886\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.21390727162361145 \t Validate_Accuracy: 0.8855\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.21289339661598206 \t Validate_Accuracy: 0.888\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2116895616054535 \t Validate_Accuracy: 0.888\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.21099936217069626 \t Validate_Accuracy: 0.8865\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.21216588467359543 \t Validate_Accuracy: 0.891\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.21128705888986588 \t Validate_Accuracy: 0.8905\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.21174843609333038 \t Validate_Accuracy: 0.8895\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2096930891275406 \t Validate_Accuracy: 0.889\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2137724682688713 \t Validate_Accuracy: 0.888\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.21395233273506165 \t Validate_Accuracy: 0.891\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.21130961924791336 \t Validate_Accuracy: 0.89\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2112778201699257 \t Validate_Accuracy: 0.892\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.21029697358608246 \t Validate_Accuracy: 0.89\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.20828595012426376 \t Validate_Accuracy: 0.8915\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.20955386757850647 \t Validate_Accuracy: 0.8905\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.21064645797014236 \t Validate_Accuracy: 0.89\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1446,  0.0617,  0.1727],\n","          [ 0.0789, -0.0059,  0.0492],\n","          [ 0.1340,  0.0593,  0.1534]]]])\n","conv1.bias tensor([-0.0725])\n","first_linear.weight tensor([[-1.4221e-01,  4.0838e-01, -5.2314e-01, -6.0374e-01,  2.6779e-01,\n","          4.8232e-01,  9.4176e-01, -1.0746e+00,  1.4917e-01],\n","        [ 6.3236e-02,  8.5346e-02,  1.5812e-01,  6.9444e-02,  1.2726e-01,\n","          1.1103e-01, -1.3135e-03,  5.7056e-02,  1.5628e-01],\n","        [-5.4680e-01, -1.9824e-02, -2.2862e-02,  1.5869e+00, -5.9402e-01,\n","          1.5801e-01, -4.5077e-01, -1.4260e-02,  4.9507e-02],\n","        [ 3.0532e-01, -8.6260e-01,  5.9119e-02,  1.2079e-01,  2.7511e-01,\n","          6.5209e-01,  2.3715e-01, -6.0328e-01, -7.2457e-02],\n","        [-3.6344e-02,  9.5857e-02,  1.1203e-01, -7.2804e-02, -1.1263e-01,\n","          1.8489e-02, -1.7560e-01, -1.9304e-01, -1.1246e-01],\n","        [ 1.5783e-01,  1.2957e-01,  1.1267e-01,  1.7972e-01,  7.0590e-02,\n","          7.6373e-02,  4.3825e-02, -5.9142e-02, -3.5371e-02],\n","        [-6.7897e-01,  5.0843e-01,  1.1942e-01,  2.8792e-01,  3.6189e-01,\n","         -7.8491e-01,  1.1366e-01, -7.9198e-01,  7.1609e-01],\n","        [ 2.9992e-01, -1.0760e+00,  5.8347e-01, -2.6313e-01,  9.4173e-01,\n","         -9.6279e-01, -1.6748e-02, -4.1845e-01,  3.2919e-01],\n","        [ 2.7012e-02, -2.4239e-03,  1.6639e-01,  1.6720e-01,  1.7954e-01,\n","          2.1300e-01,  1.0732e-01,  4.7870e-02,  1.9838e-01],\n","        [ 3.0728e-01,  4.1122e-01,  1.7599e-01, -2.1202e-01,  2.5998e-01,\n","          4.9430e-01,  2.9187e-01,  2.6949e-01,  2.7811e-01]])\n","first_linear.bias tensor([ 0.9672, -1.1393,  0.9460, -0.7355, -0.9612,  0.8704, -0.9356,  0.9317,\n","         0.7972, -0.5518])\n","linear_hidden.0.weight tensor([[ 0.6547,  0.8557,  0.6000, -0.2880,  0.4930, -0.4687, -0.6370,  0.6552,\n","         -0.4734,  0.3172],\n","        [-0.4275, -0.8303, -0.6407,  0.4073, -0.5135,  0.4574,  0.4534, -0.6016,\n","          0.5049, -0.3425],\n","        [-0.5160, -0.8244, -0.6252,  0.4469, -0.4309,  0.4970,  0.5320, -0.6420,\n","          0.4734, -0.3721],\n","        [ 0.4870,  0.4423,  0.5277, -0.2231,  0.3961, -0.4071, -0.4734,  0.5327,\n","         -0.3572,  0.3503],\n","        [-0.5947, -0.8459, -0.6873,  0.5502, -0.6251,  0.4504,  0.5895, -0.6655,\n","          0.4186, -0.4833],\n","        [ 0.0825, -0.4799, -0.0517, -0.1010, -0.1209,  0.0743, -0.0429, -0.1354,\n","          0.0526, -0.3224],\n","        [-0.4754, -0.6575, -0.5836,  0.4811, -0.4460,  0.4298,  0.5486, -0.6144,\n","          0.4307, -0.4674],\n","        [-0.6324, -0.8444, -0.5497,  0.6387, -0.4410,  0.4164,  0.4308, -0.5551,\n","          0.3667, -0.5479],\n","        [ 0.4627,  0.6574,  0.4962, -0.3993,  0.3830, -0.3984, -0.4522,  0.5118,\n","         -0.3736,  0.3546],\n","        [ 0.5444,  0.3957,  0.6664, -0.5792,  0.6871, -0.3391, -0.4508,  0.5723,\n","         -0.3099,  0.2528]])\n","linear_hidden.0.bias tensor([ 0.0781, -0.2311, -0.0653, -0.1000, -0.0503,  0.4915,  0.0268, -0.1812,\n","         0.0253, -0.3588])\n","linear_output.weight tensor([[ 1.4862, -1.3896, -1.5164,  1.1621, -1.7273, -0.6887, -1.5526, -1.3557,\n","          1.2182,  1.1227]])\n","linear_output.bias tensor([0.1992])\n","Testing out: \n","batch_size:  1585\n","train_size:  3056\n","n_epochs:  209\n","lr:  0.013234500568515292\n","weight_decay:  0.0006513595941668176\n","betas0:  0.9855344376874151\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2252, -0.0517, -0.3286],\n","          [ 0.3105, -0.0653, -0.1069],\n","          [-0.1653,  0.2608, -0.1173]]]])\n","conv1.bias tensor([-0.1096])\n","first_linear.weight tensor([[ 0.0162,  0.0734, -0.0894, -0.0977,  0.3228,  0.0266, -0.2966,  0.1412,\n","         -0.1574],\n","        [ 0.0774,  0.1501,  0.0935,  0.2444,  0.0148, -0.2190, -0.1345, -0.0223,\n","         -0.2892],\n","        [ 0.2587, -0.3295, -0.1649,  0.2885, -0.0219,  0.0710,  0.0328,  0.1306,\n","         -0.1735],\n","        [ 0.0936, -0.2328,  0.3244, -0.1380, -0.1655, -0.0276,  0.2046, -0.0527,\n","         -0.3133],\n","        [-0.0121, -0.2650, -0.1556,  0.3124,  0.1609,  0.3321,  0.3196,  0.1004,\n","          0.2184],\n","        [ 0.3020, -0.1812,  0.3101, -0.1120,  0.3003, -0.0652,  0.1013,  0.2966,\n","         -0.1626],\n","        [ 0.3042,  0.0415,  0.2281, -0.1094, -0.0800, -0.1574,  0.2557, -0.3162,\n","         -0.3029],\n","        [ 0.0996, -0.2843,  0.0480,  0.0241,  0.1221,  0.1226, -0.0227,  0.2375,\n","         -0.1620],\n","        [ 0.2662, -0.0652, -0.0107,  0.2848, -0.0270,  0.1357, -0.3187,  0.2402,\n","         -0.0336],\n","        [-0.1685, -0.3065, -0.1347, -0.0927,  0.0221,  0.1965,  0.1315, -0.1812,\n","         -0.2487]])\n","first_linear.bias tensor([-0.0638,  0.0418, -0.1929,  0.0079,  0.0469,  0.0413, -0.2596, -0.0867,\n","        -0.1359,  0.2448])\n","linear_hidden.0.weight tensor([[-0.0663,  0.2573, -0.0326, -0.1377,  0.2228,  0.2566, -0.2599, -0.1961,\n","         -0.1711, -0.2145],\n","        [-0.3127,  0.1063,  0.2115,  0.2030, -0.0305, -0.0492,  0.1503, -0.2135,\n","         -0.0840,  0.0013],\n","        [ 0.0874, -0.1293, -0.1622, -0.1528,  0.0305,  0.0041, -0.2228,  0.0372,\n","         -0.3111,  0.2215],\n","        [-0.2709,  0.2908, -0.0515, -0.1683, -0.1751,  0.0081,  0.2344, -0.1990,\n","         -0.0148, -0.0197],\n","        [-0.0665,  0.2778, -0.0415, -0.1841,  0.0019, -0.3051, -0.2102,  0.1329,\n","          0.0823,  0.2814],\n","        [-0.2277, -0.2624, -0.0543,  0.1529,  0.2113, -0.2104, -0.2968,  0.1997,\n","          0.1369,  0.3160],\n","        [-0.0145,  0.2315, -0.0178, -0.1382, -0.2094,  0.0914,  0.0146,  0.2993,\n","          0.1613, -0.0137],\n","        [-0.2857,  0.1536, -0.1954, -0.1762, -0.0856, -0.1341, -0.1076,  0.0323,\n","         -0.2338, -0.1942],\n","        [-0.0745, -0.2119, -0.2189,  0.0209, -0.2711, -0.0186,  0.1717,  0.1129,\n","         -0.1383,  0.0829],\n","        [ 0.0291,  0.1632, -0.0350,  0.1526,  0.2190,  0.0229,  0.2152, -0.1581,\n","          0.0026, -0.1222]])\n","linear_hidden.0.bias tensor([-0.0139, -0.0310,  0.0157,  0.0809,  0.3076,  0.1318, -0.2589,  0.2936,\n","         0.2355, -0.1763])\n","linear_output.weight tensor([[-0.2961, -0.0988,  0.1122,  0.0984, -0.0444,  0.2550,  0.0170, -0.0892,\n","          0.1437,  0.1769]])\n","linear_output.bias tensor([0.2432])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1585])) that is different to the input size (torch.Size([1585, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1471])) that is different to the input size (torch.Size([1471, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.699898362159729 \t Validate_Accuracy: 0.4975\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6939094066619873 \t Validate_Accuracy: 0.4965\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6938530504703522 \t Validate_Accuracy: 0.503\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6941068172454834 \t Validate_Accuracy: 0.5025\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6932136416435242 \t Validate_Accuracy: 0.4935\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6923547387123108 \t Validate_Accuracy: 0.51\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.692139059305191 \t Validate_Accuracy: 0.533\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6922759711742401 \t Validate_Accuracy: 0.5265\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6921650469303131 \t Validate_Accuracy: 0.528\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6918770372867584 \t Validate_Accuracy: 0.523\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6914798021316528 \t Validate_Accuracy: 0.5025\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6911638081073761 \t Validate_Accuracy: 0.486\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6908861398696899 \t Validate_Accuracy: 0.4865\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6907860040664673 \t Validate_Accuracy: 0.4875\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6907306611537933 \t Validate_Accuracy: 0.492\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6906131505966187 \t Validate_Accuracy: 0.495\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6904737055301666 \t Validate_Accuracy: 0.4955\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6903631687164307 \t Validate_Accuracy: 0.496\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.690362811088562 \t Validate_Accuracy: 0.5\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6903230249881744 \t Validate_Accuracy: 0.499\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6902233362197876 \t Validate_Accuracy: 0.492\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6901313364505768 \t Validate_Accuracy: 0.4905\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.69001704454422 \t Validate_Accuracy: 0.489\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6900072395801544 \t Validate_Accuracy: 0.4875\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6898954808712006 \t Validate_Accuracy: 0.4875\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6898865699768066 \t Validate_Accuracy: 0.497\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6897268891334534 \t Validate_Accuracy: 0.4975\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6896124482154846 \t Validate_Accuracy: 0.4945\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6894152462482452 \t Validate_Accuracy: 0.492\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6893797218799591 \t Validate_Accuracy: 0.493\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6892311275005341 \t Validate_Accuracy: 0.494\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6890495717525482 \t Validate_Accuracy: 0.4905\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6889605224132538 \t Validate_Accuracy: 0.4955\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6886486113071442 \t Validate_Accuracy: 0.4915\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.688549667596817 \t Validate_Accuracy: 0.492\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6880104541778564 \t Validate_Accuracy: 0.493\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6877564489841461 \t Validate_Accuracy: 0.494\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.687250941991806 \t Validate_Accuracy: 0.5025\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6861921548843384 \t Validate_Accuracy: 0.505\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.684831976890564 \t Validate_Accuracy: 0.4995\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.680999368429184 \t Validate_Accuracy: 0.4985\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6742833256721497 \t Validate_Accuracy: 0.49\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6650262475013733 \t Validate_Accuracy: 0.5415\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6537135541439056 \t Validate_Accuracy: 0.573\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6369522511959076 \t Validate_Accuracy: 0.7105\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6122360825538635 \t Validate_Accuracy: 0.718\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.5841546654701233 \t Validate_Accuracy: 0.7665\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.5531925857067108 \t Validate_Accuracy: 0.7875\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.5219683945178986 \t Validate_Accuracy: 0.7895\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.48946473002433777 \t Validate_Accuracy: 0.7895\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.4603537321090698 \t Validate_Accuracy: 0.807\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.44159696996212006 \t Validate_Accuracy: 0.798\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.4274524599313736 \t Validate_Accuracy: 0.8065\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.41857101023197174 \t Validate_Accuracy: 0.8085\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.4129667580127716 \t Validate_Accuracy: 0.803\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.40853187441825867 \t Validate_Accuracy: 0.8065\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.40552718937397003 \t Validate_Accuracy: 0.797\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.40417051315307617 \t Validate_Accuracy: 0.8095\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.39863988757133484 \t Validate_Accuracy: 0.816\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.3934451639652252 \t Validate_Accuracy: 0.802\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.38888634741306305 \t Validate_Accuracy: 0.817\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.3843816965818405 \t Validate_Accuracy: 0.8175\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.3807365447282791 \t Validate_Accuracy: 0.8175\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.3764156997203827 \t Validate_Accuracy: 0.8255\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.37253338098526 \t Validate_Accuracy: 0.8205\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.3694082200527191 \t Validate_Accuracy: 0.827\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3667858988046646 \t Validate_Accuracy: 0.8275\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.3652730733156204 \t Validate_Accuracy: 0.8275\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.36067913472652435 \t Validate_Accuracy: 0.8295\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.3582754284143448 \t Validate_Accuracy: 0.838\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.35485948622226715 \t Validate_Accuracy: 0.8395\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.3510092496871948 \t Validate_Accuracy: 0.8375\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.3474155217409134 \t Validate_Accuracy: 0.839\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.34363414347171783 \t Validate_Accuracy: 0.8435\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.34032806754112244 \t Validate_Accuracy: 0.844\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.33460085093975067 \t Validate_Accuracy: 0.8425\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.32992421090602875 \t Validate_Accuracy: 0.8485\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.327947735786438 \t Validate_Accuracy: 0.853\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.3213372528553009 \t Validate_Accuracy: 0.85\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.31665751338005066 \t Validate_Accuracy: 0.852\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.31342123448848724 \t Validate_Accuracy: 0.858\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.30774661898612976 \t Validate_Accuracy: 0.8565\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.3021092712879181 \t Validate_Accuracy: 0.86\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.3000328689813614 \t Validate_Accuracy: 0.861\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2948983907699585 \t Validate_Accuracy: 0.859\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.29216769337654114 \t Validate_Accuracy: 0.8605\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2915411591529846 \t Validate_Accuracy: 0.8575\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.287783220410347 \t Validate_Accuracy: 0.863\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2842179387807846 \t Validate_Accuracy: 0.859\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.28291700780391693 \t Validate_Accuracy: 0.857\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2808702141046524 \t Validate_Accuracy: 0.859\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.27658621966838837 \t Validate_Accuracy: 0.8575\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.27634286880493164 \t Validate_Accuracy: 0.858\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.27393724024295807 \t Validate_Accuracy: 0.8605\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.27114008367061615 \t Validate_Accuracy: 0.862\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.26912641525268555 \t Validate_Accuracy: 0.8595\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.26777979731559753 \t Validate_Accuracy: 0.862\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2686201483011246 \t Validate_Accuracy: 0.862\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.2637691795825958 \t Validate_Accuracy: 0.8615\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2666803002357483 \t Validate_Accuracy: 0.8645\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.26262225210666656 \t Validate_Accuracy: 0.864\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.26181741058826447 \t Validate_Accuracy: 0.8625\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.2597383111715317 \t Validate_Accuracy: 0.8645\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.26045575737953186 \t Validate_Accuracy: 0.864\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.25909423828125 \t Validate_Accuracy: 0.8685\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.25765618681907654 \t Validate_Accuracy: 0.8675\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2582133412361145 \t Validate_Accuracy: 0.866\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.25590938329696655 \t Validate_Accuracy: 0.8665\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2558232322335243 \t Validate_Accuracy: 0.863\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.25362733006477356 \t Validate_Accuracy: 0.868\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2553885206580162 \t Validate_Accuracy: 0.8685\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.2541457414627075 \t Validate_Accuracy: 0.8705\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2524925023317337 \t Validate_Accuracy: 0.8695\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2521910220384598 \t Validate_Accuracy: 0.8695\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2527915835380554 \t Validate_Accuracy: 0.873\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.25203704088926315 \t Validate_Accuracy: 0.873\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2527763396501541 \t Validate_Accuracy: 0.871\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.2495776042342186 \t Validate_Accuracy: 0.872\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.24811167269945145 \t Validate_Accuracy: 0.8725\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.24819980561733246 \t Validate_Accuracy: 0.876\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.24701876193284988 \t Validate_Accuracy: 0.874\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.24497590214014053 \t Validate_Accuracy: 0.873\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.24402707815170288 \t Validate_Accuracy: 0.8755\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.24436336755752563 \t Validate_Accuracy: 0.8745\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.24463197588920593 \t Validate_Accuracy: 0.875\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2415078803896904 \t Validate_Accuracy: 0.8765\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.23970217257738113 \t Validate_Accuracy: 0.876\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.2381727173924446 \t Validate_Accuracy: 0.8795\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23727916926145554 \t Validate_Accuracy: 0.8765\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.23587284237146378 \t Validate_Accuracy: 0.8845\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.23553773015737534 \t Validate_Accuracy: 0.883\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.23127148300409317 \t Validate_Accuracy: 0.883\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.22980403155088425 \t Validate_Accuracy: 0.8815\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.22791239619255066 \t Validate_Accuracy: 0.883\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.22607436031103134 \t Validate_Accuracy: 0.8815\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.22895952314138412 \t Validate_Accuracy: 0.8825\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.2259793058037758 \t Validate_Accuracy: 0.8815\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.22324416786432266 \t Validate_Accuracy: 0.8825\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.2193506956100464 \t Validate_Accuracy: 0.8815\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.22302841395139694 \t Validate_Accuracy: 0.882\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.22223152965307236 \t Validate_Accuracy: 0.8845\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.22314920276403427 \t Validate_Accuracy: 0.882\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.21703868359327316 \t Validate_Accuracy: 0.8825\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.21874022483825684 \t Validate_Accuracy: 0.8825\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.22343376278877258 \t Validate_Accuracy: 0.883\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.21920428425073624 \t Validate_Accuracy: 0.883\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.2177337482571602 \t Validate_Accuracy: 0.884\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.21674474328756332 \t Validate_Accuracy: 0.8835\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.21694635599851608 \t Validate_Accuracy: 0.8845\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.21408693492412567 \t Validate_Accuracy: 0.8815\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2134026288986206 \t Validate_Accuracy: 0.882\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.21250810474157333 \t Validate_Accuracy: 0.8865\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.21288679540157318 \t Validate_Accuracy: 0.883\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.2133677899837494 \t Validate_Accuracy: 0.8835\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.20983736962080002 \t Validate_Accuracy: 0.885\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.21195431798696518 \t Validate_Accuracy: 0.885\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.21259373426437378 \t Validate_Accuracy: 0.885\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.21466155350208282 \t Validate_Accuracy: 0.883\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.21374209225177765 \t Validate_Accuracy: 0.8835\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.21154482662677765 \t Validate_Accuracy: 0.8845\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.21256382018327713 \t Validate_Accuracy: 0.885\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.20969173312187195 \t Validate_Accuracy: 0.885\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.20961809903383255 \t Validate_Accuracy: 0.8885\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.20908204466104507 \t Validate_Accuracy: 0.8855\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.2094012051820755 \t Validate_Accuracy: 0.8855\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.2084922194480896 \t Validate_Accuracy: 0.884\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.20852618664503098 \t Validate_Accuracy: 0.885\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.20839952677488327 \t Validate_Accuracy: 0.8845\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.20852329581975937 \t Validate_Accuracy: 0.886\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.20771963894367218 \t Validate_Accuracy: 0.8865\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.20759496837854385 \t Validate_Accuracy: 0.885\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.20777802169322968 \t Validate_Accuracy: 0.8855\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.20758480578660965 \t Validate_Accuracy: 0.887\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.20754766464233398 \t Validate_Accuracy: 0.8845\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.2096586972475052 \t Validate_Accuracy: 0.887\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2093619555234909 \t Validate_Accuracy: 0.8845\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.207561694085598 \t Validate_Accuracy: 0.8865\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.2088305503129959 \t Validate_Accuracy: 0.8865\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.20836902409791946 \t Validate_Accuracy: 0.886\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.2080681025981903 \t Validate_Accuracy: 0.8855\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.2070162519812584 \t Validate_Accuracy: 0.887\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.20815574377775192 \t Validate_Accuracy: 0.8855\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.20800330489873886 \t Validate_Accuracy: 0.8865\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.2074800357222557 \t Validate_Accuracy: 0.8865\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.20729884505271912 \t Validate_Accuracy: 0.887\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.20828253030776978 \t Validate_Accuracy: 0.8865\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.20812398940324783 \t Validate_Accuracy: 0.887\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.20850416272878647 \t Validate_Accuracy: 0.887\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.20721472799777985 \t Validate_Accuracy: 0.886\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.20771201699972153 \t Validate_Accuracy: 0.8865\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.20794077217578888 \t Validate_Accuracy: 0.884\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.20860276371240616 \t Validate_Accuracy: 0.886\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.2065659537911415 \t Validate_Accuracy: 0.886\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.20705291628837585 \t Validate_Accuracy: 0.883\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.20675016194581985 \t Validate_Accuracy: 0.887\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.20819073915481567 \t Validate_Accuracy: 0.8875\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.20700079202651978 \t Validate_Accuracy: 0.886\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.20772486925125122 \t Validate_Accuracy: 0.889\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.2060268372297287 \t Validate_Accuracy: 0.8845\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.2066623941063881 \t Validate_Accuracy: 0.8855\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.20761796832084656 \t Validate_Accuracy: 0.8835\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.20679528266191483 \t Validate_Accuracy: 0.8845\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.20636636018753052 \t Validate_Accuracy: 0.8855\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.20706717669963837 \t Validate_Accuracy: 0.8855\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.2056414932012558 \t Validate_Accuracy: 0.8865\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.20726419240236282 \t Validate_Accuracy: 0.885\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.2077864110469818 \t Validate_Accuracy: 0.8865\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.20696499943733215 \t Validate_Accuracy: 0.8885\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.20579012483358383 \t Validate_Accuracy: 0.8855\n","model parameters! \n","\n","conv1.weight tensor([[[[0.1497, 0.0586, 0.1819],\n","          [0.0722, 0.0046, 0.0500],\n","          [0.1362, 0.0364, 0.1788]]]])\n","conv1.bias tensor([-0.1150])\n","first_linear.weight tensor([[-0.1187,  0.5142, -0.3893, -1.0305,  0.3685, -0.0017,  0.8937, -0.7874,\n","          0.1835],\n","        [ 0.2306,  0.4667, -0.1711, -0.1101, -0.6862,  0.2682, -0.3295,  1.2089,\n","         -0.4928],\n","        [-0.0737, -0.1388, -0.0759, -0.0104,  0.0014, -0.1255, -0.1062, -0.1071,\n","         -0.0194],\n","        [ 0.1289,  0.0153,  0.2306, -0.0383,  0.1262,  0.1316, -0.2301, -0.3097,\n","         -0.1352],\n","        [-0.0830, -0.0762, -0.1777, -0.0483, -0.1566, -0.2390, -0.1254, -0.0987,\n","         -0.1827],\n","        [ 0.0188, -0.0402, -0.1928, -0.4383, -0.1535,  0.7037,  0.1455,  0.1848,\n","         -0.6433],\n","        [-0.2436, -0.1714, -0.1581, -0.0715, -0.2611, -0.2752, -0.1152, -0.2396,\n","         -0.1229],\n","        [ 0.5892, -1.2623,  0.3327, -0.1556,  0.3682,  0.0931, -0.0376, -0.0042,\n","         -0.1076],\n","        [-0.5081,  0.0302,  0.5291,  0.2753,  0.2672, -1.2551, -0.2745,  0.0837,\n","          0.5290],\n","        [ 0.6917, -0.4043,  0.4174, -1.2310,  0.7265, -0.5171,  0.0833,  0.1913,\n","         -0.0545]])\n","first_linear.bias tensor([-0.9049, -0.8799,  0.6006,  0.4932,  0.9887,  0.1181, -1.1765, -0.6545,\n","        -0.7430,  1.0076])\n","linear_hidden.0.weight tensor([[ 0.5842,  0.5427,  0.3589,  0.3494,  0.6708, -0.0957, -0.8678,  0.5106,\n","          0.3941, -0.5778],\n","        [-0.5588, -0.4733, -0.3687, -0.3494, -0.6305,  0.0980,  0.7582, -0.4928,\n","         -0.4145,  0.5595],\n","        [-0.5184, -0.3649, -0.3930, -0.3407, -0.5865,  0.1479,  0.5483, -0.5275,\n","         -0.5264,  0.5778],\n","        [-0.5725, -0.4956, -0.3703, -0.3542, -0.6357,  0.0923,  0.8026, -0.4921,\n","         -0.4103,  0.5562],\n","        [ 0.5727,  0.4890,  0.3480,  0.3274,  0.6009, -0.0336, -0.7928,  0.4925,\n","          0.4509, -0.5556],\n","        [-0.6130, -0.8617,  0.0932, -0.1156,  0.2529, -1.4370,  0.2465, -0.2645,\n","          1.2492,  0.6658],\n","        [-0.5548, -0.4728, -0.3458, -0.3272, -0.5987,  0.0582,  0.7466, -0.4770,\n","         -0.4259,  0.5460],\n","        [ 0.5947,  0.5153,  0.3570,  0.3359,  0.6168, -0.0381, -0.8109,  0.5155,\n","          0.4612, -0.5777],\n","        [-0.5245, -0.4268, -0.3723, -0.3466, -0.5969,  0.1066,  0.7173, -0.4760,\n","         -0.4140,  0.5328],\n","        [-0.5406, -0.3982, -0.2790, -0.2593, -0.4326, -0.0111,  0.8295, -0.4689,\n","         -0.4798,  0.4526]])\n","linear_hidden.0.bias tensor([ 0.0780, -0.1095, -0.2118, -0.0921,  0.1664,  0.2849, -0.1636,  0.1827,\n","        -0.0868, -0.2081])\n","linear_output.weight tensor([[-1.5110,  1.4424,  1.5075,  1.4503, -1.2774,  1.5434,  1.2905, -1.3582,\n","          1.3468,  1.1864]])\n","linear_output.bias tensor([-0.1414])\n","Testing out: \n","batch_size:  3184\n","train_size:  2757\n","n_epochs:  179\n","lr:  0.028509766122551922\n","weight_decay:  0.00010590671751334633\n","betas0:  0.996871721689921\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1789,  0.2525, -0.0460],\n","          [-0.3178,  0.1583,  0.1720],\n","          [-0.1342, -0.0712,  0.0491]]]])\n","conv1.bias tensor([0.0829])\n","first_linear.weight tensor([[ 0.2394,  0.1675, -0.1779,  0.2720,  0.1566,  0.1350,  0.0946,  0.1208,\n","         -0.0129],\n","        [-0.3153, -0.0747, -0.1296, -0.0339,  0.1311, -0.0350, -0.1252,  0.1588,\n","         -0.2715],\n","        [ 0.0729,  0.0613,  0.0209, -0.1811, -0.1717, -0.2045, -0.1573,  0.3124,\n","         -0.2645],\n","        [-0.1069,  0.1537, -0.1601,  0.2603,  0.3238,  0.2207, -0.2957, -0.0863,\n","          0.0465],\n","        [ 0.1322,  0.2340,  0.2800,  0.1757,  0.0185,  0.0854, -0.0983, -0.1054,\n","         -0.0975],\n","        [ 0.3300,  0.1176, -0.2588,  0.3134, -0.2611,  0.1519, -0.1754,  0.1956,\n","         -0.2523],\n","        [ 0.2209, -0.3016, -0.2887,  0.0496,  0.3138,  0.2093,  0.0110,  0.2038,\n","          0.2925],\n","        [ 0.2073, -0.2001,  0.0185,  0.0295,  0.2621,  0.0901,  0.1939,  0.0372,\n","         -0.0968],\n","        [ 0.3235, -0.0294,  0.1830, -0.1785, -0.2025, -0.1934,  0.0638, -0.2998,\n","          0.1166],\n","        [ 0.1230, -0.3260, -0.2876, -0.0869, -0.0924,  0.1411,  0.2156,  0.3235,\n","         -0.3156]])\n","first_linear.bias tensor([ 0.1138,  0.1087,  0.0235,  0.3323,  0.1547,  0.3163,  0.3203, -0.2824,\n","         0.1410, -0.0114])\n","linear_hidden.0.weight tensor([[ 0.2296,  0.1170, -0.1481, -0.1810,  0.0508,  0.0584, -0.0610,  0.3120,\n","         -0.3012,  0.0309],\n","        [ 0.2547, -0.1894,  0.2290,  0.2963,  0.1694, -0.0458, -0.2820,  0.2049,\n","         -0.1766,  0.0525],\n","        [ 0.2932, -0.0496,  0.1615,  0.1749, -0.0428, -0.2828, -0.0290,  0.0330,\n","         -0.0404, -0.1159],\n","        [ 0.1948, -0.1801,  0.0573,  0.0510,  0.1194,  0.0919, -0.2930, -0.3079,\n","         -0.1015,  0.1062],\n","        [ 0.1254,  0.2588,  0.3050, -0.3118,  0.1602,  0.1386, -0.1407, -0.1269,\n","         -0.2919,  0.0501],\n","        [ 0.0261, -0.0159, -0.2796,  0.1263, -0.2299,  0.2638, -0.1240, -0.1820,\n","         -0.0917,  0.0960],\n","        [ 0.2361, -0.0131, -0.1588, -0.2418,  0.2494,  0.0174,  0.3082, -0.0056,\n","         -0.0863, -0.2132],\n","        [ 0.1579, -0.1256,  0.0895,  0.2785,  0.0442,  0.0257, -0.0156,  0.2118,\n","          0.1516, -0.1459],\n","        [-0.0949,  0.2684, -0.0683,  0.0851, -0.1140,  0.2923, -0.1754,  0.0959,\n","          0.0936, -0.1144],\n","        [ 0.0407, -0.1330, -0.2239, -0.0449, -0.2392,  0.2000,  0.2908, -0.2912,\n","          0.3161, -0.2573]])\n","linear_hidden.0.bias tensor([-0.0790, -0.2334, -0.1308, -0.1189,  0.1426, -0.1223,  0.0680,  0.0636,\n","        -0.3001,  0.1618])\n","linear_output.weight tensor([[ 0.0436,  0.0685, -0.0741, -0.1357,  0.0835,  0.1851, -0.1558, -0.2555,\n","         -0.1756,  0.2382]])\n","linear_output.bias tensor([0.0877])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2757])) that is different to the input size (torch.Size([2757, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6953843832015991 \t Validate_Accuracy: 0.562\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6926686763763428 \t Validate_Accuracy: 0.596\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6918340921401978 \t Validate_Accuracy: 0.596\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6880099177360535 \t Validate_Accuracy: 0.582\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6829966902732849 \t Validate_Accuracy: 0.577\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6768367290496826 \t Validate_Accuracy: 0.592\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6692969799041748 \t Validate_Accuracy: 0.7075\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6598590016365051 \t Validate_Accuracy: 0.76\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6478948593139648 \t Validate_Accuracy: 0.7835\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6330809593200684 \t Validate_Accuracy: 0.7915\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6157342791557312 \t Validate_Accuracy: 0.7915\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.5968484878540039 \t Validate_Accuracy: 0.7995\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.5777756571769714 \t Validate_Accuracy: 0.809\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.5584115386009216 \t Validate_Accuracy: 0.817\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.5383228659629822 \t Validate_Accuracy: 0.824\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.5188425183296204 \t Validate_Accuracy: 0.82\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.5011114478111267 \t Validate_Accuracy: 0.821\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.4847652018070221 \t Validate_Accuracy: 0.819\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.47021594643592834 \t Validate_Accuracy: 0.8155\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.4579138457775116 \t Validate_Accuracy: 0.8115\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.44744905829429626 \t Validate_Accuracy: 0.813\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.43752381205558777 \t Validate_Accuracy: 0.816\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.42674520611763 \t Validate_Accuracy: 0.817\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.4156602621078491 \t Validate_Accuracy: 0.82\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.40563035011291504 \t Validate_Accuracy: 0.8265\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.39791420102119446 \t Validate_Accuracy: 0.833\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.39162346720695496 \t Validate_Accuracy: 0.832\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3837333619594574 \t Validate_Accuracy: 0.8325\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3739577829837799 \t Validate_Accuracy: 0.8345\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3639639914035797 \t Validate_Accuracy: 0.8345\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.35490691661834717 \t Validate_Accuracy: 0.8355\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.34604018926620483 \t Validate_Accuracy: 0.844\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.33747947216033936 \t Validate_Accuracy: 0.849\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.32868194580078125 \t Validate_Accuracy: 0.8525\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.3204372823238373 \t Validate_Accuracy: 0.853\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3136626183986664 \t Validate_Accuracy: 0.8515\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.3082233667373657 \t Validate_Accuracy: 0.8555\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3035913109779358 \t Validate_Accuracy: 0.862\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.29921531677246094 \t Validate_Accuracy: 0.86\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2945958077907562 \t Validate_Accuracy: 0.866\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2900247871875763 \t Validate_Accuracy: 0.866\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2855463922023773 \t Validate_Accuracy: 0.8645\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2813016176223755 \t Validate_Accuracy: 0.864\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.27718186378479004 \t Validate_Accuracy: 0.8675\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.27358299493789673 \t Validate_Accuracy: 0.873\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2707284986972809 \t Validate_Accuracy: 0.8745\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.26894915103912354 \t Validate_Accuracy: 0.8665\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.2693184018135071 \t Validate_Accuracy: 0.876\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.26843392848968506 \t Validate_Accuracy: 0.8705\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.26168736815452576 \t Validate_Accuracy: 0.8685\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.26154395937919617 \t Validate_Accuracy: 0.8755\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2606646418571472 \t Validate_Accuracy: 0.878\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2554611563682556 \t Validate_Accuracy: 0.8695\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2565295100212097 \t Validate_Accuracy: 0.8815\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.25249651074409485 \t Validate_Accuracy: 0.881\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.25054213404655457 \t Validate_Accuracy: 0.8735\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2499472200870514 \t Validate_Accuracy: 0.8775\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.24627260863780975 \t Validate_Accuracy: 0.881\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.24696925282478333 \t Validate_Accuracy: 0.876\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.24402542412281036 \t Validate_Accuracy: 0.8755\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.24318794906139374 \t Validate_Accuracy: 0.8815\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.2423940747976303 \t Validate_Accuracy: 0.878\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.24027776718139648 \t Validate_Accuracy: 0.8785\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.24055494368076324 \t Validate_Accuracy: 0.8795\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.23853260278701782 \t Validate_Accuracy: 0.879\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.23798590898513794 \t Validate_Accuracy: 0.88\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.23799161612987518 \t Validate_Accuracy: 0.8805\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.23638349771499634 \t Validate_Accuracy: 0.881\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2361392378807068 \t Validate_Accuracy: 0.88\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.23613020777702332 \t Validate_Accuracy: 0.877\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.23507249355316162 \t Validate_Accuracy: 0.878\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.23481276631355286 \t Validate_Accuracy: 0.878\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2351071983575821 \t Validate_Accuracy: 0.878\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.234516441822052 \t Validate_Accuracy: 0.879\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2338111400604248 \t Validate_Accuracy: 0.876\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.23397648334503174 \t Validate_Accuracy: 0.8795\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.23390890657901764 \t Validate_Accuracy: 0.878\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.23306207358837128 \t Validate_Accuracy: 0.878\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2327984869480133 \t Validate_Accuracy: 0.877\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.23294177651405334 \t Validate_Accuracy: 0.878\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.23252227902412415 \t Validate_Accuracy: 0.8745\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.23192261159420013 \t Validate_Accuracy: 0.875\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.23171716928482056 \t Validate_Accuracy: 0.877\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.23165972530841827 \t Validate_Accuracy: 0.8745\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2313319444656372 \t Validate_Accuracy: 0.8775\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.23083390295505524 \t Validate_Accuracy: 0.8775\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.23061202466487885 \t Validate_Accuracy: 0.8765\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.23050512373447418 \t Validate_Accuracy: 0.8775\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.23016203939914703 \t Validate_Accuracy: 0.878\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2297412008047104 \t Validate_Accuracy: 0.8775\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.22947987914085388 \t Validate_Accuracy: 0.8785\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.22931911051273346 \t Validate_Accuracy: 0.877\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.2290484756231308 \t Validate_Accuracy: 0.8785\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2286461740732193 \t Validate_Accuracy: 0.8785\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.228303924202919 \t Validate_Accuracy: 0.879\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.2280462086200714 \t Validate_Accuracy: 0.88\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.22777186334133148 \t Validate_Accuracy: 0.8785\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.22741562128067017 \t Validate_Accuracy: 0.8785\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.2269987165927887 \t Validate_Accuracy: 0.8775\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.22661279141902924 \t Validate_Accuracy: 0.877\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.22627155482769012 \t Validate_Accuracy: 0.878\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.22592876851558685 \t Validate_Accuracy: 0.8785\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.22555744647979736 \t Validate_Accuracy: 0.8785\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.22513549029827118 \t Validate_Accuracy: 0.879\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.22469250857830048 \t Validate_Accuracy: 0.8795\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.22424288094043732 \t Validate_Accuracy: 0.8795\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.22379988431930542 \t Validate_Accuracy: 0.88\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.22336459159851074 \t Validate_Accuracy: 0.8805\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.22293876111507416 \t Validate_Accuracy: 0.8815\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2225213646888733 \t Validate_Accuracy: 0.882\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2221219837665558 \t Validate_Accuracy: 0.8825\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.22176527976989746 \t Validate_Accuracy: 0.8835\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.22150583565235138 \t Validate_Accuracy: 0.8815\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.22156651318073273 \t Validate_Accuracy: 0.8835\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.22233057022094727 \t Validate_Accuracy: 0.878\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.22566720843315125 \t Validate_Accuracy: 0.875\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2283865362405777 \t Validate_Accuracy: 0.8775\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.22909493744373322 \t Validate_Accuracy: 0.8855\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.22012737393379211 \t Validate_Accuracy: 0.881\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.22423207759857178 \t Validate_Accuracy: 0.8775\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.22844478487968445 \t Validate_Accuracy: 0.883\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21897315979003906 \t Validate_Accuracy: 0.8805\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.22432522475719452 \t Validate_Accuracy: 0.879\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.22568270564079285 \t Validate_Accuracy: 0.883\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.21831250190734863 \t Validate_Accuracy: 0.8785\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2261277735233307 \t Validate_Accuracy: 0.88\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.22031955420970917 \t Validate_Accuracy: 0.8815\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.22082030773162842 \t Validate_Accuracy: 0.883\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.22208131849765778 \t Validate_Accuracy: 0.886\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.21700561046600342 \t Validate_Accuracy: 0.8795\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.22117212414741516 \t Validate_Accuracy: 0.8855\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.21674631536006927 \t Validate_Accuracy: 0.884\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.21923872828483582 \t Validate_Accuracy: 0.8825\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.2181292474269867 \t Validate_Accuracy: 0.884\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.21667785942554474 \t Validate_Accuracy: 0.883\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.21868856251239777 \t Validate_Accuracy: 0.887\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.2154274880886078 \t Validate_Accuracy: 0.8825\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.2173367142677307 \t Validate_Accuracy: 0.8855\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.21561484038829803 \t Validate_Accuracy: 0.8865\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.215399369597435 \t Validate_Accuracy: 0.8835\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.21625325083732605 \t Validate_Accuracy: 0.8845\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.21423566341400146 \t Validate_Accuracy: 0.8865\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.21550387144088745 \t Validate_Accuracy: 0.889\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.21474438905715942 \t Validate_Accuracy: 0.887\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2137739211320877 \t Validate_Accuracy: 0.884\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.21486234664916992 \t Validate_Accuracy: 0.8855\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.21351999044418335 \t Validate_Accuracy: 0.8865\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.21321958303451538 \t Validate_Accuracy: 0.887\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.21388620138168335 \t Validate_Accuracy: 0.887\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.2127082347869873 \t Validate_Accuracy: 0.8865\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2123836874961853 \t Validate_Accuracy: 0.887\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.21290326118469238 \t Validate_Accuracy: 0.8865\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.21207445859909058 \t Validate_Accuracy: 0.8855\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.2114778310060501 \t Validate_Accuracy: 0.886\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.21192768216133118 \t Validate_Accuracy: 0.8885\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.21161417663097382 \t Validate_Accuracy: 0.888\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.21070194244384766 \t Validate_Accuracy: 0.886\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.21074290573596954 \t Validate_Accuracy: 0.889\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.21100673079490662 \t Validate_Accuracy: 0.8855\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.21038776636123657 \t Validate_Accuracy: 0.8885\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.20971707999706268 \t Validate_Accuracy: 0.8885\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.2096252143383026 \t Validate_Accuracy: 0.887\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.20974940061569214 \t Validate_Accuracy: 0.8895\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.2095988392829895 \t Validate_Accuracy: 0.888\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.20904886722564697 \t Validate_Accuracy: 0.8885\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.20856046676635742 \t Validate_Accuracy: 0.889\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.20831546187400818 \t Validate_Accuracy: 0.888\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.2082802951335907 \t Validate_Accuracy: 0.8885\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.20836690068244934 \t Validate_Accuracy: 0.887\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.20845471322536469 \t Validate_Accuracy: 0.887\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.20874124765396118 \t Validate_Accuracy: 0.886\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.20898281037807465 \t Validate_Accuracy: 0.8875\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.2098616510629654 \t Validate_Accuracy: 0.8875\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.21034707129001617 \t Validate_Accuracy: 0.8875\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.21179193258285522 \t Validate_Accuracy: 0.886\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2108585089445114 \t Validate_Accuracy: 0.8875\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.20980322360992432 \t Validate_Accuracy: 0.886\n","epoch 178\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:50:40] ax.service.managed_loop: Running optimization trial 5...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 178 \t Train Loss: 0.2072668969631195 \t Validate_Accuracy: 0.889\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.20617122948169708 \t Validate_Accuracy: 0.888\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1106, -0.0478, -0.1339],\n","          [-0.0570,  0.0096, -0.0358],\n","          [-0.1052, -0.0399, -0.1315]]]])\n","conv1.bias tensor([0.1053])\n","first_linear.weight tensor([[ 3.0744e-01,  6.4823e-01,  1.8630e-01,  7.8571e-02,  8.6420e-02,\n","          1.6211e-01,  2.2773e-01,  3.7879e-01, -4.2863e-02],\n","        [-3.6822e-01,  3.4729e-01, -1.0035e+00,  3.7297e-01, -2.6556e-01,\n","          1.2634e+00,  5.3275e-01, -6.7363e-01, -2.4373e-01],\n","        [ 1.8423e-02, -1.0546e+00,  2.8235e-01,  1.5534e+00, -4.1025e-01,\n","          5.2295e-02, -1.0160e+00,  6.7492e-01, -4.2652e-02],\n","        [ 3.7363e-02,  3.7727e-01,  3.1959e-01,  1.4271e-01,  1.7434e-01,\n","          1.3473e-01, -2.2928e-02,  3.1171e-02,  2.7626e-01],\n","        [ 7.0736e-02,  3.9380e-01,  1.1513e-01,  4.0939e-02,  1.1602e-01,\n","          1.5907e-01,  3.0760e-02, -1.9703e-01,  5.6965e-02],\n","        [-1.0287e-01,  1.0883e+00, -3.0193e-01,  5.6775e-01, -1.4337e+00,\n","          8.2303e-01, -4.8192e-01,  1.4858e+00, -8.6620e-01],\n","        [-1.4203e-03,  4.2856e-01,  2.8771e-01, -1.0316e-01,  2.7516e-01,\n","          2.3176e-01,  1.7980e-01,  6.6351e-02,  4.2625e-01],\n","        [ 3.8811e-01, -5.0949e-01, -6.4596e-02,  2.0156e-01,  4.6748e-01,\n","          4.6325e-01,  2.1791e-01,  2.0432e-01,  2.2265e-01],\n","        [-2.6236e-01,  4.4231e-01, -1.3537e-01, -1.7348e-01, -1.4714e-01,\n","         -6.6625e-01,  7.1464e-02,  5.5817e-01,  2.2362e-01],\n","        [ 1.1720e+00, -1.0822e+00, -3.8228e-02, -1.2852e+00,  3.3667e-01,\n","          5.4214e-01,  1.6279e-01,  5.9116e-01, -6.4131e-01]])\n","first_linear.bias tensor([ 0.6843, -0.8450, -0.9468,  1.0402,  1.1845,  1.2374,  0.5369, -0.6133,\n","         0.0271, -1.1139])\n","linear_hidden.0.weight tensor([[-0.1257, -0.9285, -1.0830, -0.9784, -0.6285,  0.8714, -0.6050,  1.1534,\n","         -0.7689, -0.9081],\n","        [-0.0501, -0.8595, -0.9331, -0.4400, -0.2977,  0.6492, -0.8544,  0.9865,\n","         -0.7242, -0.8998],\n","        [ 0.6419,  0.3089,  0.8107,  0.7935,  0.6446, -1.0229,  0.3016, -0.9233,\n","          0.6824,  0.5376],\n","        [ 0.5792, -0.1511,  0.8787,  0.6673,  0.7050, -1.0034,  0.1543, -1.3458,\n","          0.7192,  0.7947],\n","        [ 0.7081,  1.4394,  1.2625,  0.0579,  0.8465, -0.3544, -0.2147, -0.2949,\n","         -0.8080,  1.1263],\n","        [-0.5849, -0.4630, -0.9436, -0.6309, -0.9989,  1.1250, -0.4482,  0.9847,\n","         -0.7939, -0.6612],\n","        [ 0.7689, -0.2186, -0.2721,  0.3962,  0.8126,  0.2037,  0.7661, -0.1427,\n","         -0.2415, -0.2690],\n","        [ 0.6606, -0.1781, -0.2213,  0.7895,  0.6071,  0.3188,  0.5522, -0.1913,\n","         -0.0495, -0.2758],\n","        [ 0.6853,  0.7518,  0.6178,  0.8164,  0.7069, -0.5151,  0.3197, -0.8003,\n","          0.7372,  0.6496],\n","        [-0.8221, -1.3998, -0.3054, -0.8439, -1.0302,  0.6881, -0.2044,  0.4772,\n","         -0.0266, -1.1051]])\n","linear_hidden.0.bias tensor([-0.4682, -0.8708, -0.2325, -0.4939,  0.3756,  0.1367,  0.5302,  0.5892,\n","        -0.6030,  0.2417])\n","linear_output.weight tensor([[ 1.5772,  1.5148, -1.2264, -1.2366, -1.7085,  1.4443, -0.8163, -0.8734,\n","         -1.2923,  1.2618]])\n","linear_output.bias tensor([0.5402])\n","Testing out: \n","batch_size:  1964\n","train_size:  3417\n","n_epochs:  189\n","lr:  0.0404474782861398\n","weight_decay:  0.00011613025830280022\n","betas0:  0.9905460759566522\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2169,  0.0085, -0.1292],\n","          [ 0.2765, -0.2541, -0.1753],\n","          [-0.2555,  0.2202,  0.0311]]]])\n","conv1.bias tensor([-0.3001])\n","first_linear.weight tensor([[ 0.2632,  0.1792,  0.3325,  0.2085, -0.3294,  0.2743, -0.0372,  0.3263,\n","         -0.3045],\n","        [-0.3008,  0.1405,  0.3136,  0.0430, -0.1826, -0.3082,  0.1949, -0.0706,\n","          0.0842],\n","        [ 0.0284, -0.0867, -0.3043,  0.2146, -0.0992,  0.2176,  0.0283, -0.2357,\n","          0.1952],\n","        [ 0.0532,  0.2425,  0.2834,  0.0964,  0.2943,  0.3227,  0.2052, -0.0377,\n","          0.0482],\n","        [-0.1756,  0.2558, -0.3024, -0.0862,  0.0088,  0.3104,  0.0449,  0.0703,\n","          0.1745],\n","        [-0.0261, -0.0899, -0.2878,  0.2849, -0.2452, -0.1935, -0.1816,  0.1253,\n","         -0.3147],\n","        [-0.0514, -0.0998, -0.0167,  0.1849,  0.0199,  0.3021, -0.1795, -0.2420,\n","          0.1151],\n","        [ 0.3034,  0.0618,  0.0082, -0.3106,  0.0364,  0.0050,  0.0789, -0.2157,\n","          0.3138],\n","        [-0.1746, -0.2335,  0.1075, -0.2055, -0.0372, -0.1717, -0.3076,  0.0028,\n","          0.3237],\n","        [-0.2499, -0.2040, -0.1992, -0.2549, -0.1745, -0.0174,  0.0155,  0.2784,\n","          0.2303]])\n","first_linear.bias tensor([-0.2599, -0.2253,  0.1992, -0.0463, -0.1543,  0.1785,  0.2047,  0.0975,\n","        -0.2095, -0.0471])\n","linear_hidden.0.weight tensor([[-0.2921,  0.0397, -0.2944,  0.0660, -0.2493, -0.0491,  0.3117, -0.2579,\n","         -0.0386,  0.2536],\n","        [ 0.2795, -0.0686,  0.0036,  0.2109, -0.3066,  0.0943, -0.1361,  0.3036,\n","          0.2013, -0.2314],\n","        [ 0.0895, -0.2957, -0.2620,  0.3062,  0.0028,  0.2101, -0.2649,  0.0892,\n","          0.0168,  0.0998],\n","        [ 0.1745, -0.0206,  0.2857,  0.2098, -0.3014,  0.0627,  0.0795, -0.2125,\n","         -0.2469, -0.0011],\n","        [-0.2396,  0.0294, -0.0691, -0.0937, -0.1686, -0.2097,  0.1379,  0.2267,\n","          0.1930, -0.1834],\n","        [ 0.2479, -0.0176,  0.1623,  0.1485, -0.2444,  0.1700,  0.0388,  0.2499,\n","         -0.1450, -0.0387],\n","        [-0.0028, -0.1550, -0.1452, -0.0309, -0.1757, -0.1626,  0.0125,  0.3079,\n","          0.0289,  0.2082],\n","        [ 0.2741, -0.0301,  0.1802, -0.1529,  0.1792,  0.1223,  0.1351,  0.0412,\n","          0.0259,  0.1325],\n","        [ 0.0662,  0.1641, -0.0619,  0.3064,  0.0543, -0.1231,  0.1741, -0.1454,\n","         -0.1858, -0.1028],\n","        [-0.2723, -0.2909,  0.2059,  0.2076, -0.1357, -0.1692, -0.0974, -0.1175,\n","         -0.1639, -0.1744]])\n","linear_hidden.0.bias tensor([ 0.1027, -0.0072,  0.1722,  0.0512, -0.1282,  0.2998,  0.0340, -0.2053,\n","        -0.3100,  0.1621])\n","linear_output.weight tensor([[ 0.2343, -0.3126, -0.2875,  0.2612, -0.0300, -0.1397,  0.2617, -0.1318,\n","         -0.1811,  0.0082]])\n","linear_output.bias tensor([0.2859])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1964])) that is different to the input size (torch.Size([1964, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1453])) that is different to the input size (torch.Size([1453, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.7079798579216003 \t Validate_Accuracy: 0.583\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6861935555934906 \t Validate_Accuracy: 0.613\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6671153604984283 \t Validate_Accuracy: 0.7455\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6477271318435669 \t Validate_Accuracy: 0.784\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6182112693786621 \t Validate_Accuracy: 0.7875\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.5837490260601044 \t Validate_Accuracy: 0.8145\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.5406558811664581 \t Validate_Accuracy: 0.8165\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.4961720257997513 \t Validate_Accuracy: 0.8125\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.4576812833547592 \t Validate_Accuracy: 0.811\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.43298742175102234 \t Validate_Accuracy: 0.8175\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.41798868775367737 \t Validate_Accuracy: 0.8165\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4014259874820709 \t Validate_Accuracy: 0.8125\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.386576846241951 \t Validate_Accuracy: 0.826\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.3714408278465271 \t Validate_Accuracy: 0.8295\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.3576226383447647 \t Validate_Accuracy: 0.832\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3505382686853409 \t Validate_Accuracy: 0.8305\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.34155622124671936 \t Validate_Accuracy: 0.83\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.339364230632782 \t Validate_Accuracy: 0.8345\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.33126746118068695 \t Validate_Accuracy: 0.8365\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.32322752475738525 \t Validate_Accuracy: 0.8435\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3132082521915436 \t Validate_Accuracy: 0.847\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3074790686368942 \t Validate_Accuracy: 0.852\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.30408066511154175 \t Validate_Accuracy: 0.855\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.29765917360782623 \t Validate_Accuracy: 0.863\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.2943665087223053 \t Validate_Accuracy: 0.8675\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.2902251332998276 \t Validate_Accuracy: 0.8685\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.2887532711029053 \t Validate_Accuracy: 0.867\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.28983306884765625 \t Validate_Accuracy: 0.868\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.2872292250394821 \t Validate_Accuracy: 0.8695\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2877032607793808 \t Validate_Accuracy: 0.87\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.28600700199604034 \t Validate_Accuracy: 0.869\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.2758302390575409 \t Validate_Accuracy: 0.8585\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.2766859829425812 \t Validate_Accuracy: 0.869\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.2756945937871933 \t Validate_Accuracy: 0.8765\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.2726012319326401 \t Validate_Accuracy: 0.8745\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.270173579454422 \t Validate_Accuracy: 0.8775\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.27063943445682526 \t Validate_Accuracy: 0.8715\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.2657945156097412 \t Validate_Accuracy: 0.877\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.2642703801393509 \t Validate_Accuracy: 0.8785\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2626403570175171 \t Validate_Accuracy: 0.877\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2614586353302002 \t Validate_Accuracy: 0.8795\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.25968362390995026 \t Validate_Accuracy: 0.8775\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2581290453672409 \t Validate_Accuracy: 0.873\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2571519613265991 \t Validate_Accuracy: 0.8815\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.25503093749284744 \t Validate_Accuracy: 0.88\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2557405084371567 \t Validate_Accuracy: 0.877\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2511991783976555 \t Validate_Accuracy: 0.873\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.25090133398771286 \t Validate_Accuracy: 0.881\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.24705808609724045 \t Validate_Accuracy: 0.8855\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.24729473888874054 \t Validate_Accuracy: 0.8825\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.245351180434227 \t Validate_Accuracy: 0.874\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.24685189127922058 \t Validate_Accuracy: 0.8775\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.24822434782981873 \t Validate_Accuracy: 0.8815\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.25038833171129227 \t Validate_Accuracy: 0.88\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2454361841082573 \t Validate_Accuracy: 0.8765\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.24284500628709793 \t Validate_Accuracy: 0.8745\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.24795690923929214 \t Validate_Accuracy: 0.879\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.241993710398674 \t Validate_Accuracy: 0.878\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.23702372610569 \t Validate_Accuracy: 0.8765\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.23547367006540298 \t Validate_Accuracy: 0.882\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.23601968586444855 \t Validate_Accuracy: 0.8845\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.23515913635492325 \t Validate_Accuracy: 0.878\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.23408231884241104 \t Validate_Accuracy: 0.879\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2327541559934616 \t Validate_Accuracy: 0.882\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.22983479499816895 \t Validate_Accuracy: 0.8835\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2285461276769638 \t Validate_Accuracy: 0.8775\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.23209425061941147 \t Validate_Accuracy: 0.8775\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.22525755316019058 \t Validate_Accuracy: 0.8815\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.22687704861164093 \t Validate_Accuracy: 0.881\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2283758446574211 \t Validate_Accuracy: 0.8815\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.22825857251882553 \t Validate_Accuracy: 0.884\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.2266174554824829 \t Validate_Accuracy: 0.884\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.22748851031064987 \t Validate_Accuracy: 0.882\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.22768666595220566 \t Validate_Accuracy: 0.8855\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2258031815290451 \t Validate_Accuracy: 0.882\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.22599835693836212 \t Validate_Accuracy: 0.879\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.22330372035503387 \t Validate_Accuracy: 0.8835\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.22408730536699295 \t Validate_Accuracy: 0.886\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.22492489218711853 \t Validate_Accuracy: 0.8845\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2222345545887947 \t Validate_Accuracy: 0.8805\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.22659064084291458 \t Validate_Accuracy: 0.881\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.2224443256855011 \t Validate_Accuracy: 0.881\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.22571919858455658 \t Validate_Accuracy: 0.877\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.22170818597078323 \t Validate_Accuracy: 0.879\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2258380949497223 \t Validate_Accuracy: 0.878\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.2322351410984993 \t Validate_Accuracy: 0.8835\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2203291952610016 \t Validate_Accuracy: 0.884\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.22025958448648453 \t Validate_Accuracy: 0.8825\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.21587510406970978 \t Validate_Accuracy: 0.8805\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.22178788483142853 \t Validate_Accuracy: 0.8855\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.22173485159873962 \t Validate_Accuracy: 0.8835\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.21839796006679535 \t Validate_Accuracy: 0.884\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.220563106238842 \t Validate_Accuracy: 0.885\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.21839367598295212 \t Validate_Accuracy: 0.8845\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.22359921038150787 \t Validate_Accuracy: 0.883\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.22116274386644363 \t Validate_Accuracy: 0.879\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.22087524831295013 \t Validate_Accuracy: 0.884\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.21788117289543152 \t Validate_Accuracy: 0.881\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.21910205483436584 \t Validate_Accuracy: 0.885\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2173757329583168 \t Validate_Accuracy: 0.886\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2157081961631775 \t Validate_Accuracy: 0.884\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.21682746708393097 \t Validate_Accuracy: 0.883\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.21835550665855408 \t Validate_Accuracy: 0.8815\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2195075899362564 \t Validate_Accuracy: 0.8795\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2194100245833397 \t Validate_Accuracy: 0.8845\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.21618885546922684 \t Validate_Accuracy: 0.887\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.21814223378896713 \t Validate_Accuracy: 0.8825\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.21650882065296173 \t Validate_Accuracy: 0.881\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2161869928240776 \t Validate_Accuracy: 0.879\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.21521779894828796 \t Validate_Accuracy: 0.882\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.21666861325502396 \t Validate_Accuracy: 0.8835\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.21430791914463043 \t Validate_Accuracy: 0.882\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.21671227365732193 \t Validate_Accuracy: 0.884\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2142777070403099 \t Validate_Accuracy: 0.8865\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2134869024157524 \t Validate_Accuracy: 0.888\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.21166695654392242 \t Validate_Accuracy: 0.8815\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.21547435224056244 \t Validate_Accuracy: 0.885\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.21274779736995697 \t Validate_Accuracy: 0.882\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.2127871736884117 \t Validate_Accuracy: 0.8815\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.21505723893642426 \t Validate_Accuracy: 0.884\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.21339716017246246 \t Validate_Accuracy: 0.8865\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.20948530733585358 \t Validate_Accuracy: 0.884\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.21175061911344528 \t Validate_Accuracy: 0.886\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.21381491422653198 \t Validate_Accuracy: 0.884\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.2161441221833229 \t Validate_Accuracy: 0.8855\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.21324872970581055 \t Validate_Accuracy: 0.884\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.21520861983299255 \t Validate_Accuracy: 0.8815\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.21017524600028992 \t Validate_Accuracy: 0.8855\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.21124684810638428 \t Validate_Accuracy: 0.883\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.20826493203639984 \t Validate_Accuracy: 0.888\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.20805233716964722 \t Validate_Accuracy: 0.8885\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.2095515877008438 \t Validate_Accuracy: 0.891\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.20689348876476288 \t Validate_Accuracy: 0.887\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.2070889100432396 \t Validate_Accuracy: 0.886\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.20774933695793152 \t Validate_Accuracy: 0.8875\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2083473950624466 \t Validate_Accuracy: 0.885\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.20998245477676392 \t Validate_Accuracy: 0.8815\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.21171405911445618 \t Validate_Accuracy: 0.881\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.21036018431186676 \t Validate_Accuracy: 0.8835\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.2141524776816368 \t Validate_Accuracy: 0.8815\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.21027053892612457 \t Validate_Accuracy: 0.8865\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.21171336621046066 \t Validate_Accuracy: 0.887\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.21069543808698654 \t Validate_Accuracy: 0.8795\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.2102108672261238 \t Validate_Accuracy: 0.885\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.20964516699314117 \t Validate_Accuracy: 0.884\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.21526985615491867 \t Validate_Accuracy: 0.8885\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.20600606501102448 \t Validate_Accuracy: 0.8875\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.20274369418621063 \t Validate_Accuracy: 0.884\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.20000742375850677 \t Validate_Accuracy: 0.887\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.2008294016122818 \t Validate_Accuracy: 0.8905\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2015039473772049 \t Validate_Accuracy: 0.89\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.20053429156541824 \t Validate_Accuracy: 0.8795\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.20729978382587433 \t Validate_Accuracy: 0.886\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.197091244161129 \t Validate_Accuracy: 0.888\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.19666269421577454 \t Validate_Accuracy: 0.888\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.19606773555278778 \t Validate_Accuracy: 0.8885\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.19677797704935074 \t Validate_Accuracy: 0.89\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.1951957419514656 \t Validate_Accuracy: 0.885\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.19485526531934738 \t Validate_Accuracy: 0.891\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.19533289968967438 \t Validate_Accuracy: 0.8895\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.19296225160360336 \t Validate_Accuracy: 0.887\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.191269189119339 \t Validate_Accuracy: 0.889\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.19736698269844055 \t Validate_Accuracy: 0.8925\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.19477242976427078 \t Validate_Accuracy: 0.885\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.19281329959630966 \t Validate_Accuracy: 0.8905\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.19867635518312454 \t Validate_Accuracy: 0.8905\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.2013213112950325 \t Validate_Accuracy: 0.8815\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.21316086500883102 \t Validate_Accuracy: 0.885\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.20279547572135925 \t Validate_Accuracy: 0.8825\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.2050241455435753 \t Validate_Accuracy: 0.8865\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.20313884317874908 \t Validate_Accuracy: 0.8825\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.20384617149829865 \t Validate_Accuracy: 0.888\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.20979321748018265 \t Validate_Accuracy: 0.884\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.21570586413145065 \t Validate_Accuracy: 0.8855\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.20641908794641495 \t Validate_Accuracy: 0.886\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.19342992454767227 \t Validate_Accuracy: 0.882\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.2058989182114601 \t Validate_Accuracy: 0.8785\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.20394887030124664 \t Validate_Accuracy: 0.8875\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.19789302349090576 \t Validate_Accuracy: 0.8825\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.19431515783071518 \t Validate_Accuracy: 0.8805\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.19779189676046371 \t Validate_Accuracy: 0.883\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.1950676068663597 \t Validate_Accuracy: 0.892\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.1946031078696251 \t Validate_Accuracy: 0.888\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.19244498759508133 \t Validate_Accuracy: 0.8845\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.1941026672720909 \t Validate_Accuracy: 0.8895\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.18882962316274643 \t Validate_Accuracy: 0.8865\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.19336751103401184 \t Validate_Accuracy: 0.888\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.188982792198658 \t Validate_Accuracy: 0.892\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.19226033985614777 \t Validate_Accuracy: 0.889\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1372, -0.0378, -0.1649],\n","          [-0.0632,  0.0063, -0.0363],\n","          [-0.1251, -0.0300, -0.1563]]]])\n","conv1.bias tensor([0.0721])\n","first_linear.weight tensor([[-0.1630, -0.0386,  0.1077,  1.0718, -0.9226, -0.0134, -1.1712,  1.2429,\n","         -0.2402],\n","        [ 0.4043, -0.2600,  0.3821,  0.1241,  0.5998,  0.2126,  0.4982,  0.5882,\n","          0.6895],\n","        [-0.1558,  0.6498, -0.0322, -0.7250,  0.5838, -0.6172,  0.2545, -0.0666,\n","          0.1195],\n","        [-1.3636,  1.8623, -0.8915,  0.8830, -0.7731,  0.3074, -0.1343, -0.2036,\n","         -0.0153],\n","        [-0.1218,  0.0905, -0.9743, -1.2601,  0.1630,  0.9750, -0.2318,  0.4509,\n","         -0.4354],\n","        [-0.1635, -0.0566, -0.2753, -0.0064, -0.2290, -0.3772,  0.2643, -0.0815,\n","          0.1412],\n","        [ 0.1612,  0.2816,  0.1971, -0.2176,  0.5787,  0.5387,  0.1034,  0.0175,\n","          0.1305],\n","        [-1.1364, -0.8131, -0.2270,  0.0434,  0.9369, -0.4292, -0.3252, -0.6412,\n","          1.1047],\n","        [ 0.0727, -0.1056,  0.9365, -0.4118, -0.7462, -1.6607, -0.3158,  0.5103,\n","          0.8311],\n","        [ 0.0631,  0.5325, -0.5564, -0.2943, -1.4504,  0.6045,  1.0880, -0.7083,\n","          0.5086]])\n","first_linear.bias tensor([-0.8659, -1.1164, -0.4790,  1.2777,  0.5895,  1.6976,  0.4816, -0.4951,\n","         0.8968, -1.0205])\n","linear_hidden.0.weight tensor([[-0.6193,  0.4309, -1.5332,  1.0834,  0.0557, -1.2909, -0.6706, -0.3650,\n","          0.8115, -0.6210],\n","        [ 1.1638, -0.8021,  0.3778, -0.3599, -1.6530,  1.1384,  0.3310,  1.5420,\n","         -0.7551,  0.5975],\n","        [-0.1166, -0.5064, -0.4013, -0.6037,  0.1673,  1.0884, -0.2607,  0.4593,\n","          0.6176, -0.0680],\n","        [-0.4877,  0.6353,  0.2553,  0.2888, -1.0807, -0.9140,  0.2016, -1.4855,\n","         -0.2941, -0.5892],\n","        [-1.8146,  0.3704,  0.3123,  0.7746, -1.6106, -0.4439, -0.2361,  1.9094,\n","          1.4679, -1.0471],\n","        [-0.2301, -0.3899, -0.0234, -0.2771, -0.7155,  0.7786,  0.9824, -0.0900,\n","         -0.0418, -0.1782],\n","        [-0.9515,  0.0506, -1.3564,  0.7860,  0.5169, -1.3367, -0.6039,  0.0511,\n","          0.1984, -0.7125],\n","        [ 0.9745, -0.8177,  0.4079, -0.8173, -0.4897,  1.4560,  0.8310,  0.4698,\n","          0.2467,  0.3548],\n","        [ 0.4046, -0.1513,  0.9881, -0.9599,  0.4636,  0.9473,  0.9754,  0.4319,\n","         -0.7888,  0.6229],\n","        [ 0.4634,  0.2464,  0.3380, -0.4214, -1.0900,  0.4650,  0.4016, -1.6643,\n","         -1.0627, -0.1371]])\n","linear_hidden.0.bias tensor([-0.2001,  0.1068,  1.1078, -0.7965,  0.6128,  1.2029, -0.2638,  0.1770,\n","         0.0745,  0.7489])\n","linear_output.weight tensor([[ 1.9861, -2.4742, -1.5642,  1.2104,  1.9354, -1.7526,  2.1582, -1.8685,\n","         -1.0704, -1.2256]])\n","linear_output.bias tensor([0.1135])\n","Testing out: \n","batch_size:  2107\n","train_size:  3330\n","n_epochs:  155\n","lr:  0.05834667943159567\n","weight_decay:  0.0008578767358241399\n","betas0:  0.9997294495966311\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1775, -0.2387, -0.3056],\n","          [ 0.0960,  0.2684,  0.1301],\n","          [-0.2936, -0.0021,  0.2558]]]])\n","conv1.bias tensor([-0.0238])\n","first_linear.weight tensor([[-0.0948,  0.0260, -0.0933, -0.1710,  0.2610, -0.1525,  0.0939, -0.1655,\n","         -0.2797],\n","        [-0.0426, -0.0115, -0.0467,  0.2616, -0.3172,  0.2403, -0.3219, -0.2514,\n","          0.0386],\n","        [ 0.0520, -0.0822, -0.2503, -0.1980,  0.0865,  0.1972, -0.2884, -0.0397,\n","          0.0444],\n","        [ 0.0147, -0.1138, -0.0575,  0.0281, -0.0973,  0.1220, -0.0867,  0.2556,\n","          0.1793],\n","        [-0.0247, -0.2399, -0.2253, -0.2674, -0.1721, -0.1985, -0.2767,  0.2206,\n","         -0.1057],\n","        [ 0.1808,  0.1515, -0.0143, -0.3066,  0.3118,  0.0191, -0.1871, -0.2291,\n","          0.2439],\n","        [ 0.3095, -0.1732, -0.1652,  0.3058, -0.1258, -0.0152, -0.0232,  0.0574,\n","          0.0463],\n","        [ 0.0083, -0.1145, -0.1169, -0.0353, -0.0973, -0.2880,  0.3153,  0.0686,\n","         -0.1449],\n","        [-0.0167,  0.1309,  0.0661,  0.3040, -0.0550, -0.1929, -0.1285,  0.1099,\n","          0.2547],\n","        [ 0.1998,  0.1247,  0.1845, -0.0088, -0.2558,  0.1874, -0.0475, -0.3307,\n","          0.1817]])\n","first_linear.bias tensor([-0.1063,  0.1185,  0.1687,  0.1420, -0.2050, -0.0808, -0.1167,  0.2861,\n","        -0.2276,  0.2232])\n","linear_hidden.0.weight tensor([[-0.0587,  0.1277, -0.2584,  0.1459,  0.1168, -0.2807,  0.1230,  0.1104,\n","          0.0761, -0.2656],\n","        [ 0.2958,  0.2504,  0.1538, -0.1494, -0.1963, -0.2993, -0.1789,  0.2097,\n","         -0.2299,  0.2961],\n","        [-0.3106, -0.1300, -0.1908,  0.0661,  0.1473,  0.2340, -0.1866,  0.2954,\n","          0.1029,  0.0110],\n","        [ 0.1217, -0.2203, -0.2983, -0.2102, -0.2228, -0.2839,  0.2888, -0.2032,\n","          0.2070,  0.1012],\n","        [-0.1970, -0.3145,  0.2360,  0.0430,  0.1529,  0.2710,  0.0793,  0.0857,\n","         -0.0907,  0.2917],\n","        [-0.2149, -0.3068, -0.0741, -0.0083,  0.1697,  0.1036,  0.2316, -0.2993,\n","         -0.0228,  0.1892],\n","        [-0.2325,  0.0609,  0.0240,  0.2019, -0.1457, -0.0806,  0.2438,  0.2202,\n","          0.1097, -0.1354],\n","        [-0.0751,  0.0708, -0.1620,  0.3151,  0.0468, -0.1639,  0.1676, -0.2215,\n","         -0.1097, -0.1641],\n","        [ 0.2986, -0.2479,  0.2305,  0.1039, -0.1179, -0.0546,  0.0327, -0.0866,\n","         -0.2777,  0.1345],\n","        [ 0.0048, -0.2448,  0.2860, -0.0655,  0.0629,  0.0576,  0.1629,  0.3029,\n","         -0.1948,  0.1089]])\n","linear_hidden.0.bias tensor([ 0.1747,  0.2489,  0.0145,  0.0199, -0.0091,  0.2726,  0.1777,  0.2784,\n","        -0.2000, -0.1530])\n","linear_output.weight tensor([[ 0.3040,  0.1254,  0.0325, -0.2228,  0.0572, -0.1797, -0.2243, -0.1021,\n","         -0.2350, -0.1939]])\n","linear_output.bias tensor([0.0844])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2107])) that is different to the input size (torch.Size([2107, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1223])) that is different to the input size (torch.Size([1223, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6970566213130951 \t Validate_Accuracy: 0.5015\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6934252679347992 \t Validate_Accuracy: 0.497\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6913086473941803 \t Validate_Accuracy: 0.626\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6797288060188293 \t Validate_Accuracy: 0.79\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6543748080730438 \t Validate_Accuracy: 0.6655\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6122320294380188 \t Validate_Accuracy: 0.8145\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.5606295466423035 \t Validate_Accuracy: 0.814\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5149022042751312 \t Validate_Accuracy: 0.803\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.4922853410243988 \t Validate_Accuracy: 0.792\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.4778033345937729 \t Validate_Accuracy: 0.81\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.4554288238286972 \t Validate_Accuracy: 0.819\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4366394132375717 \t Validate_Accuracy: 0.809\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.4282867759466171 \t Validate_Accuracy: 0.815\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.4229786992073059 \t Validate_Accuracy: 0.818\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.408235639333725 \t Validate_Accuracy: 0.824\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3996073454618454 \t Validate_Accuracy: 0.8235\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.38718412816524506 \t Validate_Accuracy: 0.8375\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3706262409687042 \t Validate_Accuracy: 0.8385\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.36228492856025696 \t Validate_Accuracy: 0.8455\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.35871049761772156 \t Validate_Accuracy: 0.8535\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3467879295349121 \t Validate_Accuracy: 0.8565\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.34510524570941925 \t Validate_Accuracy: 0.8545\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3409576863050461 \t Validate_Accuracy: 0.855\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3383251130580902 \t Validate_Accuracy: 0.853\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.332689493894577 \t Validate_Accuracy: 0.844\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.33545994758605957 \t Validate_Accuracy: 0.848\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.33538246154785156 \t Validate_Accuracy: 0.8525\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3295729160308838 \t Validate_Accuracy: 0.854\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3289239704608917 \t Validate_Accuracy: 0.849\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.329363688826561 \t Validate_Accuracy: 0.8545\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.32085810601711273 \t Validate_Accuracy: 0.8515\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.32460957765579224 \t Validate_Accuracy: 0.8535\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.32127368450164795 \t Validate_Accuracy: 0.8575\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3222937136888504 \t Validate_Accuracy: 0.8525\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.32913294434547424 \t Validate_Accuracy: 0.854\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.32188330590724945 \t Validate_Accuracy: 0.854\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.32021765410900116 \t Validate_Accuracy: 0.8585\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3280716985464096 \t Validate_Accuracy: 0.855\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.32134024798870087 \t Validate_Accuracy: 0.8525\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.324800044298172 \t Validate_Accuracy: 0.8515\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.3268883228302002 \t Validate_Accuracy: 0.854\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.319172739982605 \t Validate_Accuracy: 0.8555\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.32441096007823944 \t Validate_Accuracy: 0.8515\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.322926864027977 \t Validate_Accuracy: 0.8515\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.31933967769145966 \t Validate_Accuracy: 0.864\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.3152675926685333 \t Validate_Accuracy: 0.86\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.3165517896413803 \t Validate_Accuracy: 0.8575\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.31863610446453094 \t Validate_Accuracy: 0.857\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.31337837874889374 \t Validate_Accuracy: 0.8605\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.3137471675872803 \t Validate_Accuracy: 0.8635\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.3186280131340027 \t Validate_Accuracy: 0.8625\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3124532997608185 \t Validate_Accuracy: 0.862\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.31339314579963684 \t Validate_Accuracy: 0.863\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.3142741918563843 \t Validate_Accuracy: 0.8625\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.3141081780195236 \t Validate_Accuracy: 0.8635\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.31477008759975433 \t Validate_Accuracy: 0.8655\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.3032308369874954 \t Validate_Accuracy: 0.8665\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.30491532385349274 \t Validate_Accuracy: 0.859\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.30624742805957794 \t Validate_Accuracy: 0.858\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.30500195920467377 \t Validate_Accuracy: 0.8675\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.29909417033195496 \t Validate_Accuracy: 0.8565\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.30388933420181274 \t Validate_Accuracy: 0.8545\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.3061937093734741 \t Validate_Accuracy: 0.861\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2981306165456772 \t Validate_Accuracy: 0.8715\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.29394620656967163 \t Validate_Accuracy: 0.864\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.29734790325164795 \t Validate_Accuracy: 0.867\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.2886088788509369 \t Validate_Accuracy: 0.864\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.28991852700710297 \t Validate_Accuracy: 0.8685\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2829854190349579 \t Validate_Accuracy: 0.87\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2860049456357956 \t Validate_Accuracy: 0.869\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.27762264013290405 \t Validate_Accuracy: 0.865\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.27562515437602997 \t Validate_Accuracy: 0.861\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2818068414926529 \t Validate_Accuracy: 0.8615\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.27344846725463867 \t Validate_Accuracy: 0.858\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2727462351322174 \t Validate_Accuracy: 0.8665\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.26681387424468994 \t Validate_Accuracy: 0.867\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.2737639993429184 \t Validate_Accuracy: 0.862\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.27275750786066055 \t Validate_Accuracy: 0.8655\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.26863184571266174 \t Validate_Accuracy: 0.8775\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2643207907676697 \t Validate_Accuracy: 0.876\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2641315460205078 \t Validate_Accuracy: 0.8595\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.26405470073223114 \t Validate_Accuracy: 0.8615\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.27762526273727417 \t Validate_Accuracy: 0.8615\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.27432340383529663 \t Validate_Accuracy: 0.871\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.26563283801078796 \t Validate_Accuracy: 0.8525\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.2722679525613785 \t Validate_Accuracy: 0.8705\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.26327256858348846 \t Validate_Accuracy: 0.869\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2616364359855652 \t Validate_Accuracy: 0.879\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.25500813126564026 \t Validate_Accuracy: 0.88\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2519361153244972 \t Validate_Accuracy: 0.8805\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2511853575706482 \t Validate_Accuracy: 0.8785\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.25338196009397507 \t Validate_Accuracy: 0.875\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.25228581577539444 \t Validate_Accuracy: 0.8795\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.24796362221240997 \t Validate_Accuracy: 0.8805\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.24671590328216553 \t Validate_Accuracy: 0.878\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.24858271330595016 \t Validate_Accuracy: 0.875\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.24820782244205475 \t Validate_Accuracy: 0.874\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.24731846898794174 \t Validate_Accuracy: 0.8795\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.24535802006721497 \t Validate_Accuracy: 0.876\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2443843111395836 \t Validate_Accuracy: 0.8745\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.24732089787721634 \t Validate_Accuracy: 0.871\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2482861801981926 \t Validate_Accuracy: 0.8745\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.24603339284658432 \t Validate_Accuracy: 0.874\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2436118721961975 \t Validate_Accuracy: 0.871\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.24671325087547302 \t Validate_Accuracy: 0.8735\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.24335971474647522 \t Validate_Accuracy: 0.876\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.24294831603765488 \t Validate_Accuracy: 0.8775\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2410755380988121 \t Validate_Accuracy: 0.8735\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.23481033742427826 \t Validate_Accuracy: 0.877\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.24295956641435623 \t Validate_Accuracy: 0.8705\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.24114999920129776 \t Validate_Accuracy: 0.8725\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.23845792561769485 \t Validate_Accuracy: 0.876\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2396032139658928 \t Validate_Accuracy: 0.875\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.23653356730937958 \t Validate_Accuracy: 0.8745\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.23693294823169708 \t Validate_Accuracy: 0.872\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.23231347650289536 \t Validate_Accuracy: 0.877\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2331884279847145 \t Validate_Accuracy: 0.879\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.23070762306451797 \t Validate_Accuracy: 0.8695\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.23681746423244476 \t Validate_Accuracy: 0.875\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.23488815873861313 \t Validate_Accuracy: 0.875\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.24068551510572433 \t Validate_Accuracy: 0.8815\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.23494943231344223 \t Validate_Accuracy: 0.8725\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.23940297961235046 \t Validate_Accuracy: 0.871\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.23737525939941406 \t Validate_Accuracy: 0.8815\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.23654283583164215 \t Validate_Accuracy: 0.8735\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2369358241558075 \t Validate_Accuracy: 0.877\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2365652620792389 \t Validate_Accuracy: 0.8725\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.23388969898223877 \t Validate_Accuracy: 0.874\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23888979852199554 \t Validate_Accuracy: 0.873\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.23361807316541672 \t Validate_Accuracy: 0.873\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.23468191176652908 \t Validate_Accuracy: 0.8765\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.23567018657922745 \t Validate_Accuracy: 0.8775\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.23387642949819565 \t Validate_Accuracy: 0.8735\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.23746998608112335 \t Validate_Accuracy: 0.868\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.23806188255548477 \t Validate_Accuracy: 0.8745\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.24076160788536072 \t Validate_Accuracy: 0.876\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.23717763274908066 \t Validate_Accuracy: 0.8725\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.24813009798526764 \t Validate_Accuracy: 0.867\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.236730694770813 \t Validate_Accuracy: 0.879\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.24308818578720093 \t Validate_Accuracy: 0.881\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.23553045094013214 \t Validate_Accuracy: 0.8695\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.229172982275486 \t Validate_Accuracy: 0.879\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.23260944336652756 \t Validate_Accuracy: 0.87\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.23531340807676315 \t Validate_Accuracy: 0.87\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2315290942788124 \t Validate_Accuracy: 0.8745\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.23582565784454346 \t Validate_Accuracy: 0.8775\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.235362246632576 \t Validate_Accuracy: 0.8815\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.22963011264801025 \t Validate_Accuracy: 0.8755\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.2332945466041565 \t Validate_Accuracy: 0.878\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.23522789776325226 \t Validate_Accuracy: 0.8675\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.23120760172605515 \t Validate_Accuracy: 0.88\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.2347181737422943 \t Validate_Accuracy: 0.8725\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.23472485691308975 \t Validate_Accuracy: 0.8815\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.22954951971769333 \t Validate_Accuracy: 0.8725\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.23382117599248886 \t Validate_Accuracy: 0.88\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1685, -0.0657, -0.1989],\n","          [-0.0796,  0.0087, -0.0618],\n","          [-0.1645, -0.0620, -0.1662]]]])\n","conv1.bias tensor([0.0110])\n","first_linear.weight tensor([[-1.0668,  1.2278, -0.6096,  0.6924, -0.6763,  0.3806,  0.1129, -0.0909,\n","         -0.0501],\n","        [-0.1007,  0.2964,  0.3218,  0.0535,  0.0635,  0.3094, -0.1902,  0.1311,\n","          0.2003],\n","        [-0.2977, -0.1119, -0.2657, -0.1644, -0.4591, -0.4449, -0.2715, -0.2694,\n","         -0.3400],\n","        [ 0.3413,  0.3554, -0.6479,  0.2284, -0.9852,  1.2987, -0.1618,  0.4328,\n","         -0.6351],\n","        [ 0.0173, -0.0779,  0.2770, -0.5724, -0.0844,  0.1612, -0.2724, -1.0936,\n","         -0.0704],\n","        [-0.2587, -0.1738, -0.0802, -0.0985, -0.0071, -0.0069, -0.0058,  0.2999,\n","          0.1410],\n","        [-0.0096, -0.1611,  0.3551,  0.9946, -0.6663, -0.0621, -1.0959,  1.4215,\n","         -0.3023],\n","        [ 0.2784,  0.7063,  0.4430,  0.4568,  0.2774, -0.0842,  0.0733,  0.2474,\n","         -0.3567],\n","        [ 0.3743,  0.0049,  0.0807,  0.2148,  0.1930,  0.1905,  0.0457, -0.1963,\n","          0.1184],\n","        [-0.6543, -0.0132,  0.2838,  1.2494, -0.1325, -0.5189, -0.5521, -0.4784,\n","          0.6891]])\n","first_linear.bias tensor([ 1.0521,  0.9864,  1.0551, -1.0149, -0.4016,  1.0719,  1.2248,  0.4221,\n","        -0.3203, -0.9478])\n","linear_hidden.0.weight tensor([[ 7.2035e-01, -7.6346e-01, -8.3122e-01, -5.5079e-01,  3.5394e-01,\n","         -7.6939e-01,  7.8849e-01, -3.2930e-01,  2.8449e-01, -5.8569e-01],\n","        [ 5.2718e-01, -5.4198e-01, -5.8893e-01, -4.4906e-01,  2.7358e-01,\n","         -4.8498e-01,  5.8527e-01, -2.2261e-01,  2.2263e-01, -4.6386e-01],\n","        [-1.4553e-01,  1.3971e-01,  1.6053e-01,  8.1532e-02, -1.4681e-01,\n","          1.2791e-01, -1.6917e-01,  8.3124e-02, -8.9105e-02,  9.3511e-02],\n","        [ 4.7506e-01, -1.5215e-01, -1.4788e-01, -5.4244e-01,  2.4931e-01,\n","         -2.3695e-01, -3.4837e-01, -9.9449e-02,  1.1193e-01, -1.1968e-01],\n","        [-2.8141e-07,  5.4215e-07,  9.9662e-07, -3.7367e-07, -3.0828e-08,\n","          6.3768e-07, -1.5208e-07,  6.5022e-07, -1.6159e-07,  4.1251e-07],\n","        [-8.8144e-02,  2.8513e-01,  2.9255e-01,  2.7677e-02, -1.5013e-01,\n","          2.9813e-01, -5.4409e-02,  5.9434e-02, -4.3155e-02,  4.3310e-02],\n","        [-6.3768e-01,  3.4704e-01,  3.7975e-01,  8.1789e-01, -1.0346e-01,\n","          3.4175e-01, -9.3678e-01,  3.2221e-01, -5.5400e-02,  4.6263e-01],\n","        [-8.8661e-06,  1.2000e-04,  1.1258e-04,  3.3282e-05, -8.0112e-05,\n","          8.1287e-05, -5.2460e-05,  3.5760e-05, -3.9558e-05, -1.6564e-05],\n","        [-6.2210e-01,  6.9608e-01,  7.5848e-01,  4.6714e-01, -3.5542e-01,\n","          6.0907e-01, -6.8189e-01,  2.5521e-01, -2.7696e-01,  5.0787e-01],\n","        [-6.1936e-01,  6.7487e-01,  7.3752e-01,  4.7172e-01, -2.8031e-01,\n","          6.0517e-01, -6.6150e-01,  3.0507e-01, -2.6036e-01,  5.0921e-01]])\n","linear_hidden.0.bias tensor([ 1.5828e-01, -7.5939e-02,  1.7234e-01,  2.6959e-01,  2.9471e-07,\n","         4.9072e-01,  6.7832e-01,  3.1178e-05, -1.3769e-01, -9.7950e-02])\n","linear_output.weight tensor([[ 1.8769e+00,  1.3496e+00, -2.9022e-01,  6.0109e-01,  1.8523e-07,\n","         -9.4797e-01, -1.4827e+00, -3.9511e-05, -1.6343e+00, -1.6094e+00]])\n","linear_output.bias tensor([0.1808])\n","Testing out: \n","batch_size:  917\n","train_size:  3660\n","n_epochs:  157\n","lr:  0.02558003010705444\n","weight_decay:  0.00028655327554673023\n","betas0:  0.9720093639064897\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0964, -0.0432, -0.0268],\n","          [ 0.0402, -0.2160, -0.0915],\n","          [ 0.0821, -0.0275, -0.2924]]]])\n","conv1.bias tensor([0.2346])\n","first_linear.weight tensor([[ 2.7994e-01, -2.0802e-02, -2.9964e-01, -1.1825e-01, -1.5057e-01,\n","         -2.9891e-01,  1.7796e-01, -2.8607e-01, -1.4313e-01],\n","        [-1.5342e-01,  1.3063e-01,  1.5459e-01, -2.0789e-01, -1.7516e-01,\n","         -1.8332e-01, -2.8488e-01,  9.3904e-02, -3.6256e-02],\n","        [-2.0913e-01, -2.3913e-01, -3.2790e-01, -1.9506e-01, -2.4710e-02,\n","          3.1520e-01, -1.9297e-01, -2.6327e-01,  2.8873e-01],\n","        [-2.8038e-01,  2.4032e-01, -1.4575e-01,  7.5547e-02, -2.3129e-01,\n","          1.9857e-04,  2.4961e-01, -2.2621e-02,  1.7089e-01],\n","        [-2.1574e-01, -1.6071e-01, -3.0347e-01,  2.0951e-01, -9.6382e-02,\n","          1.4191e-01,  3.1621e-01, -2.5796e-02, -1.0062e-01],\n","        [ 1.9083e-01,  1.0731e-01,  8.9784e-02, -2.2173e-01, -2.8408e-01,\n","          9.7157e-02,  2.7235e-01, -3.2204e-01, -1.0808e-01],\n","        [ 1.1055e-01,  1.2965e-01,  2.1839e-01, -2.1517e-01, -2.0539e-01,\n","          1.1610e-01, -8.7247e-02, -1.9070e-01, -2.0591e-02],\n","        [ 1.4237e-01, -1.2564e-01,  1.8387e-01,  1.7379e-01,  2.0437e-01,\n","          3.0143e-01, -1.2007e-01,  1.2135e-01,  3.3002e-01],\n","        [-3.4923e-02, -5.7012e-02,  7.6893e-02, -3.6641e-02,  2.0559e-01,\n","          1.9326e-01,  2.3530e-01,  2.8740e-01, -4.1603e-03],\n","        [-1.2849e-01,  1.4302e-01, -2.5204e-01, -2.2738e-01, -2.2697e-01,\n","          9.5442e-02, -5.0434e-02, -1.8085e-01, -2.9704e-02]])\n","first_linear.bias tensor([-0.0686, -0.2702, -0.2524,  0.1887,  0.0325, -0.0214, -0.2391,  0.1638,\n","        -0.2724,  0.1764])\n","linear_hidden.0.weight tensor([[-0.1036, -0.1165, -0.2522, -0.1970,  0.0391,  0.0563,  0.2660, -0.1368,\n","          0.1818, -0.2352],\n","        [-0.0729, -0.0521, -0.2894,  0.1278,  0.1767,  0.2251, -0.1949, -0.2725,\n","         -0.0471,  0.0569],\n","        [-0.2257,  0.1305, -0.2089, -0.2421,  0.2294, -0.0094,  0.2296, -0.1915,\n","          0.0939, -0.1348],\n","        [-0.0463,  0.0366, -0.1727, -0.2105, -0.1880,  0.2199,  0.0507, -0.1102,\n","          0.2126,  0.1470],\n","        [ 0.0747,  0.0096, -0.0857,  0.2531, -0.2127,  0.0584, -0.1867,  0.1945,\n","          0.0125, -0.2956],\n","        [ 0.0661,  0.0315, -0.2936,  0.1815,  0.0036, -0.1510,  0.1799,  0.0566,\n","         -0.0176,  0.0716],\n","        [-0.1892,  0.2319, -0.1090, -0.0801,  0.1215, -0.1373, -0.2843, -0.1957,\n","         -0.1275,  0.2647],\n","        [-0.1024,  0.0083, -0.1834, -0.2195, -0.2156, -0.1577, -0.1573,  0.0411,\n","          0.0935, -0.1677],\n","        [ 0.1531, -0.0698,  0.3053,  0.1977,  0.0259, -0.0265,  0.0216, -0.1463,\n","         -0.1919,  0.0294],\n","        [-0.0897,  0.2531,  0.0740, -0.0054,  0.2413,  0.0715,  0.0280, -0.2820,\n","         -0.0074,  0.0637]])\n","linear_hidden.0.bias tensor([-0.1754,  0.1014,  0.0635,  0.2693, -0.2601,  0.0250,  0.1952,  0.2029,\n","        -0.1872,  0.0598])\n","linear_output.weight tensor([[-0.0931, -0.0626,  0.2058,  0.2311, -0.2221, -0.0535, -0.2132,  0.1724,\n","         -0.0556,  0.0904]])\n","linear_output.bias tensor([0.1784])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6933406591415405 \t Validate_Accuracy: 0.633\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([917])) that is different to the input size (torch.Size([917, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([909])) that is different to the input size (torch.Size([909, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6715883761644363 \t Validate_Accuracy: 0.6175\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6312347948551178 \t Validate_Accuracy: 0.8065\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.5710520297288895 \t Validate_Accuracy: 0.8245\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.5095825344324112 \t Validate_Accuracy: 0.82\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.46954426169395447 \t Validate_Accuracy: 0.821\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.449299156665802 \t Validate_Accuracy: 0.817\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.43195782601833344 \t Validate_Accuracy: 0.81\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.4192747697234154 \t Validate_Accuracy: 0.8215\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.4041189178824425 \t Validate_Accuracy: 0.8185\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.3921358659863472 \t Validate_Accuracy: 0.832\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.37875884026288986 \t Validate_Accuracy: 0.8355\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.3695918917655945 \t Validate_Accuracy: 0.8415\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.35786455124616623 \t Validate_Accuracy: 0.847\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.34573399275541306 \t Validate_Accuracy: 0.848\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3385600373148918 \t Validate_Accuracy: 0.8535\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.336295410990715 \t Validate_Accuracy: 0.857\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3319609761238098 \t Validate_Accuracy: 0.845\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.3252306655049324 \t Validate_Accuracy: 0.855\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.3194144740700722 \t Validate_Accuracy: 0.8535\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.31662552803754807 \t Validate_Accuracy: 0.8555\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3095843568444252 \t Validate_Accuracy: 0.8685\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3080497533082962 \t Validate_Accuracy: 0.853\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.30313344299793243 \t Validate_Accuracy: 0.868\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.29735589772462845 \t Validate_Accuracy: 0.8685\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.2913089990615845 \t Validate_Accuracy: 0.87\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.28582365810871124 \t Validate_Accuracy: 0.86\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.2794346585869789 \t Validate_Accuracy: 0.872\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.2742125242948532 \t Validate_Accuracy: 0.8745\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2709611877799034 \t Validate_Accuracy: 0.873\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.26367202401161194 \t Validate_Accuracy: 0.8645\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.26163439452648163 \t Validate_Accuracy: 0.8655\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.2553895078599453 \t Validate_Accuracy: 0.871\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.2504971958696842 \t Validate_Accuracy: 0.872\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.24440113455057144 \t Validate_Accuracy: 0.8725\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2411971390247345 \t Validate_Accuracy: 0.871\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.24058100581169128 \t Validate_Accuracy: 0.876\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.23801572993397713 \t Validate_Accuracy: 0.878\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.23546774685382843 \t Validate_Accuracy: 0.8805\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2277185283601284 \t Validate_Accuracy: 0.875\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2287387140095234 \t Validate_Accuracy: 0.886\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.22880061715841293 \t Validate_Accuracy: 0.8815\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.22320141270756721 \t Validate_Accuracy: 0.8815\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.22193286195397377 \t Validate_Accuracy: 0.8825\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2211751565337181 \t Validate_Accuracy: 0.8875\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.21983450278639793 \t Validate_Accuracy: 0.881\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.21813887730240822 \t Validate_Accuracy: 0.8855\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.21665258705615997 \t Validate_Accuracy: 0.8885\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.21713116765022278 \t Validate_Accuracy: 0.883\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.21418796852231026 \t Validate_Accuracy: 0.886\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2177475392818451 \t Validate_Accuracy: 0.8905\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.21465612575411797 \t Validate_Accuracy: 0.8845\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.21582506597042084 \t Validate_Accuracy: 0.8865\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.21960687637329102 \t Validate_Accuracy: 0.8875\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2168363332748413 \t Validate_Accuracy: 0.886\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.2165759950876236 \t Validate_Accuracy: 0.886\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.21344075351953506 \t Validate_Accuracy: 0.888\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2163924090564251 \t Validate_Accuracy: 0.887\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.21849174425005913 \t Validate_Accuracy: 0.89\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.21785073727369308 \t Validate_Accuracy: 0.89\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2189790904521942 \t Validate_Accuracy: 0.886\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.21444429084658623 \t Validate_Accuracy: 0.8875\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.21237601339817047 \t Validate_Accuracy: 0.887\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.21266543865203857 \t Validate_Accuracy: 0.89\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.21467992290854454 \t Validate_Accuracy: 0.882\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.21673142537474632 \t Validate_Accuracy: 0.886\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.21645324304699898 \t Validate_Accuracy: 0.8875\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.21462105959653854 \t Validate_Accuracy: 0.8885\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.21615877747535706 \t Validate_Accuracy: 0.8865\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.21386823058128357 \t Validate_Accuracy: 0.8895\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.21357041597366333 \t Validate_Accuracy: 0.889\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.21239355579018593 \t Validate_Accuracy: 0.886\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.22106876224279404 \t Validate_Accuracy: 0.889\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.21140584349632263 \t Validate_Accuracy: 0.888\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.21709925308823586 \t Validate_Accuracy: 0.8885\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2097095251083374 \t Validate_Accuracy: 0.8845\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.20961685478687286 \t Validate_Accuracy: 0.8895\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.21025054529309273 \t Validate_Accuracy: 0.89\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2100072279572487 \t Validate_Accuracy: 0.887\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2118411734700203 \t Validate_Accuracy: 0.8855\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2161216102540493 \t Validate_Accuracy: 0.889\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.21576518937945366 \t Validate_Accuracy: 0.8865\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.21524107828736305 \t Validate_Accuracy: 0.8925\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.2136259861290455 \t Validate_Accuracy: 0.8865\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2121714912354946 \t Validate_Accuracy: 0.89\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.20929979905486107 \t Validate_Accuracy: 0.883\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.20860126242041588 \t Validate_Accuracy: 0.8905\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.21099409461021423 \t Validate_Accuracy: 0.886\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.20976567640900612 \t Validate_Accuracy: 0.889\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.21071986481547356 \t Validate_Accuracy: 0.89\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2125953771173954 \t Validate_Accuracy: 0.8835\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.21537493914365768 \t Validate_Accuracy: 0.887\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.2226334549486637 \t Validate_Accuracy: 0.8865\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2135121002793312 \t Validate_Accuracy: 0.887\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.21195107325911522 \t Validate_Accuracy: 0.885\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.21052265912294388 \t Validate_Accuracy: 0.8905\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.21133947744965553 \t Validate_Accuracy: 0.8905\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2103927582502365 \t Validate_Accuracy: 0.8895\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.21191278100013733 \t Validate_Accuracy: 0.889\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.20830606669187546 \t Validate_Accuracy: 0.8835\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.21070507541298866 \t Validate_Accuracy: 0.8865\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.21418210864067078 \t Validate_Accuracy: 0.8895\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.21613368764519691 \t Validate_Accuracy: 0.882\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.21500271931290627 \t Validate_Accuracy: 0.8895\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.22739327698946 \t Validate_Accuracy: 0.871\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2289894036948681 \t Validate_Accuracy: 0.8845\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.21607215330004692 \t Validate_Accuracy: 0.8905\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.21474721655249596 \t Validate_Accuracy: 0.8895\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2146167792379856 \t Validate_Accuracy: 0.88\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.21742581576108932 \t Validate_Accuracy: 0.8885\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.21250061318278313 \t Validate_Accuracy: 0.886\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.2094327248632908 \t Validate_Accuracy: 0.887\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.21104805916547775 \t Validate_Accuracy: 0.8875\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2090708464384079 \t Validate_Accuracy: 0.888\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.20936844125390053 \t Validate_Accuracy: 0.888\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.21101665869355202 \t Validate_Accuracy: 0.8885\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.21541160345077515 \t Validate_Accuracy: 0.888\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.21058885008096695 \t Validate_Accuracy: 0.882\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.21285539492964745 \t Validate_Accuracy: 0.8865\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.21211716905236244 \t Validate_Accuracy: 0.8885\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.20927003771066666 \t Validate_Accuracy: 0.8865\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21192143857479095 \t Validate_Accuracy: 0.887\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.20751845091581345 \t Validate_Accuracy: 0.8865\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.20805981382727623 \t Validate_Accuracy: 0.888\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.20813915506005287 \t Validate_Accuracy: 0.8865\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.21080312877893448 \t Validate_Accuracy: 0.888\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.20835686102509499 \t Validate_Accuracy: 0.8895\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.21009789779782295 \t Validate_Accuracy: 0.887\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.20754947885870934 \t Validate_Accuracy: 0.89\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.20906436443328857 \t Validate_Accuracy: 0.889\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.2092762142419815 \t Validate_Accuracy: 0.8895\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.20803841575980186 \t Validate_Accuracy: 0.8865\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.21091432124376297 \t Validate_Accuracy: 0.888\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.20831071585416794 \t Validate_Accuracy: 0.8865\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.20934752747416496 \t Validate_Accuracy: 0.8885\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.20686710253357887 \t Validate_Accuracy: 0.8925\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.20732148736715317 \t Validate_Accuracy: 0.8865\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.20808222144842148 \t Validate_Accuracy: 0.888\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.2072647586464882 \t Validate_Accuracy: 0.8865\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.20839055255055428 \t Validate_Accuracy: 0.89\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.20878827944397926 \t Validate_Accuracy: 0.888\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.20931433141231537 \t Validate_Accuracy: 0.8875\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.20803309231996536 \t Validate_Accuracy: 0.8855\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.21210144087672234 \t Validate_Accuracy: 0.882\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.21132512763142586 \t Validate_Accuracy: 0.8825\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.21223752573132515 \t Validate_Accuracy: 0.8875\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.21011459454894066 \t Validate_Accuracy: 0.8855\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.20982547849416733 \t Validate_Accuracy: 0.8845\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.21342667937278748 \t Validate_Accuracy: 0.889\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.20874887704849243 \t Validate_Accuracy: 0.888\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.20941872522234917 \t Validate_Accuracy: 0.883\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.20773005113005638 \t Validate_Accuracy: 0.89\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.20896247774362564 \t Validate_Accuracy: 0.8875\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.20850061625242233 \t Validate_Accuracy: 0.889\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.2092910334467888 \t Validate_Accuracy: 0.8905\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.20980101823806763 \t Validate_Accuracy: 0.886\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.21952752396464348 \t Validate_Accuracy: 0.893\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1238, -0.0419, -0.1700],\n","          [-0.0591,  0.0050, -0.0508],\n","          [-0.1285, -0.0549, -0.1635]]]])\n","conv1.bias tensor([0.0861])\n","first_linear.weight tensor([[ 0.3323,  0.2108,  0.5725,  0.2038,  0.3477, -0.1147, -0.3700, -0.1878,\n","          0.3926],\n","        [-0.3873,  0.0506,  0.0528, -0.1839, -0.3117, -0.6915, -0.3569, -0.1817,\n","         -0.1342],\n","        [-0.8326,  0.9653, -0.4363,  0.0773,  0.0620, -0.2116,  0.6262, -1.1426,\n","          0.5977],\n","        [-0.6654,  1.4963, -0.4379,  0.7504, -1.0592,  0.2124, -0.4517,  0.8131,\n","         -0.2639],\n","        [ 0.0561,  0.1671, -0.7526,  0.1982, -0.3183,  1.7417,  0.7325, -0.3941,\n","         -0.4398],\n","        [ 0.3012,  0.3173, -0.0571, -1.6489,  0.7544, -0.4218,  0.7658, -0.5004,\n","          0.1333],\n","        [-0.0468, -0.1616, -0.0848, -0.0351, -0.0187, -0.0381,  0.0112,  0.0547,\n","          0.0377],\n","        [ 0.2790,  0.4035,  0.1253, -0.0953,  0.2611,  0.6220,  0.3040,  0.2518,\n","          0.0361],\n","        [ 0.0258, -0.0569,  0.1607,  0.1906,  0.2242,  0.2085,  0.1498,  0.1470,\n","          0.2938],\n","        [ 0.3932,  0.4441, -0.3830, -0.9382, -0.3870,  0.6203,  0.0515,  0.5634,\n","         -0.5400]])\n","first_linear.bias tensor([-0.5083, -1.0387, -1.0380,  1.1854,  1.1075,  0.8982, -1.2547,  0.5582,\n","        -1.2529, -0.7611])\n","linear_hidden.0.weight tensor([[ 0.3216, -0.3519,  0.3599, -0.9607, -0.7986, -0.4019, -0.3567,  0.2891,\n","         -0.6585, -0.7381],\n","        [-0.1485, -0.2762, -0.1355, -0.0738,  0.0030,  0.0029, -0.3596,  0.1761,\n","         -0.4064, -0.0670],\n","        [ 0.2092,  0.7244, -0.7730,  0.5962,  0.7150,  0.7530,  0.5350, -0.8639,\n","          0.8347, -0.9031],\n","        [ 0.2378,  0.5093, -0.5933,  0.6013,  0.4615,  0.6134,  0.4405, -0.6255,\n","          0.9791, -0.5943],\n","        [-0.4313, -0.4316,  0.4892, -0.5970, -0.7268, -0.6443, -0.5956,  0.4188,\n","         -0.8929,  0.4191],\n","        [-0.1066, -0.6014,  0.5794, -0.5780, -0.4937, -0.6033, -0.3238,  0.7679,\n","         -0.7768,  0.7674],\n","        [-0.3809, -0.1530,  0.5642, -0.6625, -0.6152, -0.9314, -0.4534,  0.1549,\n","         -0.7640,  0.7294],\n","        [ 0.1844,  0.6103, -0.7651,  0.6607,  0.3242, -0.1946,  0.1971, -0.6603,\n","          0.5605, -0.9948],\n","        [ 0.3498, -0.0560,  0.8983, -0.5697, -1.0302, -0.9768, -0.3736, -0.0753,\n","         -0.7417, -0.9197],\n","        [ 0.1501,  0.3599,  0.2802, -0.0170, -0.1117, -0.1087,  0.3377, -0.1664,\n","          0.3494,  0.1675]])\n","linear_hidden.0.bias tensor([ 0.1447,  0.4286,  0.0859,  0.1991,  0.0800, -0.0295,  0.7696,  0.3157,\n","         0.2421, -0.4480])\n","linear_output.weight tensor([[-1.0732, -0.7776,  1.8364,  1.3680, -1.4661, -1.4553, -1.6225,  1.2264,\n","         -1.3463,  1.0901]])\n","linear_output.bias tensor([0.0484])\n","Testing out: \n","batch_size:  3338\n","train_size:  3745\n","n_epochs:  158\n","lr:  0.04534022536745299\n","weight_decay:  0.0004848269627554254\n","betas0:  0.9985127666371433\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.2398,  0.3132, -0.3234],\n","          [-0.2316,  0.0602, -0.0740],\n","          [ 0.3305,  0.2824, -0.1065]]]])\n","conv1.bias tensor([-0.3149])\n","first_linear.weight tensor([[-0.2726, -0.2381,  0.1446, -0.1731,  0.1419, -0.2623,  0.1161,  0.1108,\n","         -0.1213],\n","        [ 0.0404, -0.0861,  0.3161,  0.1654, -0.1889, -0.1652,  0.0985,  0.2594,\n","          0.0100],\n","        [ 0.3042,  0.2806,  0.0874, -0.0833, -0.2803,  0.3253,  0.1629,  0.3297,\n","         -0.0503],\n","        [ 0.0223,  0.1217, -0.1663,  0.2731,  0.1899,  0.1390, -0.2965,  0.1660,\n","         -0.1606],\n","        [-0.0808, -0.2295, -0.2161,  0.2557, -0.2744, -0.0336, -0.1954,  0.3263,\n","          0.0214],\n","        [ 0.0833,  0.0242, -0.3168, -0.0948,  0.2146, -0.0173,  0.0283, -0.2293,\n","          0.3083],\n","        [ 0.2175, -0.2034,  0.1022, -0.1193,  0.2198, -0.0973, -0.0319, -0.2442,\n","          0.0405],\n","        [-0.2180,  0.0213, -0.1001, -0.0668,  0.1804,  0.3158,  0.3034,  0.1543,\n","         -0.3317],\n","        [-0.0201,  0.0272, -0.0916, -0.0951,  0.0397,  0.1637, -0.2324, -0.0179,\n","          0.0204],\n","        [-0.0535,  0.0758, -0.1980, -0.1841,  0.1196, -0.1379,  0.0516, -0.2862,\n","         -0.2466]])\n","first_linear.bias tensor([-0.2278, -0.1914,  0.2532,  0.2425, -0.1374,  0.2905, -0.0348,  0.0226,\n","         0.2471,  0.1327])\n","linear_hidden.0.weight tensor([[ 0.1779,  0.2870, -0.2736, -0.1859, -0.0886, -0.2476, -0.0663, -0.1240,\n","          0.0542,  0.0038],\n","        [-0.2587,  0.0658, -0.2944,  0.2552, -0.1892, -0.1672,  0.2308, -0.2434,\n","          0.2274,  0.0866],\n","        [ 0.1163,  0.0078,  0.0127, -0.1704, -0.2939,  0.3088,  0.2779,  0.0018,\n","         -0.0173, -0.3148],\n","        [-0.1239,  0.2988, -0.0953,  0.0705, -0.2057, -0.2500,  0.3131, -0.2403,\n","          0.1839, -0.1302],\n","        [ 0.2301, -0.2525, -0.1489,  0.2770, -0.3021,  0.3135, -0.0437, -0.2071,\n","          0.2791, -0.1284],\n","        [ 0.0865, -0.0604, -0.0708,  0.3048,  0.1495, -0.2301,  0.0281,  0.1607,\n","         -0.2327, -0.0311],\n","        [ 0.2740, -0.1436,  0.0947,  0.1689,  0.3093,  0.1207, -0.2210, -0.0276,\n","          0.0246, -0.2531],\n","        [-0.1994,  0.2272, -0.2093,  0.2044,  0.1455, -0.0616, -0.2502, -0.2968,\n","          0.1428, -0.1658],\n","        [ 0.1153,  0.0257,  0.0202,  0.0849,  0.0150,  0.0154, -0.2256, -0.0526,\n","         -0.2393, -0.1633],\n","        [ 0.2693,  0.0445,  0.2136,  0.1307, -0.2624, -0.2591,  0.2354,  0.0504,\n","         -0.1639,  0.1249]])\n","linear_hidden.0.bias tensor([ 0.2157, -0.2157,  0.0505, -0.3068, -0.0332,  0.2832, -0.2088,  0.2917,\n","         0.2981, -0.0258])\n","linear_output.weight tensor([[ 0.2285, -0.2733, -0.1403,  0.1549, -0.0797,  0.0167, -0.0865,  0.1690,\n","          0.2802,  0.2607]])\n","linear_output.bias tensor([0.0686])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([3338])) that is different to the input size (torch.Size([3338, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([407])) that is different to the input size (torch.Size([407, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6956997513771057 \t Validate_Accuracy: 0.494\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6933992803096771 \t Validate_Accuracy: 0.497\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6970742642879486 \t Validate_Accuracy: 0.493\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6944491267204285 \t Validate_Accuracy: 0.503\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6963295340538025 \t Validate_Accuracy: 0.5235\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6944851577281952 \t Validate_Accuracy: 0.5105\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6936366558074951 \t Validate_Accuracy: 0.5385\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6932419538497925 \t Validate_Accuracy: 0.484\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6918240487575531 \t Validate_Accuracy: 0.4795\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6938412487506866 \t Validate_Accuracy: 0.481\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6909970045089722 \t Validate_Accuracy: 0.4805\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6911981403827667 \t Validate_Accuracy: 0.4955\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6911157369613647 \t Validate_Accuracy: 0.5185\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6880536377429962 \t Validate_Accuracy: 0.574\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6873292028903961 \t Validate_Accuracy: 0.5875\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6712289750576019 \t Validate_Accuracy: 0.608\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6589324176311493 \t Validate_Accuracy: 0.602\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6283189356327057 \t Validate_Accuracy: 0.7515\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.5829260945320129 \t Validate_Accuracy: 0.772\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.5249360203742981 \t Validate_Accuracy: 0.8055\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.49514132738113403 \t Validate_Accuracy: 0.806\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.45946213603019714 \t Validate_Accuracy: 0.8115\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.4505235552787781 \t Validate_Accuracy: 0.81\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.46453535556793213 \t Validate_Accuracy: 0.8045\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.4394149035215378 \t Validate_Accuracy: 0.7795\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.4174605756998062 \t Validate_Accuracy: 0.811\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.47440776228904724 \t Validate_Accuracy: 0.8005\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.4358462393283844 \t Validate_Accuracy: 0.8255\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.4327317923307419 \t Validate_Accuracy: 0.826\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3949950933456421 \t Validate_Accuracy: 0.809\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.4058247208595276 \t Validate_Accuracy: 0.8285\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.38624702394008636 \t Validate_Accuracy: 0.8165\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3724724352359772 \t Validate_Accuracy: 0.8275\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3667580783367157 \t Validate_Accuracy: 0.827\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.34662848711013794 \t Validate_Accuracy: 0.835\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3456542193889618 \t Validate_Accuracy: 0.835\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.3444322347640991 \t Validate_Accuracy: 0.8365\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.33839964866638184 \t Validate_Accuracy: 0.8465\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.29453107714653015 \t Validate_Accuracy: 0.8445\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.31164631247520447 \t Validate_Accuracy: 0.852\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2992088347673416 \t Validate_Accuracy: 0.853\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.30359096825122833 \t Validate_Accuracy: 0.8515\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.3151087164878845 \t Validate_Accuracy: 0.853\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.28565381467342377 \t Validate_Accuracy: 0.86\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.27526845037937164 \t Validate_Accuracy: 0.8535\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2820815294981003 \t Validate_Accuracy: 0.8705\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.27417027950286865 \t Validate_Accuracy: 0.87\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.29289981722831726 \t Validate_Accuracy: 0.8695\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.27217352390289307 \t Validate_Accuracy: 0.8745\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.25012803077697754 \t Validate_Accuracy: 0.873\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.242470420897007 \t Validate_Accuracy: 0.884\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2644442319869995 \t Validate_Accuracy: 0.8855\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2561628520488739 \t Validate_Accuracy: 0.8805\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.24653910100460052 \t Validate_Accuracy: 0.8825\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2410172000527382 \t Validate_Accuracy: 0.8815\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.2455539107322693 \t Validate_Accuracy: 0.887\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2186412811279297 \t Validate_Accuracy: 0.882\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.23475651443004608 \t Validate_Accuracy: 0.881\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.22338661551475525 \t Validate_Accuracy: 0.88\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.22110549360513687 \t Validate_Accuracy: 0.8785\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.21337566524744034 \t Validate_Accuracy: 0.8865\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.2347431778907776 \t Validate_Accuracy: 0.876\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.22429337352514267 \t Validate_Accuracy: 0.881\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2346581667661667 \t Validate_Accuracy: 0.8855\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2163720801472664 \t Validate_Accuracy: 0.8865\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.21082428842782974 \t Validate_Accuracy: 0.885\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.20935402065515518 \t Validate_Accuracy: 0.8875\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.21748965233564377 \t Validate_Accuracy: 0.8875\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.20837142318487167 \t Validate_Accuracy: 0.886\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2102522850036621 \t Validate_Accuracy: 0.884\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2202071025967598 \t Validate_Accuracy: 0.8815\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.21954604238271713 \t Validate_Accuracy: 0.8835\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2250162661075592 \t Validate_Accuracy: 0.888\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.21272318065166473 \t Validate_Accuracy: 0.8865\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2090933471918106 \t Validate_Accuracy: 0.8785\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.23527776449918747 \t Validate_Accuracy: 0.885\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.22901026904582977 \t Validate_Accuracy: 0.882\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.2135051190853119 \t Validate_Accuracy: 0.8825\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.20909430086612701 \t Validate_Accuracy: 0.877\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.23034533113241196 \t Validate_Accuracy: 0.8775\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.24256765097379684 \t Validate_Accuracy: 0.88\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.24228625744581223 \t Validate_Accuracy: 0.8865\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.23374756425619125 \t Validate_Accuracy: 0.881\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.23138248920440674 \t Validate_Accuracy: 0.8855\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.20433085411787033 \t Validate_Accuracy: 0.87\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.23111655563116074 \t Validate_Accuracy: 0.879\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.24566173553466797 \t Validate_Accuracy: 0.8845\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.21658045053482056 \t Validate_Accuracy: 0.8815\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.24746840447187424 \t Validate_Accuracy: 0.885\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2536562532186508 \t Validate_Accuracy: 0.8845\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.22681818902492523 \t Validate_Accuracy: 0.8835\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2574373185634613 \t Validate_Accuracy: 0.879\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.21486978232860565 \t Validate_Accuracy: 0.891\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.22219989448785782 \t Validate_Accuracy: 0.884\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.20741290599107742 \t Validate_Accuracy: 0.892\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.22490543127059937 \t Validate_Accuracy: 0.8815\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.20774763822555542 \t Validate_Accuracy: 0.8825\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.21367784589529037 \t Validate_Accuracy: 0.887\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.21457915753126144 \t Validate_Accuracy: 0.885\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2335430458188057 \t Validate_Accuracy: 0.8875\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.20873618870973587 \t Validate_Accuracy: 0.8835\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.20768337696790695 \t Validate_Accuracy: 0.8845\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.22287895530462265 \t Validate_Accuracy: 0.8905\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2014838457107544 \t Validate_Accuracy: 0.891\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.22602012008428574 \t Validate_Accuracy: 0.893\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.20263399183750153 \t Validate_Accuracy: 0.8895\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2163299024105072 \t Validate_Accuracy: 0.8915\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.20108819007873535 \t Validate_Accuracy: 0.887\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.21226439625024796 \t Validate_Accuracy: 0.885\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.22215671092271805 \t Validate_Accuracy: 0.8855\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.20315656810998917 \t Validate_Accuracy: 0.8875\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.210523322224617 \t Validate_Accuracy: 0.889\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.21615269780158997 \t Validate_Accuracy: 0.8925\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.21558096259832382 \t Validate_Accuracy: 0.889\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.22129709273576736 \t Validate_Accuracy: 0.8865\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.21717406064271927 \t Validate_Accuracy: 0.892\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.22380799800157547 \t Validate_Accuracy: 0.8915\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.2151421681046486 \t Validate_Accuracy: 0.886\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.21234656870365143 \t Validate_Accuracy: 0.8845\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.22682151943445206 \t Validate_Accuracy: 0.8905\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.20308475941419601 \t Validate_Accuracy: 0.889\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21795490384101868 \t Validate_Accuracy: 0.891\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.20547038316726685 \t Validate_Accuracy: 0.882\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.21780993789434433 \t Validate_Accuracy: 0.886\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.20478500425815582 \t Validate_Accuracy: 0.891\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.22373655438423157 \t Validate_Accuracy: 0.895\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.22516555339097977 \t Validate_Accuracy: 0.88\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.23889576643705368 \t Validate_Accuracy: 0.8895\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23019862920045853 \t Validate_Accuracy: 0.8865\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.20461717247962952 \t Validate_Accuracy: 0.883\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.22350917011499405 \t Validate_Accuracy: 0.886\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.23203591257333755 \t Validate_Accuracy: 0.8875\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.19159731268882751 \t Validate_Accuracy: 0.8885\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.19575675576925278 \t Validate_Accuracy: 0.889\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.21971210092306137 \t Validate_Accuracy: 0.883\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.23118674755096436 \t Validate_Accuracy: 0.8795\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.21932482719421387 \t Validate_Accuracy: 0.88\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.2164434865117073 \t Validate_Accuracy: 0.883\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.22157442569732666 \t Validate_Accuracy: 0.8895\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.2069924771785736 \t Validate_Accuracy: 0.889\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2113550379872322 \t Validate_Accuracy: 0.8955\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.21509627252817154 \t Validate_Accuracy: 0.893\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.21988078951835632 \t Validate_Accuracy: 0.8875\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.19936735928058624 \t Validate_Accuracy: 0.8875\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.21231385320425034 \t Validate_Accuracy: 0.891\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.1927838921546936 \t Validate_Accuracy: 0.891\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.21528858691453934 \t Validate_Accuracy: 0.892\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.19296308606863022 \t Validate_Accuracy: 0.882\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.22262903302907944 \t Validate_Accuracy: 0.883\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.2017686441540718 \t Validate_Accuracy: 0.885\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2202654704451561 \t Validate_Accuracy: 0.888\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.21232083439826965 \t Validate_Accuracy: 0.8855\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2165999710559845 \t Validate_Accuracy: 0.894\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.1889856457710266 \t Validate_Accuracy: 0.8915\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.21081557869911194 \t Validate_Accuracy: 0.8935\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.2157096043229103 \t Validate_Accuracy: 0.8855\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.20991910994052887 \t Validate_Accuracy: 0.888\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.19289512187242508 \t Validate_Accuracy: 0.892\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1420,  0.0359,  0.1962],\n","          [ 0.0784, -0.0114,  0.0288],\n","          [ 0.1271,  0.0494,  0.1922]]]])\n","conv1.bias tensor([-0.0725])\n","first_linear.weight tensor([[-4.3178e-02,  1.2543e-01, -5.2630e-01, -2.0175e-01,  5.6439e-02,\n","          3.7707e-02,  7.2173e-01,  3.1163e-01,  1.9117e-01],\n","        [ 2.8516e-01,  1.9561e-01,  1.8791e-01,  1.4578e-01,  3.4407e-01,\n","          3.6404e-01,  1.5975e-01,  1.0214e-02,  3.1654e-01],\n","        [-5.8908e-01,  7.6490e-01, -3.8177e-02, -5.1724e-01, -3.9565e-01,\n","         -1.8513e-01,  7.0290e-01, -4.2978e-02, -5.6640e-01],\n","        [-5.5018e-01, -2.1754e-01,  2.3434e-02,  1.8155e+00, -7.3043e-01,\n","          5.2887e-01, -4.6737e-01,  8.8853e-03, -3.0537e-02],\n","        [ 1.5080e-01,  2.6688e-01,  2.8011e-01,  1.7508e-01, -5.2465e-01,\n","         -9.0944e-01, -1.1594e+00,  1.5889e+00, -1.4777e-01],\n","        [ 1.0949e+00, -1.6311e+00,  5.7202e-01, -7.9446e-01,  7.4472e-01,\n","          9.2246e-02,  4.6611e-02,  1.2963e-01, -3.0108e-01],\n","        [ 2.0046e-01, -1.6420e-04,  1.7139e-01,  1.6238e-01,  4.1153e-01,\n","          2.1265e-01,  1.7436e-01,  3.5209e-01,  3.5926e-01],\n","        [ 3.4979e-01,  9.6706e-02, -3.1838e-01,  1.9784e-01, -1.0248e+00,\n","          1.2575e+00, -2.9683e-01,  1.1192e+00, -1.0392e+00],\n","        [-2.9104e-01, -3.0891e-01,  6.5947e-01,  2.9136e-01,  1.3042e-01,\n","         -4.0184e-01, -4.5647e-02,  2.4886e-01,  4.8840e-02],\n","        [ 5.3384e-02,  8.6110e-01, -6.6973e-01, -5.8212e-01, -2.5471e-01,\n","          1.1389e+00,  4.2742e-01, -2.4179e-01, -1.3710e-01]])\n","first_linear.bias tensor([-0.1318, -1.4550,  0.4115,  1.0157,  1.1355,  1.2257,  1.1774, -1.1581,\n","        -0.3123,  0.6569])\n","linear_hidden.0.weight tensor([[ 0.2083,  0.8915,  0.1917,  0.3917,  0.3172,  0.3565, -0.6507, -0.1275,\n","         -0.1420, -0.0086],\n","        [ 0.3263,  0.6882, -0.2843,  0.2529,  0.1346, -0.0328,  0.0494, -0.1437,\n","          0.3142,  0.5525],\n","        [ 0.1497,  1.1208,  0.3505,  0.4475,  0.3594,  0.4860, -0.8087, -0.1756,\n","         -0.3048, -0.1403],\n","        [ 0.0200,  0.7053,  0.0082,  0.4496,  0.3555,  0.1419,  0.3433, -0.1657,\n","          0.3905,  0.9538],\n","        [-0.2882, -1.2644, -0.4546, -0.5382, -0.5280, -0.6709,  0.4875,  0.6024,\n","          0.2059, -0.6408],\n","        [-0.5721,  1.1053, -0.1074,  0.6877,  0.3674,  0.8866, -0.3448, -0.7692,\n","          0.2911, -0.5787],\n","        [ 0.4371,  0.7486,  0.5392,  0.1713,  0.1464,  0.6300, -0.3817, -0.7935,\n","         -0.4986,  0.7404],\n","        [-0.2416,  0.9602, -0.1575,  0.9986,  0.6513,  0.5161, -0.2285, -0.7681,\n","          0.1915, -0.4926],\n","        [ 0.2457,  1.0602,  0.3112,  0.3907,  0.5640,  0.6097, -0.4324, -0.5539,\n","         -0.3175,  0.6156],\n","        [ 0.1985,  0.8763,  0.2997,  0.4087,  0.5113,  0.5794, -0.4054, -0.5515,\n","         -0.2753,  0.6167]])\n","linear_hidden.0.bias tensor([-0.3359, -0.6180, -0.3102, -0.8957,  0.2295, -0.0055, -0.2139, -0.0165,\n","        -0.2809, -0.4227])\n","linear_output.weight tensor([[ 1.2004,  0.7765,  1.5129,  1.0669, -1.7835,  1.3544,  1.3006,  1.3793,\n","          1.5406,  1.3851]])\n","linear_output.bias tensor([-0.3197])\n","Testing out: \n","batch_size:  1080\n","train_size:  3526\n","n_epochs:  111\n","lr:  0.107051506332365\n","weight_decay:  0.00043954317914716545\n","betas0:  0.9996357372978657\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2320, -0.1685,  0.0025],\n","          [-0.0464, -0.2432,  0.1130],\n","          [-0.2846, -0.0069,  0.1310]]]])\n","conv1.bias tensor([0.0545])\n","first_linear.weight tensor([[ 0.0472,  0.0517, -0.1685,  0.0973,  0.3315,  0.1536, -0.1326, -0.0970,\n","         -0.2647],\n","        [ 0.3132, -0.0165,  0.2936, -0.1210, -0.2680, -0.1574,  0.0291,  0.0330,\n","          0.1350],\n","        [-0.3190,  0.0502,  0.0728, -0.1640,  0.0129, -0.1813, -0.2740,  0.0088,\n","         -0.3138],\n","        [-0.2011, -0.0868,  0.0240,  0.3228,  0.2444, -0.1437, -0.2596, -0.1087,\n","         -0.1862],\n","        [-0.3002, -0.2765, -0.0921, -0.2844,  0.2991,  0.3256, -0.1989,  0.1221,\n","          0.1502],\n","        [-0.3107,  0.0793, -0.2050, -0.2973, -0.0992, -0.0415,  0.2322, -0.1992,\n","         -0.3281],\n","        [ 0.2214, -0.2795, -0.0763, -0.1511, -0.3139,  0.2230, -0.1634, -0.0480,\n","          0.0726],\n","        [ 0.1562,  0.1562,  0.0827,  0.1967,  0.1005,  0.2693,  0.0323,  0.0667,\n","         -0.2086],\n","        [-0.2814,  0.1032,  0.0569,  0.0438, -0.1874,  0.1187, -0.0971,  0.1080,\n","          0.1280],\n","        [ 0.1486,  0.2823, -0.0113,  0.0555, -0.3244,  0.0178,  0.1703,  0.1845,\n","         -0.1394]])\n","first_linear.bias tensor([ 0.0227, -0.2553, -0.1485, -0.1050, -0.1299, -0.1662, -0.1918, -0.0203,\n","        -0.0937,  0.1909])\n","linear_hidden.0.weight tensor([[ 3.0731e-02, -1.8778e-01,  1.4487e-01,  4.9957e-02,  3.1431e-01,\n","         -4.7726e-02,  2.3518e-01, -1.9988e-01, -1.9554e-01, -1.5299e-01],\n","        [ 2.9371e-01,  2.1976e-01, -1.3227e-01,  3.3822e-02,  1.6880e-01,\n","         -1.0432e-02,  2.6012e-01,  2.4834e-01, -1.8196e-01, -1.7613e-01],\n","        [-6.0500e-02, -2.8336e-01,  5.5407e-02, -1.5227e-03,  3.8588e-02,\n","         -2.1423e-01,  9.5120e-02, -1.2937e-01, -1.0078e-01,  2.2561e-01],\n","        [ 2.0465e-01, -1.2472e-02,  2.0619e-01, -2.3401e-01, -1.4110e-01,\n","          1.2502e-02, -1.9397e-02,  2.8678e-01, -2.7702e-02,  1.5796e-01],\n","        [-1.8594e-01, -1.4574e-01, -2.2174e-01,  1.1682e-01,  2.6919e-01,\n","         -1.5964e-01,  1.4681e-01, -2.3449e-02, -2.9936e-02, -1.6364e-01],\n","        [ 2.2374e-01, -1.2188e-01,  2.8743e-01,  7.7074e-02, -1.1320e-01,\n","         -1.4748e-01, -2.2806e-01, -1.1322e-01, -2.5190e-01,  4.9112e-03],\n","        [-1.5082e-01, -4.5641e-02,  2.5259e-01,  8.8238e-02,  2.0874e-02,\n","          7.7459e-02, -2.8513e-01, -5.5074e-02, -5.5452e-02,  1.9959e-01],\n","        [-1.2533e-01,  3.3427e-03, -2.8294e-04, -1.5445e-01, -2.9094e-02,\n","          2.1770e-01,  7.3179e-02, -6.3745e-02,  1.2179e-01, -2.6410e-01],\n","        [-1.5835e-01,  2.4325e-01, -2.8690e-01,  9.1226e-02, -1.6178e-01,\n","          3.2307e-02, -1.9912e-01,  2.0527e-01, -2.3496e-01, -1.0029e-02],\n","        [-2.7624e-01, -1.2541e-01, -1.6269e-01, -2.9513e-01, -1.0449e-01,\n","         -1.3880e-01, -7.4295e-02,  1.2378e-01, -2.6316e-01,  1.2172e-01]])\n","linear_hidden.0.bias tensor([ 0.1358, -0.2495,  0.0410, -0.2821,  0.3108,  0.2360, -0.1806,  0.0688,\n","        -0.0467, -0.2453])\n","linear_output.weight tensor([[-0.3010, -0.2207,  0.3094, -0.1828,  0.0003,  0.1429,  0.2675,  0.2113,\n","         -0.2664,  0.1821]])\n","linear_output.bias tensor([-0.2203])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7032931298017502 \t Validate_Accuracy: 0.6395\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1080])) that is different to the input size (torch.Size([1080, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([286])) that is different to the input size (torch.Size([286, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6399931311607361 \t Validate_Accuracy: 0.7965\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.5124683752655983 \t Validate_Accuracy: 0.8055\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.4496164247393608 \t Validate_Accuracy: 0.8045\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.4322058632969856 \t Validate_Accuracy: 0.8215\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.3992621526122093 \t Validate_Accuracy: 0.824\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.3920150548219681 \t Validate_Accuracy: 0.8225\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.3778050094842911 \t Validate_Accuracy: 0.841\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.3432097062468529 \t Validate_Accuracy: 0.8525\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.34034405648708344 \t Validate_Accuracy: 0.849\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.3286140188574791 \t Validate_Accuracy: 0.849\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.3261665031313896 \t Validate_Accuracy: 0.85\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.31236570328474045 \t Validate_Accuracy: 0.846\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.3366278260946274 \t Validate_Accuracy: 0.8385\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.3288562074303627 \t Validate_Accuracy: 0.8455\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3172719478607178 \t Validate_Accuracy: 0.857\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3035613223910332 \t Validate_Accuracy: 0.849\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3158141300082207 \t Validate_Accuracy: 0.843\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.30431637167930603 \t Validate_Accuracy: 0.8345\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.31003499031066895 \t Validate_Accuracy: 0.857\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.29117439687252045 \t Validate_Accuracy: 0.8565\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3022209033370018 \t Validate_Accuracy: 0.8565\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.2797947898507118 \t Validate_Accuracy: 0.8575\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.2911309078335762 \t Validate_Accuracy: 0.857\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.29225437343120575 \t Validate_Accuracy: 0.859\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.29313337057828903 \t Validate_Accuracy: 0.8605\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.2896403446793556 \t Validate_Accuracy: 0.8535\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3047717958688736 \t Validate_Accuracy: 0.856\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3038351684808731 \t Validate_Accuracy: 0.8395\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2977701351046562 \t Validate_Accuracy: 0.851\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.30436502397060394 \t Validate_Accuracy: 0.8635\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.29131118953227997 \t Validate_Accuracy: 0.8615\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.29577378183603287 \t Validate_Accuracy: 0.8615\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.27958257496356964 \t Validate_Accuracy: 0.8605\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.2810128331184387 \t Validate_Accuracy: 0.8505\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2780643403530121 \t Validate_Accuracy: 0.856\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.2936868444085121 \t Validate_Accuracy: 0.859\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.30311042070388794 \t Validate_Accuracy: 0.8365\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.30082833021879196 \t Validate_Accuracy: 0.8605\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.28746896982192993 \t Validate_Accuracy: 0.8595\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.28007908165454865 \t Validate_Accuracy: 0.8515\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.28997691720724106 \t Validate_Accuracy: 0.8585\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.28573033958673477 \t Validate_Accuracy: 0.855\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.29332205653190613 \t Validate_Accuracy: 0.87\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2875746190547943 \t Validate_Accuracy: 0.864\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.28484126925468445 \t Validate_Accuracy: 0.857\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2747298330068588 \t Validate_Accuracy: 0.856\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.28184277564287186 \t Validate_Accuracy: 0.8585\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.28360671550035477 \t Validate_Accuracy: 0.861\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.2855558916926384 \t Validate_Accuracy: 0.8545\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.27914147078990936 \t Validate_Accuracy: 0.8625\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2845698148012161 \t Validate_Accuracy: 0.8625\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2756248116493225 \t Validate_Accuracy: 0.8465\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2929375097155571 \t Validate_Accuracy: 0.8665\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2689605914056301 \t Validate_Accuracy: 0.865\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.28294437378644943 \t Validate_Accuracy: 0.8545\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2886135131120682 \t Validate_Accuracy: 0.855\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2810655012726784 \t Validate_Accuracy: 0.868\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.2745463326573372 \t Validate_Accuracy: 0.852\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.28440308570861816 \t Validate_Accuracy: 0.863\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.28361303731799126 \t Validate_Accuracy: 0.859\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.28396082669496536 \t Validate_Accuracy: 0.861\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.2749403268098831 \t Validate_Accuracy: 0.869\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2790221720933914 \t Validate_Accuracy: 0.858\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2703428193926811 \t Validate_Accuracy: 0.858\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.27525898814201355 \t Validate_Accuracy: 0.859\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.27045975625514984 \t Validate_Accuracy: 0.8515\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.28032592684030533 \t Validate_Accuracy: 0.8665\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.28198105841875076 \t Validate_Accuracy: 0.8685\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.28370556607842445 \t Validate_Accuracy: 0.8615\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2783069908618927 \t Validate_Accuracy: 0.855\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.28894083201885223 \t Validate_Accuracy: 0.865\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2776305824518204 \t Validate_Accuracy: 0.8515\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2821144089102745 \t Validate_Accuracy: 0.861\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.27239876985549927 \t Validate_Accuracy: 0.8485\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2765354812145233 \t Validate_Accuracy: 0.852\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.2800697460770607 \t Validate_Accuracy: 0.8525\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.26595208048820496 \t Validate_Accuracy: 0.8645\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2803244888782501 \t Validate_Accuracy: 0.868\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.27749956399202347 \t Validate_Accuracy: 0.8575\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.28405119478702545 \t Validate_Accuracy: 0.854\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.27748876065015793 \t Validate_Accuracy: 0.866\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.27008799463510513 \t Validate_Accuracy: 0.8645\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.2695026695728302 \t Validate_Accuracy: 0.8605\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2694007083773613 \t Validate_Accuracy: 0.8605\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.2610357664525509 \t Validate_Accuracy: 0.848\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.266128845512867 \t Validate_Accuracy: 0.8615\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2706182524561882 \t Validate_Accuracy: 0.8595\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.27825281769037247 \t Validate_Accuracy: 0.86\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2814946323633194 \t Validate_Accuracy: 0.869\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.26655441522598267 \t Validate_Accuracy: 0.8735\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2611026354134083 \t Validate_Accuracy: 0.8675\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.27432625368237495 \t Validate_Accuracy: 0.858\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.26519838720560074 \t Validate_Accuracy: 0.861\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.28858134150505066 \t Validate_Accuracy: 0.8675\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.27266400307416916 \t Validate_Accuracy: 0.857\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.2625586539506912 \t Validate_Accuracy: 0.863\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.271634966135025 \t Validate_Accuracy: 0.8625\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.27594559267163277 \t Validate_Accuracy: 0.8635\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2632379047572613 \t Validate_Accuracy: 0.864\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2646585926413536 \t Validate_Accuracy: 0.8605\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.26808610558509827 \t Validate_Accuracy: 0.861\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.26289284601807594 \t Validate_Accuracy: 0.8605\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.27734455466270447 \t Validate_Accuracy: 0.8595\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2720663994550705 \t Validate_Accuracy: 0.86\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2715993784368038 \t Validate_Accuracy: 0.862\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.26633142307400703 \t Validate_Accuracy: 0.87\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.26872558146715164 \t Validate_Accuracy: 0.855\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2756248116493225 \t Validate_Accuracy: 0.859\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.27312376350164413 \t Validate_Accuracy: 0.869\n","epoch 111\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:52:45] ax.service.managed_loop: Running optimization trial 6...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 111 \t Train Loss: 0.26860594749450684 \t Validate_Accuracy: 0.857\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.2543, -0.1934, -0.3063],\n","          [-0.0814,  0.0530, -0.0588],\n","          [-0.2725, -0.0909, -0.1425]]]])\n","conv1.bias tensor([0.0042])\n","first_linear.weight tensor([[ 9.9509e-01, -1.4198e+00,  1.6349e+00, -3.6961e-01, -5.1542e-02,\n","         -7.9767e-01,  1.8320e-01,  1.6811e-03,  4.3511e-01],\n","        [ 9.4989e-02, -3.6178e-01, -2.1237e-01,  2.4430e-01, -7.5957e-01,\n","         -9.5168e-01, -3.1932e-01,  5.2917e-02, -8.9777e-01],\n","        [-5.6234e-01, -8.6204e-04,  5.5793e-01, -7.8152e-01, -7.2910e-01,\n","          4.9552e-02, -6.7986e-01, -5.8232e-01, -2.2806e-01],\n","        [ 4.0933e-01, -1.2113e-01,  6.0503e-01,  1.0441e+00, -9.0990e-01,\n","          2.8873e-01, -1.0358e+00,  1.2046e+00,  6.0252e-02],\n","        [ 3.0656e-02,  1.8295e-01,  7.8450e-01,  1.8653e-01, -6.6158e-02,\n","          8.6879e-01,  2.8617e-01,  7.3143e-01,  7.5385e-01],\n","        [ 8.7929e-01, -1.4448e-01,  1.6574e-01, -5.5511e-01,  7.5758e-01,\n","          4.5105e-01,  8.3075e-01, -1.2242e+00,  2.3313e-01],\n","        [ 9.1238e-01,  8.0041e-01,  4.9492e-01,  6.9484e-01, -2.3298e-01,\n","         -3.3070e-01,  6.2450e-01, -2.3970e-01, -6.2557e-01],\n","        [ 8.7544e-01,  4.8238e-01,  5.3152e-01,  4.4808e-01,  8.4136e-01,\n","          5.5410e-02, -3.4766e-01,  4.1157e-02,  2.1160e-01],\n","        [-4.9294e-01,  9.8911e-01, -4.3777e-01,  6.1228e-01, -2.1601e+00,\n","          1.0384e+00,  9.5230e-02,  5.5667e-01, -2.7866e-01],\n","        [ 1.3198e-01, -8.3360e-02, -5.7568e-01, -7.2368e-01, -6.5623e-01,\n","          4.3409e-02,  1.2530e+00, -6.5671e-01,  1.7424e-01]])\n","first_linear.bias tensor([-1.4771, -1.3501, -1.3475,  1.0090, -1.5310, -0.0290, -1.2849,  1.3002,\n","        -1.4738,  0.7720])\n","linear_hidden.0.weight tensor([[-1.3872e+00,  2.0861e-01,  8.6988e-01,  4.6924e-01,  8.3263e-01,\n","          4.7801e-01,  2.0685e-01, -6.4409e-01, -6.1126e-01,  1.1261e+00],\n","        [ 1.0316e+00, -7.3207e-01,  1.4334e-02, -6.4730e-01, -9.3351e-01,\n","          9.1957e-01, -4.3230e-01,  4.5163e-01,  1.4228e+00, -1.9408e-01],\n","        [-8.0855e-04,  8.0424e-05,  2.7430e-04,  4.1251e-04,  4.2234e-04,\n","          2.7405e-04,  7.4514e-04, -1.4106e-04, -1.2841e-03,  2.7295e-04],\n","        [ 6.0009e-01, -4.4758e-01, -1.7052e-01, -9.1801e-02, -9.4340e-01,\n","          6.9663e-02, -6.9313e-02,  5.4213e-01,  2.9730e-01,  1.3416e-01],\n","        [ 6.2194e-02, -2.0725e-02,  3.1483e-02, -6.4057e-02,  1.4415e-02,\n","         -1.1510e-01,  3.2583e-02, -3.3766e-02,  9.5975e-02,  1.6656e-02],\n","        [-4.8782e-01,  4.5679e-01,  3.5318e-02,  1.7790e+00, -2.8324e-01,\n","         -4.9172e-01,  5.9706e-02, -2.0348e-01, -1.0969e+00, -5.1490e-01],\n","        [-7.2066e-01, -2.7236e-01,  2.6843e-01, -8.3156e-01,  4.1214e-01,\n","          1.1593e+00,  2.0377e-01, -1.4200e-01, -1.0377e+00,  1.3043e+00],\n","        [ 2.3306e-01,  7.0363e-03,  3.6746e-01,  5.1224e-01,  5.9046e-01,\n","          7.0677e-01,  1.3818e+00,  1.5068e-01, -3.1626e-01, -7.3713e-01],\n","        [ 1.6289e-01, -5.1406e-01, -4.0265e-01,  1.9511e-01, -3.8020e-01,\n","         -1.0082e-01, -2.2325e-01,  3.1513e-01,  2.7184e-02,  1.2639e-01],\n","        [ 3.0032e-01, -9.7238e-01, -6.9798e-01,  3.7583e-01, -3.9295e-01,\n","         -8.4421e-02, -1.5624e-01,  5.5503e-01,  1.5026e-01,  1.8075e-01]])\n","linear_hidden.0.bias tensor([ 0.2834,  0.3469, -0.0007,  0.3510, -0.0745,  0.2677,  0.4785, -0.5633,\n","         0.7088,  0.7003])\n","linear_output.weight tensor([[ 1.2615e+00, -1.2902e+00,  1.2702e-04, -6.6074e-01, -3.0299e-02,\n","          1.7750e+00,  1.7452e+00,  1.0515e+00, -8.3627e-01, -1.2137e+00]])\n","linear_output.bias tensor([-0.3255])\n","Testing out: \n","batch_size:  2568\n","train_size:  3476\n","n_epochs:  136\n","lr:  0.03489965621739506\n","weight_decay:  0.0003243748688847592\n","betas0:  0.995517200914019\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.2222, -0.0636, -0.0936],\n","          [-0.1854, -0.1975,  0.0147],\n","          [ 0.3187,  0.0429, -0.3311]]]])\n","conv1.bias tensor([0.2355])\n","first_linear.weight tensor([[ 0.2861,  0.3112, -0.3001,  0.2010,  0.1277,  0.0675,  0.0233,  0.1956,\n","          0.2752],\n","        [-0.2302,  0.1588,  0.2898,  0.2592, -0.1893,  0.1100,  0.0520,  0.1583,\n","         -0.3041],\n","        [-0.2412,  0.2223, -0.2856, -0.3322,  0.2969,  0.1067,  0.0442,  0.0296,\n","         -0.1527],\n","        [-0.2609, -0.0297, -0.2656, -0.2284,  0.0436, -0.0614, -0.0290,  0.1508,\n","         -0.0594],\n","        [ 0.0107, -0.2291,  0.1992, -0.2036,  0.2728, -0.1685,  0.1511,  0.2313,\n","         -0.3072],\n","        [ 0.3157, -0.2804, -0.1184,  0.2004,  0.0603, -0.0545, -0.2719,  0.3189,\n","          0.3004],\n","        [-0.2211,  0.2105,  0.0116,  0.0491, -0.2662,  0.0379, -0.2562,  0.2863,\n","          0.3034],\n","        [ 0.0599,  0.3105,  0.0519, -0.2428, -0.0481,  0.2397, -0.2627,  0.2532,\n","         -0.1549],\n","        [ 0.2904, -0.1355, -0.3305, -0.2076,  0.1087,  0.2786,  0.3092, -0.3149,\n","          0.0885],\n","        [ 0.2587, -0.1701,  0.0082,  0.0521, -0.0782,  0.1821, -0.1250, -0.2387,\n","          0.2054]])\n","first_linear.bias tensor([-0.2532, -0.2138,  0.1911,  0.2831,  0.1332, -0.0931, -0.0467, -0.3196,\n","         0.0907, -0.3168])\n","linear_hidden.0.weight tensor([[-0.2486, -0.2393,  0.2561, -0.1940,  0.1546, -0.0486, -0.0195, -0.2974,\n","         -0.0755,  0.1895],\n","        [ 0.0846,  0.2592,  0.0484, -0.2307,  0.1357, -0.2784,  0.0812, -0.0972,\n","          0.1960,  0.1682],\n","        [-0.0373, -0.2641,  0.1014,  0.2494,  0.0799,  0.0042, -0.0943, -0.0112,\n","          0.0022,  0.0347],\n","        [-0.2513, -0.0071, -0.2985,  0.1330,  0.1136,  0.1457, -0.2563,  0.0861,\n","          0.0627, -0.1452],\n","        [ 0.1176, -0.2305,  0.2844, -0.2820,  0.1573, -0.1827,  0.2208,  0.2077,\n","         -0.0287,  0.1623],\n","        [-0.0298,  0.3053,  0.0806,  0.3080, -0.1958, -0.2011, -0.2386, -0.1233,\n","          0.0254, -0.1114],\n","        [-0.1473,  0.2273,  0.2682,  0.1283, -0.2416, -0.1584, -0.2737,  0.0763,\n","         -0.2030, -0.0953],\n","        [-0.2801,  0.0200,  0.2144,  0.3035, -0.2944,  0.0706, -0.2496, -0.1239,\n","         -0.2962, -0.2385],\n","        [-0.0319, -0.0851,  0.2237, -0.1644,  0.2880,  0.1624, -0.2404, -0.0914,\n","          0.1343, -0.1364],\n","        [-0.0132, -0.1764,  0.2987, -0.2565, -0.1241, -0.1798, -0.1112,  0.2459,\n","          0.3090,  0.2536]])\n","linear_hidden.0.bias tensor([ 0.2001,  0.0778, -0.0356,  0.0274, -0.0988,  0.3103, -0.3106,  0.1589,\n","        -0.2296, -0.1121])\n","linear_output.weight tensor([[-0.1217, -0.0651,  0.1453, -0.2014,  0.0450, -0.0933,  0.2039,  0.0333,\n","          0.0425, -0.0493]])\n","linear_output.bias tensor([-0.0190])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6956081688404083 \t Validate_Accuracy: 0.5265\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2568])) that is different to the input size (torch.Size([2568, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([908])) that is different to the input size (torch.Size([908, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6929388344287872 \t Validate_Accuracy: 0.4995\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6926884055137634 \t Validate_Accuracy: 0.558\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6897078156471252 \t Validate_Accuracy: 0.5325\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6856678426265717 \t Validate_Accuracy: 0.5615\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6771183907985687 \t Validate_Accuracy: 0.5935\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6573483049869537 \t Validate_Accuracy: 0.7035\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6234309673309326 \t Validate_Accuracy: 0.781\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5778015851974487 \t Validate_Accuracy: 0.81\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.5273618996143341 \t Validate_Accuracy: 0.8215\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.4749194532632828 \t Validate_Accuracy: 0.823\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4229704439640045 \t Validate_Accuracy: 0.822\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.39782728254795074 \t Validate_Accuracy: 0.8305\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.3908500373363495 \t Validate_Accuracy: 0.826\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.3732597529888153 \t Validate_Accuracy: 0.833\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.36030904948711395 \t Validate_Accuracy: 0.828\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3490499258041382 \t Validate_Accuracy: 0.828\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3421403765678406 \t Validate_Accuracy: 0.847\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.32994140684604645 \t Validate_Accuracy: 0.851\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.3141351491212845 \t Validate_Accuracy: 0.8565\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.2972940653562546 \t Validate_Accuracy: 0.865\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.29233449697494507 \t Validate_Accuracy: 0.864\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.2819775193929672 \t Validate_Accuracy: 0.861\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.2663160711526871 \t Validate_Accuracy: 0.8655\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.26758691668510437 \t Validate_Accuracy: 0.865\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.26071183383464813 \t Validate_Accuracy: 0.8735\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.2645081728696823 \t Validate_Accuracy: 0.8645\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.2611888349056244 \t Validate_Accuracy: 0.865\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.2519063651561737 \t Validate_Accuracy: 0.868\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2571955993771553 \t Validate_Accuracy: 0.865\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.24314622581005096 \t Validate_Accuracy: 0.8735\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.25353696197271347 \t Validate_Accuracy: 0.8725\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.242986761033535 \t Validate_Accuracy: 0.8745\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.249738872051239 \t Validate_Accuracy: 0.8745\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.23683852702379227 \t Validate_Accuracy: 0.874\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2524772435426712 \t Validate_Accuracy: 0.88\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.24366408586502075 \t Validate_Accuracy: 0.8685\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.25121571123600006 \t Validate_Accuracy: 0.869\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.23851832002401352 \t Validate_Accuracy: 0.8725\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.23260971158742905 \t Validate_Accuracy: 0.877\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.23546509444713593 \t Validate_Accuracy: 0.872\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.23330948501825333 \t Validate_Accuracy: 0.873\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.22572433203458786 \t Validate_Accuracy: 0.877\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.22476492822170258 \t Validate_Accuracy: 0.8785\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.21872282028198242 \t Validate_Accuracy: 0.8815\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.22094545513391495 \t Validate_Accuracy: 0.88\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.22689607739448547 \t Validate_Accuracy: 0.8795\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.2234276980161667 \t Validate_Accuracy: 0.8825\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.21249186992645264 \t Validate_Accuracy: 0.881\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.22218497097492218 \t Validate_Accuracy: 0.8825\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.22533155977725983 \t Validate_Accuracy: 0.882\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.22654681652784348 \t Validate_Accuracy: 0.883\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2186652272939682 \t Validate_Accuracy: 0.884\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.21898867189884186 \t Validate_Accuracy: 0.8885\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.21493405103683472 \t Validate_Accuracy: 0.884\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.21693314611911774 \t Validate_Accuracy: 0.892\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2100241556763649 \t Validate_Accuracy: 0.888\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.21482133120298386 \t Validate_Accuracy: 0.89\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.20518633723258972 \t Validate_Accuracy: 0.8845\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2139948084950447 \t Validate_Accuracy: 0.889\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2153656929731369 \t Validate_Accuracy: 0.88\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.22386375814676285 \t Validate_Accuracy: 0.8895\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.21263370662927628 \t Validate_Accuracy: 0.891\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.22619327902793884 \t Validate_Accuracy: 0.888\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.22412420809268951 \t Validate_Accuracy: 0.89\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.21070487052202225 \t Validate_Accuracy: 0.8895\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.218323215842247 \t Validate_Accuracy: 0.894\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.20610565692186356 \t Validate_Accuracy: 0.888\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.20699723064899445 \t Validate_Accuracy: 0.896\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.19991162419319153 \t Validate_Accuracy: 0.8895\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2034076377749443 \t Validate_Accuracy: 0.8915\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.20221348106861115 \t Validate_Accuracy: 0.887\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.20326650142669678 \t Validate_Accuracy: 0.8985\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.19851120561361313 \t Validate_Accuracy: 0.901\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2070493847131729 \t Validate_Accuracy: 0.8935\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.20839288085699081 \t Validate_Accuracy: 0.8945\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.19841963797807693 \t Validate_Accuracy: 0.8925\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.20798896998167038 \t Validate_Accuracy: 0.8955\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.20242881029844284 \t Validate_Accuracy: 0.894\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.19714561104774475 \t Validate_Accuracy: 0.898\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2061925008893013 \t Validate_Accuracy: 0.898\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.2010352611541748 \t Validate_Accuracy: 0.896\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2028532400727272 \t Validate_Accuracy: 0.8985\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.2044038400053978 \t Validate_Accuracy: 0.8905\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.19696836918592453 \t Validate_Accuracy: 0.8935\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.20318716764450073 \t Validate_Accuracy: 0.895\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2021828144788742 \t Validate_Accuracy: 0.895\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.1953815594315529 \t Validate_Accuracy: 0.893\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2121187224984169 \t Validate_Accuracy: 0.895\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.20271778851747513 \t Validate_Accuracy: 0.894\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.19712721556425095 \t Validate_Accuracy: 0.888\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.20317425578832626 \t Validate_Accuracy: 0.887\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.21911901235580444 \t Validate_Accuracy: 0.883\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.20570911467075348 \t Validate_Accuracy: 0.8615\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.2293284758925438 \t Validate_Accuracy: 0.8755\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.22777244448661804 \t Validate_Accuracy: 0.896\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.21814580261707306 \t Validate_Accuracy: 0.893\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.20392732322216034 \t Validate_Accuracy: 0.8945\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.20986277610063553 \t Validate_Accuracy: 0.8895\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2093954235315323 \t Validate_Accuracy: 0.8955\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.20416971296072006 \t Validate_Accuracy: 0.8885\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2021544724702835 \t Validate_Accuracy: 0.893\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.2102324739098549 \t Validate_Accuracy: 0.894\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.19185705482959747 \t Validate_Accuracy: 0.894\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.19471410661935806 \t Validate_Accuracy: 0.897\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.19379443675279617 \t Validate_Accuracy: 0.899\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.1977733001112938 \t Validate_Accuracy: 0.9005\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.19053442031145096 \t Validate_Accuracy: 0.9025\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.1976671814918518 \t Validate_Accuracy: 0.8975\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.1978081911802292 \t Validate_Accuracy: 0.9\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.18769624829292297 \t Validate_Accuracy: 0.8975\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.19321180135011673 \t Validate_Accuracy: 0.898\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.19900865852832794 \t Validate_Accuracy: 0.8995\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.20119114965200424 \t Validate_Accuracy: 0.899\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.19152314215898514 \t Validate_Accuracy: 0.8975\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.1920899674296379 \t Validate_Accuracy: 0.8965\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.19134575873613358 \t Validate_Accuracy: 0.896\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.1943541243672371 \t Validate_Accuracy: 0.894\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.2101910412311554 \t Validate_Accuracy: 0.898\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.19893833249807358 \t Validate_Accuracy: 0.8875\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.2010122835636139 \t Validate_Accuracy: 0.898\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.19743415713310242 \t Validate_Accuracy: 0.895\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.20251992344856262 \t Validate_Accuracy: 0.901\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.19257783889770508 \t Validate_Accuracy: 0.9\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.19661276787519455 \t Validate_Accuracy: 0.9005\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.1932126060128212 \t Validate_Accuracy: 0.9005\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.1936437487602234 \t Validate_Accuracy: 0.894\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.18024824559688568 \t Validate_Accuracy: 0.898\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.1978144496679306 \t Validate_Accuracy: 0.8945\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.19285111129283905 \t Validate_Accuracy: 0.897\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.19956590235233307 \t Validate_Accuracy: 0.9025\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.19825434684753418 \t Validate_Accuracy: 0.887\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.21291930973529816 \t Validate_Accuracy: 0.893\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.20168643444776535 \t Validate_Accuracy: 0.8985\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.1936117485165596 \t Validate_Accuracy: 0.9055\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.189625084400177 \t Validate_Accuracy: 0.899\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1464, -0.0449, -0.1725],\n","          [-0.0656,  0.0274, -0.0522],\n","          [-0.1246, -0.0429, -0.1589]]]])\n","conv1.bias tensor([0.0591])\n","first_linear.weight tensor([[ 0.1339,  0.2056,  0.1328,  0.2363,  0.1712,  0.2848,  0.0302,  0.1986,\n","          0.1427],\n","        [-0.1511,  0.0378, -0.1686, -0.1582, -0.2556, -0.2489,  0.0286, -0.2538,\n","         -0.2947],\n","        [ 0.1510,  0.5290, -0.3678, -1.6391,  0.3343,  0.0955,  1.1176, -0.6102,\n","         -0.0844],\n","        [ 0.4314, -0.3822,  0.3664, -0.3314,  0.3879, -0.2650, -0.8508,  1.2811,\n","         -0.6368],\n","        [ 0.4899, -1.5520,  0.5911,  0.0476,  0.0883,  0.3546,  0.4650, -0.2251,\n","         -0.3153],\n","        [ 0.7099, -0.9403,  0.4339, -0.9918,  1.1131, -0.7056,  0.0592, -0.4038,\n","          0.4893],\n","        [ 0.1616,  0.1317,  0.1771,  0.0586,  0.1028,  0.1085, -0.0566, -0.0145,\n","          0.1625],\n","        [ 0.2644, -0.1747, -0.7065,  0.2765,  0.3726,  0.5977, -0.5323, -0.3759,\n","          0.2195],\n","        [-0.3256,  0.0294, -0.5127, -0.0434,  1.0182,  0.6925,  0.3538, -0.3298,\n","         -0.7998],\n","        [ 0.8193, -0.0175, -0.3754, -0.4166, -1.1038,  1.0680, -0.0210,  0.6513,\n","         -0.5850]])\n","first_linear.bias tensor([-1.2757, -1.1863,  1.0379,  1.0939,  0.8071, -0.9497,  1.2834,  0.4911,\n","        -0.7119, -1.0358])\n","linear_hidden.0.weight tensor([[-1.1490, -0.7376, -0.6090, -0.7034, -0.4652,  0.6878,  0.7019, -0.5690,\n","          0.1597,  0.7005],\n","        [ 1.0498,  0.8597,  0.7021,  0.5120,  0.4944, -0.6937, -0.6007,  0.5062,\n","         -0.4446, -0.5513],\n","        [ 0.9837,  0.4565,  0.8330,  0.8131,  0.5565, -0.7496, -0.8694,  0.5728,\n","          0.2236, -0.4351],\n","        [-1.1375, -0.7657, -0.8131, -0.4010, -0.4433,  0.7182,  0.6434, -0.3906,\n","          0.4558,  0.5662],\n","        [ 0.8338,  0.6309,  0.5845,  0.5505,  0.5686, -0.6400, -0.7969,  0.6235,\n","         -0.4113, -0.4191],\n","        [-0.7120, -0.1122,  0.0535,  0.1955, -0.1812, -0.0137,  0.2177, -0.2221,\n","          0.0797, -0.0017],\n","        [ 0.1338,  0.6782,  0.1030, -0.1127, -0.2531, -0.0644, -0.5984, -0.2576,\n","          0.1035, -0.0159],\n","        [ 0.6915,  0.5655,  0.6405,  0.3564,  0.0724, -0.6147, -0.5829,  0.0054,\n","         -0.9228, -0.7403],\n","        [-0.5192, -0.4861, -0.3784, -0.4494, -0.3627,  0.6006,  0.4635, -0.4930,\n","          0.0988,  0.3454],\n","        [-0.7432, -0.9446, -0.2845, -0.6249, -0.6060,  0.3949,  0.6489,  0.5023,\n","          1.4571,  1.1707]])\n","linear_hidden.0.bias tensor([ 0.0092, -0.0336, -0.1930,  0.0504, -0.0594,  0.9127, -0.5156, -0.1317,\n","         0.2532, -0.2424])\n","linear_output.weight tensor([[-1.5191,  1.5330,  1.5936, -1.5608,  1.5598, -0.9017,  1.1539,  1.1370,\n","         -1.0582, -1.6470]])\n","linear_output.bias tensor([-0.0909])\n","Testing out: \n","batch_size:  1537\n","train_size:  3812\n","n_epochs:  205\n","lr:  0.034929988779108544\n","weight_decay:  0.0016588017693726527\n","betas0:  0.9901934652406246\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1021,  0.1215, -0.1524],\n","          [-0.1824,  0.1201, -0.2143],\n","          [-0.1647,  0.0672, -0.0728]]]])\n","conv1.bias tensor([-0.0778])\n","first_linear.weight tensor([[ 0.2872,  0.0499, -0.1110,  0.2520, -0.1758, -0.2513,  0.0414, -0.0681,\n","         -0.1111],\n","        [-0.3232,  0.3083,  0.2924,  0.1021,  0.1249, -0.0856,  0.0296,  0.2452,\n","          0.0736],\n","        [-0.1842, -0.1854, -0.1531, -0.0953, -0.2038,  0.2027, -0.2962,  0.1046,\n","         -0.0154],\n","        [-0.0929, -0.1943,  0.0045,  0.0845, -0.2893,  0.2971, -0.2924,  0.0217,\n","         -0.2978],\n","        [-0.1268,  0.2049,  0.0405,  0.3124, -0.1621, -0.0555, -0.1790, -0.2702,\n","          0.2093],\n","        [-0.0509, -0.2890, -0.3094, -0.2649,  0.2147,  0.2829,  0.2567, -0.2488,\n","          0.2854],\n","        [ 0.2604, -0.2644,  0.1886,  0.2472,  0.2254,  0.0040,  0.2623,  0.0046,\n","         -0.2741],\n","        [ 0.0020, -0.2635,  0.0325,  0.0553,  0.1085,  0.0098, -0.1082, -0.0138,\n","          0.1273],\n","        [ 0.1479,  0.2281,  0.2059,  0.3165, -0.1301, -0.0374,  0.0567,  0.0445,\n","         -0.0950],\n","        [ 0.3143,  0.0802, -0.0552, -0.0055,  0.0153,  0.2352,  0.0936, -0.0923,\n","          0.1513]])\n","first_linear.bias tensor([ 0.0460,  0.1209, -0.1937,  0.0311,  0.1104,  0.0537, -0.0595,  0.1634,\n","         0.2524,  0.0055])\n","linear_hidden.0.weight tensor([[ 0.2216,  0.2234, -0.1622,  0.1324, -0.1326,  0.0330, -0.2700, -0.0074,\n","          0.0189, -0.2474],\n","        [-0.2033,  0.0279,  0.3137, -0.0695, -0.1099,  0.2341, -0.1888, -0.3119,\n","         -0.2960, -0.1763],\n","        [ 0.1291,  0.1169,  0.3097, -0.1437,  0.1430,  0.1825, -0.0723, -0.0496,\n","         -0.1401,  0.1286],\n","        [-0.2318, -0.2041, -0.1184,  0.0341, -0.0844,  0.1535,  0.1642, -0.3075,\n","         -0.0587, -0.2976],\n","        [ 0.2057, -0.0906,  0.1249,  0.2661, -0.3136, -0.0792,  0.2988,  0.0164,\n","          0.1136, -0.1057],\n","        [ 0.1716, -0.0921,  0.1178, -0.0782, -0.1668, -0.0342, -0.1219,  0.2788,\n","         -0.0420,  0.1179],\n","        [-0.1773, -0.2748,  0.2996,  0.1601,  0.1009, -0.1055,  0.1264,  0.2943,\n","         -0.2088, -0.0285],\n","        [-0.3039, -0.2603, -0.1276, -0.0595, -0.0325,  0.2651, -0.1485,  0.1358,\n","          0.2888, -0.1684],\n","        [ 0.0493, -0.1837, -0.1881,  0.3009,  0.3123,  0.1941,  0.2465,  0.2738,\n","         -0.0383, -0.0540],\n","        [-0.1573,  0.1448,  0.2994,  0.1688, -0.2851, -0.1508, -0.2226, -0.1743,\n","         -0.2423,  0.2455]])\n","linear_hidden.0.bias tensor([ 0.2015,  0.1434, -0.0904, -0.3049, -0.1763,  0.1082, -0.2527, -0.2585,\n","        -0.2468, -0.0328])\n","linear_output.weight tensor([[ 0.1783,  0.0695,  0.1372, -0.1898, -0.1632, -0.2331, -0.1366, -0.1655,\n","         -0.1242, -0.1942]])\n","linear_output.bias tensor([0.0057])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1537])) that is different to the input size (torch.Size([1537, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([738])) that is different to the input size (torch.Size([738, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6963546474774679 \t Validate_Accuracy: 0.503\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6929350892702738 \t Validate_Accuracy: 0.467\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6932532787322998 \t Validate_Accuracy: 0.5045\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6921535134315491 \t Validate_Accuracy: 0.4635\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6923534274101257 \t Validate_Accuracy: 0.484\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6921993096669515 \t Validate_Accuracy: 0.4995\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6922433177630106 \t Validate_Accuracy: 0.514\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6916840473810831 \t Validate_Accuracy: 0.4745\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6913769443829855 \t Validate_Accuracy: 0.5155\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6912450393040975 \t Validate_Accuracy: 0.567\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6907891432444254 \t Validate_Accuracy: 0.5275\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6893025437990824 \t Validate_Accuracy: 0.514\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6830681165059408 \t Validate_Accuracy: 0.5345\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6662119825681051 \t Validate_Accuracy: 0.651\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6187992493311564 \t Validate_Accuracy: 0.798\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.5291485687096914 \t Validate_Accuracy: 0.8135\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.45725707213083905 \t Validate_Accuracy: 0.791\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.43113675713539124 \t Validate_Accuracy: 0.822\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.4272535840670268 \t Validate_Accuracy: 0.817\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.4196786880493164 \t Validate_Accuracy: 0.816\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.39752750595410663 \t Validate_Accuracy: 0.8315\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3867190182209015 \t Validate_Accuracy: 0.829\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3796372612317403 \t Validate_Accuracy: 0.8305\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.36623021960258484 \t Validate_Accuracy: 0.828\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3573462466398875 \t Validate_Accuracy: 0.8305\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3636816640694936 \t Validate_Accuracy: 0.83\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3528307278951009 \t Validate_Accuracy: 0.845\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3501932621002197 \t Validate_Accuracy: 0.8345\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.3398958047231038 \t Validate_Accuracy: 0.844\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3379395504792531 \t Validate_Accuracy: 0.8365\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.33482204874356586 \t Validate_Accuracy: 0.843\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.32365699609120685 \t Validate_Accuracy: 0.8525\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3105195661385854 \t Validate_Accuracy: 0.8555\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.302437961101532 \t Validate_Accuracy: 0.8455\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.29513498147328693 \t Validate_Accuracy: 0.8625\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3004721403121948 \t Validate_Accuracy: 0.863\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.29481734832127887 \t Validate_Accuracy: 0.846\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.2954024374485016 \t Validate_Accuracy: 0.8515\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.30153919259707135 \t Validate_Accuracy: 0.865\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2925929129123688 \t Validate_Accuracy: 0.8635\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2881839871406555 \t Validate_Accuracy: 0.8645\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.28493009010950726 \t Validate_Accuracy: 0.8575\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2792544464270274 \t Validate_Accuracy: 0.866\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.27594565351804096 \t Validate_Accuracy: 0.858\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2782696485519409 \t Validate_Accuracy: 0.8615\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2792717119057973 \t Validate_Accuracy: 0.8635\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2773100733757019 \t Validate_Accuracy: 0.8635\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.2759907742341359 \t Validate_Accuracy: 0.865\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.2657669981320699 \t Validate_Accuracy: 0.865\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.2613152315219243 \t Validate_Accuracy: 0.865\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2679634839296341 \t Validate_Accuracy: 0.867\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.26039475699265796 \t Validate_Accuracy: 0.8625\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.25969942410786945 \t Validate_Accuracy: 0.8695\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.24785013993581137 \t Validate_Accuracy: 0.866\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2527971665064494 \t Validate_Accuracy: 0.863\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.2438171406586965 \t Validate_Accuracy: 0.8755\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2506195654471715 \t Validate_Accuracy: 0.869\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.24365111192067465 \t Validate_Accuracy: 0.8785\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.2331215093533198 \t Validate_Accuracy: 0.8685\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.240402489900589 \t Validate_Accuracy: 0.874\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.23935895164807638 \t Validate_Accuracy: 0.8795\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.22731794913609824 \t Validate_Accuracy: 0.88\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.22782071431477866 \t Validate_Accuracy: 0.887\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.22831826905409494 \t Validate_Accuracy: 0.8785\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.22939852376778921 \t Validate_Accuracy: 0.882\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.22991190354029337 \t Validate_Accuracy: 0.8825\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.23202891647815704 \t Validate_Accuracy: 0.878\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.23724579314390817 \t Validate_Accuracy: 0.8755\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2365204393863678 \t Validate_Accuracy: 0.887\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2308227668205897 \t Validate_Accuracy: 0.882\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.23215031623840332 \t Validate_Accuracy: 0.885\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.22305400172869363 \t Validate_Accuracy: 0.885\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.223121111591657 \t Validate_Accuracy: 0.886\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.232904722293218 \t Validate_Accuracy: 0.884\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2261810004711151 \t Validate_Accuracy: 0.882\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.23688332239786783 \t Validate_Accuracy: 0.8835\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.2310215731461843 \t Validate_Accuracy: 0.8835\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.22684667011102042 \t Validate_Accuracy: 0.883\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.23176521559556326 \t Validate_Accuracy: 0.875\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2304010589917501 \t Validate_Accuracy: 0.8835\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.23393769562244415 \t Validate_Accuracy: 0.884\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.23089653253555298 \t Validate_Accuracy: 0.887\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.22595440844694772 \t Validate_Accuracy: 0.885\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.22417018314202627 \t Validate_Accuracy: 0.882\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.22720294197400412 \t Validate_Accuracy: 0.8835\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.22346142927805582 \t Validate_Accuracy: 0.889\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.21966401735941568 \t Validate_Accuracy: 0.8905\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.21762177348136902 \t Validate_Accuracy: 0.8855\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2215010126431783 \t Validate_Accuracy: 0.8905\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.21982255578041077 \t Validate_Accuracy: 0.887\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.22020449241002402 \t Validate_Accuracy: 0.8805\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2238504687945048 \t Validate_Accuracy: 0.8725\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.23636257648468018 \t Validate_Accuracy: 0.88\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.23364736636479697 \t Validate_Accuracy: 0.877\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.22899022698402405 \t Validate_Accuracy: 0.882\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.2197279085715612 \t Validate_Accuracy: 0.889\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.22261285781860352 \t Validate_Accuracy: 0.889\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.22105997304121652 \t Validate_Accuracy: 0.89\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.21994486451148987 \t Validate_Accuracy: 0.889\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.21482991178830466 \t Validate_Accuracy: 0.8855\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2161228209733963 \t Validate_Accuracy: 0.8865\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2236240953207016 \t Validate_Accuracy: 0.8875\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.22736692428588867 \t Validate_Accuracy: 0.8835\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.21850218872229257 \t Validate_Accuracy: 0.8875\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.21765220165252686 \t Validate_Accuracy: 0.8895\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2168989529212316 \t Validate_Accuracy: 0.8855\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.21675077080726624 \t Validate_Accuracy: 0.887\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2206749568382899 \t Validate_Accuracy: 0.89\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.21865061422189078 \t Validate_Accuracy: 0.8875\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.21790074308713278 \t Validate_Accuracy: 0.8895\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.21524474521478018 \t Validate_Accuracy: 0.8855\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.2152791072924932 \t Validate_Accuracy: 0.89\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.21627351144949594 \t Validate_Accuracy: 0.8845\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.21578250328699747 \t Validate_Accuracy: 0.8905\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.21535131831963858 \t Validate_Accuracy: 0.892\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.21081088483333588 \t Validate_Accuracy: 0.89\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.211105744043986 \t Validate_Accuracy: 0.884\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.211167444785436 \t Validate_Accuracy: 0.8815\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.22496108214060465 \t Validate_Accuracy: 0.8805\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.22089892625808716 \t Validate_Accuracy: 0.8845\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.22342426578203836 \t Validate_Accuracy: 0.8895\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21940398712952933 \t Validate_Accuracy: 0.8885\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.21674756705760956 \t Validate_Accuracy: 0.887\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.21620655059814453 \t Validate_Accuracy: 0.89\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.21192283431688944 \t Validate_Accuracy: 0.8915\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2170624484618505 \t Validate_Accuracy: 0.8875\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2223226527372996 \t Validate_Accuracy: 0.8875\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.21339207390944162 \t Validate_Accuracy: 0.8845\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.21587700645128885 \t Validate_Accuracy: 0.8815\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.21263232827186584 \t Validate_Accuracy: 0.8855\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.21753311653931937 \t Validate_Accuracy: 0.8845\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.2137763351202011 \t Validate_Accuracy: 0.888\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.22334501643975577 \t Validate_Accuracy: 0.888\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.22224964698155722 \t Validate_Accuracy: 0.882\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.21960807343324026 \t Validate_Accuracy: 0.879\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2233222872018814 \t Validate_Accuracy: 0.874\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.23273688554763794 \t Validate_Accuracy: 0.874\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.23065702617168427 \t Validate_Accuracy: 0.876\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.2299681156873703 \t Validate_Accuracy: 0.877\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.23146726191043854 \t Validate_Accuracy: 0.8775\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2263014813264211 \t Validate_Accuracy: 0.88\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.23022348682085672 \t Validate_Accuracy: 0.8785\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.22947552303473154 \t Validate_Accuracy: 0.8885\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.2211571385463079 \t Validate_Accuracy: 0.886\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2192580004533132 \t Validate_Accuracy: 0.8865\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.2219036469856898 \t Validate_Accuracy: 0.8905\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.21557917694250742 \t Validate_Accuracy: 0.8845\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.21660016477108002 \t Validate_Accuracy: 0.888\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.21185117463270822 \t Validate_Accuracy: 0.888\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.21276388068993887 \t Validate_Accuracy: 0.8895\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.21340289215246835 \t Validate_Accuracy: 0.8885\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.21461326877276102 \t Validate_Accuracy: 0.888\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2107225557168325 \t Validate_Accuracy: 0.891\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.20661008854707083 \t Validate_Accuracy: 0.892\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.21535629034042358 \t Validate_Accuracy: 0.8865\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.20916071037451425 \t Validate_Accuracy: 0.8825\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.2192446937163671 \t Validate_Accuracy: 0.8855\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.21834926307201385 \t Validate_Accuracy: 0.8885\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.2226948936780294 \t Validate_Accuracy: 0.892\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.21297953029473624 \t Validate_Accuracy: 0.885\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2145549257596334 \t Validate_Accuracy: 0.889\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.22159950931866965 \t Validate_Accuracy: 0.8825\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.21862539649009705 \t Validate_Accuracy: 0.889\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.21784408390522003 \t Validate_Accuracy: 0.8885\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.21362090110778809 \t Validate_Accuracy: 0.8925\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.22209941844145456 \t Validate_Accuracy: 0.8925\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.21455089251200357 \t Validate_Accuracy: 0.882\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.22366511821746826 \t Validate_Accuracy: 0.885\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.21826757490634918 \t Validate_Accuracy: 0.8815\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.22086062530676523 \t Validate_Accuracy: 0.8825\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.22113162775834402 \t Validate_Accuracy: 0.883\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.22073994080225626 \t Validate_Accuracy: 0.887\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.21133904655774435 \t Validate_Accuracy: 0.881\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.21411235630512238 \t Validate_Accuracy: 0.8865\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.21996046602725983 \t Validate_Accuracy: 0.8825\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.21789988378683725 \t Validate_Accuracy: 0.8885\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.21504799028237662 \t Validate_Accuracy: 0.8875\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.21801837285359701 \t Validate_Accuracy: 0.889\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.21270500123500824 \t Validate_Accuracy: 0.89\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.21272799372673035 \t Validate_Accuracy: 0.886\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.21326151490211487 \t Validate_Accuracy: 0.888\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.2102509786685308 \t Validate_Accuracy: 0.884\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.21140268445014954 \t Validate_Accuracy: 0.8815\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.21733801563580832 \t Validate_Accuracy: 0.882\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.21384688715140024 \t Validate_Accuracy: 0.883\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.21603820224603018 \t Validate_Accuracy: 0.89\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.21910980840524039 \t Validate_Accuracy: 0.8895\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.2231649806102117 \t Validate_Accuracy: 0.884\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.21993902325630188 \t Validate_Accuracy: 0.888\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.2139797806739807 \t Validate_Accuracy: 0.8835\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.2150596926609675 \t Validate_Accuracy: 0.889\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.2197708785533905 \t Validate_Accuracy: 0.885\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.21812586983044943 \t Validate_Accuracy: 0.8885\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.21303944289684296 \t Validate_Accuracy: 0.886\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.21940860152244568 \t Validate_Accuracy: 0.89\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.21288850903511047 \t Validate_Accuracy: 0.8825\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.21681990722815195 \t Validate_Accuracy: 0.888\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.217339426279068 \t Validate_Accuracy: 0.886\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.21782111624876657 \t Validate_Accuracy: 0.887\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.21291224161783853 \t Validate_Accuracy: 0.8855\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.21237970888614655 \t Validate_Accuracy: 0.8915\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.2140580415725708 \t Validate_Accuracy: 0.8895\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.21894014875094095 \t Validate_Accuracy: 0.89\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.2088723431030909 \t Validate_Accuracy: 0.8875\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.21306673189004263 \t Validate_Accuracy: 0.8885\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1953, -0.0640, -0.2267],\n","          [-0.0820, -0.0034, -0.0529],\n","          [-0.1899, -0.0832, -0.1842]]]])\n","conv1.bias tensor([0.0381])\n","first_linear.weight tensor([[-2.1072e-01, -1.4006e-01, -3.6229e-01, -1.4035e-01, -5.2780e-02,\n","         -3.4659e-01,  3.9998e-01,  4.7961e-01,  1.2451e-01],\n","        [ 3.2063e-01, -1.3126e+00,  2.9643e-01,  1.8487e-01,  2.6905e-02,\n","          3.1456e-01, -2.7457e-02,  5.3035e-02, -3.8419e-01],\n","        [-1.3195e-01,  3.6340e-02,  4.3372e-02, -7.9853e-02,  6.2466e-02,\n","         -3.2515e-04,  5.6078e-02, -6.6950e-02,  1.0836e-01],\n","        [ 4.2976e-02, -1.6654e-01, -2.7522e-01, -4.0202e-01,  5.7121e-01,\n","          4.6098e-01,  8.1537e-01, -1.3587e+00,  3.0504e-01],\n","        [ 4.5415e-01,  1.7541e-01, -3.2377e-01,  4.2635e-02, -6.6103e-01,\n","          1.1577e+00, -1.7655e-01,  6.3399e-01, -6.6608e-01],\n","        [ 6.9595e-01, -9.1405e-01,  5.3301e-01, -5.1964e-01,  6.7162e-01,\n","         -6.2455e-01, -2.2217e-01,  1.2545e-01,  1.6228e-01],\n","        [-4.4120e-01,  1.2807e-01,  2.0049e-01, -4.7167e-01, -3.5031e-01,\n","         -4.8211e-01,  1.2067e-01,  2.4307e-01, -5.5336e-01],\n","        [ 1.1858e-01,  1.7162e-01,  2.3187e-01,  8.7337e-02,  2.4688e-01,\n","          1.8887e-01,  3.9943e-02,  1.2008e-01,  2.7946e-01],\n","        [-2.0385e-01, -2.8090e-01, -1.3823e-01, -1.7425e-01, -2.4058e-01,\n","         -2.5887e-01, -1.4993e-01, -1.8853e-01, -2.3776e-01],\n","        [-4.4947e-01, -1.5046e-01,  2.2788e-01,  1.6072e+00, -3.5201e-01,\n","         -7.1468e-02, -6.5623e-01,  2.8333e-02,  2.3454e-01]])\n","first_linear.bias tensor([ 3.8516e-01,  8.2322e-01,  2.8806e-02, -1.0489e+00, -9.4480e-01,\n","        -9.1663e-01, -8.0772e-04,  1.0271e+00,  1.2263e+00, -1.0487e+00])\n","linear_hidden.0.weight tensor([[-0.1909,  0.3497,  0.0391, -0.4457, -0.4566, -0.4737, -0.2209, -0.5579,\n","         -0.5073, -0.4710],\n","        [-0.1762,  0.3199,  0.0405, -0.4090, -0.4251, -0.4379, -0.2031, -0.5080,\n","         -0.4603, -0.4372],\n","        [-0.1958,  0.3562,  0.0386, -0.4556, -0.4640, -0.4819, -0.2273, -0.5791,\n","         -0.5177, -0.4797],\n","        [ 0.1909, -0.3512, -0.0393,  0.4472,  0.4583,  0.4757,  0.2210,  0.5559,\n","          0.5066,  0.4716],\n","        [ 0.1650, -0.3000, -0.0418,  0.3837,  0.4035,  0.4140,  0.1897,  0.4675,\n","          0.4262,  0.4133],\n","        [ 0.2010, -0.3658, -0.0378,  0.4680,  0.4741,  0.4935,  0.2336,  0.5978,\n","          0.5320,  0.4904],\n","        [ 0.1469, -0.2676, -0.0433,  0.3427,  0.3671,  0.3746,  0.1675,  0.4088,\n","          0.3777,  0.3729],\n","        [ 0.1822, -0.3325, -0.0402,  0.4248,  0.4388,  0.4535,  0.2108,  0.5267,\n","          0.4762,  0.4513],\n","        [ 0.1546, -0.2809, -0.0427,  0.3595,  0.3823,  0.3904,  0.1769,  0.4332,\n","          0.3982,  0.3901],\n","        [-0.1558,  0.2835,  0.0425, -0.3627, -0.3852, -0.3938, -0.1784, -0.4363,\n","         -0.4011, -0.3927]])\n","linear_hidden.0.bias tensor([-0.4594, -0.4412, -0.4580,  0.4652,  0.4350,  0.4627,  0.4125,  0.4541,\n","         0.4216, -0.4251])\n","linear_output.weight tensor([[ 1.2943,  1.1927,  1.3246, -1.2961, -1.1184, -1.3603, -0.9944, -1.2358,\n","         -1.0492,  1.0597]])\n","linear_output.bias tensor([-0.4475])\n","Testing out: \n","batch_size:  1566\n","train_size:  2363\n","n_epochs:  187\n","lr:  0.01109367663142496\n","weight_decay:  0.00013182433151259738\n","betas0:  0.9861572480840174\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.0941,  0.0677, -0.1345],\n","          [ 0.1338, -0.1964, -0.2747],\n","          [-0.0359, -0.1474,  0.2719]]]])\n","conv1.bias tensor([-0.0954])\n","first_linear.weight tensor([[ 0.1238,  0.1057, -0.2604,  0.1701,  0.0531, -0.2732,  0.2399,  0.2850,\n","          0.1055],\n","        [ 0.0068,  0.2057,  0.3234, -0.2785,  0.3207,  0.2293,  0.1411,  0.1591,\n","          0.2378],\n","        [-0.1280,  0.2808, -0.0959, -0.2424,  0.0921, -0.1755,  0.0149,  0.2594,\n","         -0.3148],\n","        [ 0.0987, -0.1106, -0.0833,  0.0395,  0.1929,  0.0152,  0.1649,  0.0798,\n","         -0.1497],\n","        [ 0.2968, -0.3190, -0.0119,  0.0796, -0.0182,  0.3090, -0.2013,  0.2602,\n","          0.0365],\n","        [ 0.1320,  0.2050, -0.0999,  0.1058,  0.0489,  0.3175,  0.3195,  0.1870,\n","         -0.3309],\n","        [ 0.0762,  0.0129,  0.1651,  0.0770,  0.1726,  0.1516, -0.1766,  0.0284,\n","          0.1009],\n","        [ 0.2922, -0.0274,  0.0047,  0.2143, -0.2758,  0.0556, -0.0963, -0.1326,\n","          0.1120],\n","        [-0.1728,  0.3147,  0.2512,  0.2251,  0.3095,  0.2348, -0.0086,  0.1867,\n","         -0.2959],\n","        [ 0.2888,  0.1298, -0.2549,  0.2663,  0.2524, -0.0796,  0.2052,  0.1540,\n","         -0.3120]])\n","first_linear.bias tensor([-0.2619,  0.1161,  0.0405, -0.1565,  0.3232,  0.1664,  0.3249, -0.0150,\n","        -0.0676,  0.1362])\n","linear_hidden.0.weight tensor([[ 1.5024e-03, -1.8864e-01,  1.4282e-01, -2.7200e-01, -1.3217e-01,\n","         -2.5452e-01,  3.4634e-02,  2.5188e-01, -2.7603e-01, -5.9418e-02],\n","        [ 3.1662e-03, -9.2233e-02,  3.0938e-01,  2.3239e-01, -2.5697e-01,\n","          4.0277e-02, -1.1561e-01,  2.7702e-01, -7.1466e-02,  2.4775e-02],\n","        [ 2.2989e-02,  1.3446e-02, -1.6549e-01,  7.7714e-02,  1.8831e-01,\n","          1.1969e-02,  2.1029e-01, -8.7225e-02,  2.8461e-01,  3.1244e-01],\n","        [-8.8919e-02, -2.2864e-01,  9.4429e-02, -1.0862e-01, -1.8586e-01,\n","         -1.4885e-02,  5.5368e-02,  2.8165e-01,  6.9578e-03, -1.9528e-02],\n","        [-1.1695e-01,  2.6144e-01,  1.8445e-01,  1.4554e-01, -1.2839e-01,\n","         -2.2047e-01, -2.4026e-01,  3.0987e-01,  1.6865e-04, -1.1742e-01],\n","        [-2.0521e-02, -1.6314e-01,  2.4178e-01, -6.7033e-03, -3.1286e-02,\n","          2.2929e-01, -1.3244e-01, -2.1017e-01,  4.1921e-02, -1.2470e-01],\n","        [ 1.3570e-01,  1.3611e-01, -2.0831e-01, -2.9469e-01, -9.1167e-02,\n","         -3.1955e-02, -2.4318e-01, -5.4044e-02,  7.0480e-02,  2.9366e-01],\n","        [-2.2418e-01, -1.8886e-01,  1.7090e-01,  3.1591e-01,  1.5941e-03,\n","         -1.6755e-01, -4.0893e-02,  6.0098e-02,  1.3988e-01, -2.7962e-01],\n","        [-8.6235e-02, -2.4425e-01,  2.0662e-03,  3.0334e-01, -2.7745e-01,\n","          3.0424e-01, -2.2090e-03, -7.9094e-02,  3.2885e-02, -2.8956e-01],\n","        [-1.8542e-01, -2.3445e-01, -2.6131e-01, -2.0230e-01, -2.5960e-01,\n","          2.7556e-01,  1.7306e-01, -2.6043e-01, -6.0436e-02, -5.3617e-02]])\n","linear_hidden.0.bias tensor([-0.2024, -0.0381,  0.2062,  0.2480, -0.0156, -0.3134, -0.0430, -0.1667,\n","        -0.1591, -0.2369])\n","linear_output.weight tensor([[ 0.0020,  0.0929,  0.1232, -0.0242,  0.1390, -0.1500,  0.1760,  0.1744,\n","          0.3042,  0.1138]])\n","linear_output.bias tensor([-0.0002])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6936900019645691 \t Validate_Accuracy: 0.5035\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1566])) that is different to the input size (torch.Size([1566, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([797])) that is different to the input size (torch.Size([797, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 2\n","Epoch: 2 \t Train Loss: 0.6931565403938293 \t Validate_Accuracy: 0.511\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6927517056465149 \t Validate_Accuracy: 0.5345\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6926664710044861 \t Validate_Accuracy: 0.549\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6910472214221954 \t Validate_Accuracy: 0.5505\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6892112791538239 \t Validate_Accuracy: 0.5785\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.686807781457901 \t Validate_Accuracy: 0.5845\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6820991337299347 \t Validate_Accuracy: 0.5855\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6761163771152496 \t Validate_Accuracy: 0.59\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6683769226074219 \t Validate_Accuracy: 0.608\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6583583950996399 \t Validate_Accuracy: 0.621\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6472035050392151 \t Validate_Accuracy: 0.6335\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6317122876644135 \t Validate_Accuracy: 0.6525\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6168273389339447 \t Validate_Accuracy: 0.765\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.598149985074997 \t Validate_Accuracy: 0.7985\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.579963743686676 \t Validate_Accuracy: 0.81\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.5594062507152557 \t Validate_Accuracy: 0.8155\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.5380204617977142 \t Validate_Accuracy: 0.816\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.5219099223613739 \t Validate_Accuracy: 0.822\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.5032009184360504 \t Validate_Accuracy: 0.824\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.4845671206712723 \t Validate_Accuracy: 0.824\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.47372613847255707 \t Validate_Accuracy: 0.823\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.45744381844997406 \t Validate_Accuracy: 0.8245\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.4478263407945633 \t Validate_Accuracy: 0.8215\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.45017628371715546 \t Validate_Accuracy: 0.8245\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.4231636971235275 \t Validate_Accuracy: 0.829\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.4293944090604782 \t Validate_Accuracy: 0.8285\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.4112723618745804 \t Validate_Accuracy: 0.83\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.40304572880268097 \t Validate_Accuracy: 0.8265\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3961174786090851 \t Validate_Accuracy: 0.8275\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.386675089597702 \t Validate_Accuracy: 0.827\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.3866664320230484 \t Validate_Accuracy: 0.826\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3761952072381973 \t Validate_Accuracy: 0.829\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3698420822620392 \t Validate_Accuracy: 0.83\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.36409035325050354 \t Validate_Accuracy: 0.834\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3544127494096756 \t Validate_Accuracy: 0.837\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.34420233964920044 \t Validate_Accuracy: 0.842\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3466317653656006 \t Validate_Accuracy: 0.8465\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.336944580078125 \t Validate_Accuracy: 0.846\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.3359770178794861 \t Validate_Accuracy: 0.8505\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.3251803517341614 \t Validate_Accuracy: 0.849\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.3201643079519272 \t Validate_Accuracy: 0.8515\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.31752532720565796 \t Validate_Accuracy: 0.848\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.31559187173843384 \t Validate_Accuracy: 0.848\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.3000984787940979 \t Validate_Accuracy: 0.8545\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.30405324697494507 \t Validate_Accuracy: 0.8525\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.30156664550304413 \t Validate_Accuracy: 0.852\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.30330541729927063 \t Validate_Accuracy: 0.862\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.29671043157577515 \t Validate_Accuracy: 0.8665\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.2882058173418045 \t Validate_Accuracy: 0.865\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2920004278421402 \t Validate_Accuracy: 0.8625\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.28709541261196136 \t Validate_Accuracy: 0.8625\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.28707487881183624 \t Validate_Accuracy: 0.8665\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2812442481517792 \t Validate_Accuracy: 0.8655\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.28349266946315765 \t Validate_Accuracy: 0.8635\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.28134000301361084 \t Validate_Accuracy: 0.867\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.27320869266986847 \t Validate_Accuracy: 0.867\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2869066596031189 \t Validate_Accuracy: 0.866\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.28921087086200714 \t Validate_Accuracy: 0.8675\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.28174085915088654 \t Validate_Accuracy: 0.867\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.27326056361198425 \t Validate_Accuracy: 0.867\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.28334634006023407 \t Validate_Accuracy: 0.8675\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.2790134996175766 \t Validate_Accuracy: 0.865\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2768714427947998 \t Validate_Accuracy: 0.8655\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.278695210814476 \t Validate_Accuracy: 0.867\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.28018584847450256 \t Validate_Accuracy: 0.8635\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.27179187536239624 \t Validate_Accuracy: 0.8625\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.2773941606283188 \t Validate_Accuracy: 0.862\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.2773810476064682 \t Validate_Accuracy: 0.8625\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.26974399387836456 \t Validate_Accuracy: 0.864\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.27734246850013733 \t Validate_Accuracy: 0.864\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.27824148535728455 \t Validate_Accuracy: 0.8665\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.27236080169677734 \t Validate_Accuracy: 0.865\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.27138859033584595 \t Validate_Accuracy: 0.867\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.26589086651802063 \t Validate_Accuracy: 0.866\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.26845191419124603 \t Validate_Accuracy: 0.868\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.27016906440258026 \t Validate_Accuracy: 0.867\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.26986347138881683 \t Validate_Accuracy: 0.868\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2671525776386261 \t Validate_Accuracy: 0.8705\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2730795592069626 \t Validate_Accuracy: 0.8655\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.26699861884117126 \t Validate_Accuracy: 0.865\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.27252480387687683 \t Validate_Accuracy: 0.863\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.265714555978775 \t Validate_Accuracy: 0.8625\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.27749529480934143 \t Validate_Accuracy: 0.867\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.26246029883623123 \t Validate_Accuracy: 0.8655\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.27094148099422455 \t Validate_Accuracy: 0.8625\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2702108770608902 \t Validate_Accuracy: 0.86\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.26365064084529877 \t Validate_Accuracy: 0.8665\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2717286944389343 \t Validate_Accuracy: 0.868\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.262015201151371 \t Validate_Accuracy: 0.8625\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2666594088077545 \t Validate_Accuracy: 0.866\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.26789626479148865 \t Validate_Accuracy: 0.8715\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.267158180475235 \t Validate_Accuracy: 0.8665\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2654224932193756 \t Validate_Accuracy: 0.8635\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.2699689716100693 \t Validate_Accuracy: 0.864\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.26477059721946716 \t Validate_Accuracy: 0.869\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.25741297006607056 \t Validate_Accuracy: 0.8665\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2662700265645981 \t Validate_Accuracy: 0.865\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.26225075125694275 \t Validate_Accuracy: 0.864\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.26348380744457245 \t Validate_Accuracy: 0.863\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2654678672552109 \t Validate_Accuracy: 0.865\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2631940096616745 \t Validate_Accuracy: 0.8665\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.2582716941833496 \t Validate_Accuracy: 0.865\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2589331418275833 \t Validate_Accuracy: 0.8655\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2600439041852951 \t Validate_Accuracy: 0.87\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.26254647970199585 \t Validate_Accuracy: 0.8695\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.26243123412132263 \t Validate_Accuracy: 0.8625\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2567892447113991 \t Validate_Accuracy: 0.865\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.265361450612545 \t Validate_Accuracy: 0.871\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.25483161211013794 \t Validate_Accuracy: 0.868\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.25606001913547516 \t Validate_Accuracy: 0.866\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.25328878313302994 \t Validate_Accuracy: 0.8705\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.25875765085220337 \t Validate_Accuracy: 0.8695\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2580641806125641 \t Validate_Accuracy: 0.864\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2550751566886902 \t Validate_Accuracy: 0.864\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.26368532329797745 \t Validate_Accuracy: 0.8755\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.25712238997220993 \t Validate_Accuracy: 0.8705\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.24971526861190796 \t Validate_Accuracy: 0.862\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.2578156962990761 \t Validate_Accuracy: 0.8665\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.25532980263233185 \t Validate_Accuracy: 0.8705\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.25922127068042755 \t Validate_Accuracy: 0.8755\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.2604844719171524 \t Validate_Accuracy: 0.8685\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.25245383381843567 \t Validate_Accuracy: 0.867\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.24984422326087952 \t Validate_Accuracy: 0.871\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.2479982003569603 \t Validate_Accuracy: 0.8725\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.24720291048288345 \t Validate_Accuracy: 0.873\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2448507696390152 \t Validate_Accuracy: 0.8695\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.2547430247068405 \t Validate_Accuracy: 0.873\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.25312068313360214 \t Validate_Accuracy: 0.873\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.2458023875951767 \t Validate_Accuracy: 0.8705\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.2549550235271454 \t Validate_Accuracy: 0.871\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.24818506836891174 \t Validate_Accuracy: 0.8725\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.24953379482030869 \t Validate_Accuracy: 0.8705\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.2472679689526558 \t Validate_Accuracy: 0.872\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.24432562291622162 \t Validate_Accuracy: 0.87\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.24097470194101334 \t Validate_Accuracy: 0.869\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.24767669290304184 \t Validate_Accuracy: 0.869\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.24352512508630753 \t Validate_Accuracy: 0.8765\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.24515629559755325 \t Validate_Accuracy: 0.8765\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.24236708134412766 \t Validate_Accuracy: 0.875\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.24075472354888916 \t Validate_Accuracy: 0.8695\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.24470041692256927 \t Validate_Accuracy: 0.868\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.24602121114730835 \t Validate_Accuracy: 0.873\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.24114572256803513 \t Validate_Accuracy: 0.8745\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.24161327630281448 \t Validate_Accuracy: 0.872\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.2447797805070877 \t Validate_Accuracy: 0.866\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.24336065351963043 \t Validate_Accuracy: 0.8675\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.2326412796974182 \t Validate_Accuracy: 0.8735\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.2392939329147339 \t Validate_Accuracy: 0.8715\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.24259715527296066 \t Validate_Accuracy: 0.861\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.24257443845272064 \t Validate_Accuracy: 0.8675\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.23556272685527802 \t Validate_Accuracy: 0.868\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.24160438030958176 \t Validate_Accuracy: 0.8675\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.24632681906223297 \t Validate_Accuracy: 0.8735\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.24335354566574097 \t Validate_Accuracy: 0.8685\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.2360961139202118 \t Validate_Accuracy: 0.8705\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.23773037642240524 \t Validate_Accuracy: 0.8695\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.24073994159698486 \t Validate_Accuracy: 0.8675\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.24397973716259003 \t Validate_Accuracy: 0.871\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.23669423907995224 \t Validate_Accuracy: 0.8715\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.23644152283668518 \t Validate_Accuracy: 0.8645\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.23603401333093643 \t Validate_Accuracy: 0.8635\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.23843208700418472 \t Validate_Accuracy: 0.8655\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.23615902662277222 \t Validate_Accuracy: 0.864\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.24237962812185287 \t Validate_Accuracy: 0.867\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.22800154983997345 \t Validate_Accuracy: 0.8695\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.23498643189668655 \t Validate_Accuracy: 0.8655\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.23705464601516724 \t Validate_Accuracy: 0.863\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.24614280462265015 \t Validate_Accuracy: 0.8685\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.2379465028643608 \t Validate_Accuracy: 0.8665\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.23833630234003067 \t Validate_Accuracy: 0.8635\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.23137277364730835 \t Validate_Accuracy: 0.865\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.23482829332351685 \t Validate_Accuracy: 0.8715\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.2341613620519638 \t Validate_Accuracy: 0.8625\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.2365545779466629 \t Validate_Accuracy: 0.867\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2436646968126297 \t Validate_Accuracy: 0.8655\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.23199056088924408 \t Validate_Accuracy: 0.8665\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.23609518259763718 \t Validate_Accuracy: 0.8685\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.23328307271003723 \t Validate_Accuracy: 0.869\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.23754897713661194 \t Validate_Accuracy: 0.869\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.2345958799123764 \t Validate_Accuracy: 0.8705\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.23298264294862747 \t Validate_Accuracy: 0.869\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.23363518714904785 \t Validate_Accuracy: 0.869\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.23419180512428284 \t Validate_Accuracy: 0.8655\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.23102542012929916 \t Validate_Accuracy: 0.864\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.23333793133497238 \t Validate_Accuracy: 0.871\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.2215048149228096 \t Validate_Accuracy: 0.867\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1705, -0.0761, -0.2038],\n","          [-0.0970,  0.0100, -0.0702],\n","          [-0.1371, -0.0874, -0.1726]]]])\n","conv1.bias tensor([0.0551])\n","first_linear.weight tensor([[ 2.7841e-01,  4.2552e-01, -3.9568e-01,  1.7540e-01,  3.0195e-02,\n","         -1.6808e-02,  3.4577e-01, -6.4022e-01, -1.0312e-01],\n","        [ 4.1171e-01,  1.5526e-01,  2.4215e-01, -3.4325e-01,  2.1545e-01,\n","          3.4387e-01, -4.5276e-02,  3.4655e-01,  5.0794e-02],\n","        [ 8.2904e-01, -6.4374e-02, -2.4701e-01, -1.2888e+00,  8.7290e-02,\n","          6.3810e-01,  3.2666e-01,  7.3261e-01, -6.8531e-01],\n","        [-3.3409e-01, -3.4230e-03, -3.1044e-01,  1.3551e-01, -2.7959e-01,\n","          6.8498e-02,  1.3261e-02, -2.5409e-01, -1.6662e-01],\n","        [ 1.4741e-01,  1.6284e-04,  1.5316e-01, -4.7281e-02,  1.2591e-01,\n","          4.0210e-02, -1.9088e-02,  2.5205e-01,  5.4604e-02],\n","        [-4.0785e-01,  6.8720e-01, -9.1575e-01,  6.1001e-01, -7.7476e-01,\n","          1.1112e+00,  3.8015e-02, -1.6506e-01, -5.1516e-01],\n","        [ 2.6978e-02, -2.0212e-01, -6.5807e-02, -3.5518e-01, -3.4573e-01,\n","         -2.4786e-01, -3.8286e-01, -2.7951e-01, -2.8933e-01],\n","        [-2.0324e-02, -4.4291e-03,  2.8973e-01,  8.6274e-01, -7.6304e-01,\n","          1.4981e-02, -9.0983e-01,  1.4888e+00, -4.0115e-01],\n","        [-7.2863e-03,  4.2272e-01,  2.3171e-01,  1.2549e-01,  1.9262e-01,\n","          2.5637e-01,  1.1545e-02, -2.3766e-01,  8.6131e-02],\n","        [ 3.8417e-01,  2.0834e-01,  1.9114e-01,  4.3391e-01, -1.1124e-01,\n","         -1.4730e-01, -1.3989e-01,  6.7212e-01, -4.3968e-01]])\n","first_linear.bias tensor([-0.4803,  0.4682,  1.0719, -0.5261,  0.8739, -1.3216,  1.1476,  1.2431,\n","        -0.7915,  0.2507])\n","linear_hidden.0.weight tensor([[-0.3916,  0.4081, -0.6600, -0.4399,  0.5530,  0.7642,  0.4979, -0.8527,\n","         -0.9767,  0.5663],\n","        [ 0.5316, -0.4612,  0.9223,  0.3468, -0.7430, -0.9243, -0.6807,  1.0980,\n","          0.7395, -0.5289],\n","        [ 0.3953, -0.5811,  0.6417,  0.4111, -0.4752, -0.8355, -0.3645,  0.8067,\n","          1.0715, -0.4236],\n","        [-0.3865,  0.3827, -0.8349, -0.4688,  0.5237,  0.8359,  0.3554, -0.9302,\n","         -0.7720,  0.4303],\n","        [ 0.5620, -0.3489,  0.9897,  0.5475, -0.7199, -0.9189, -0.7884,  1.1241,\n","          0.7660, -0.4969],\n","        [-0.4937,  0.8157,  0.9074, -0.3226,  0.4842,  1.3247,  0.5679, -1.1275,\n","         -0.4281,  0.4417],\n","        [ 0.5027, -0.0769,  0.5209, -0.1918, -0.5090, -0.3564, -0.5347,  0.2301,\n","          0.8724,  0.2181],\n","        [-0.1244, -0.5788,  0.9015,  0.5556, -0.3142, -0.2516, -0.3619,  0.4704,\n","          0.2710, -0.3817],\n","        [-0.0644, -0.7913,  0.8403,  0.8380, -0.7268, -0.0113, -0.3687,  0.2160,\n","          0.0984, -0.5323],\n","        [-0.2162, -0.5594,  0.4079,  0.2637, -0.5358,  0.0087, -0.0717,  0.1058,\n","          0.0245, -0.2943]])\n","linear_hidden.0.bias tensor([-0.5141,  0.1272,  0.4138,  0.0261,  0.1945, -0.6840, -0.5198, -0.4595,\n","        -0.5462, -0.4722])\n","linear_output.weight tensor([[-1.2438,  1.4913,  1.3856, -1.5252,  1.5149, -1.3040,  0.8636,  0.6510,\n","          0.7636,  0.2744]])\n","linear_output.bias tensor([0.5119])\n","Testing out: \n","batch_size:  2987\n","train_size:  3010\n","n_epochs:  126\n","lr:  0.0377474609709037\n","weight_decay:  0.00021410049775872573\n","betas0:  0.9998865934900326\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2918, -0.0468,  0.1575],\n","          [ 0.2636,  0.3104, -0.0559],\n","          [-0.0110,  0.0632, -0.0390]]]])\n","conv1.bias tensor([-0.1560])\n","first_linear.weight tensor([[-0.1620,  0.3293, -0.2984, -0.0791,  0.2910,  0.3188,  0.2140, -0.1011,\n","          0.0609],\n","        [-0.2885, -0.1300, -0.1249,  0.0394,  0.1100,  0.2547, -0.1054,  0.0077,\n","          0.1389],\n","        [ 0.2421, -0.3017,  0.3243,  0.0409, -0.1440,  0.0690,  0.2247, -0.2331,\n","          0.1416],\n","        [-0.0626, -0.1427, -0.2296, -0.1836,  0.0361, -0.1901, -0.1505, -0.0291,\n","          0.1812],\n","        [-0.0870,  0.3165,  0.1321,  0.0593,  0.2287, -0.0849,  0.1702, -0.0107,\n","         -0.0430],\n","        [ 0.0743, -0.1901,  0.0346,  0.2853, -0.2798, -0.1870, -0.0436,  0.3199,\n","         -0.2659],\n","        [-0.0241,  0.1049, -0.0339, -0.2974, -0.3289,  0.2219,  0.0783,  0.1586,\n","          0.0335],\n","        [-0.2204,  0.0508, -0.1542, -0.0150,  0.2637, -0.0827, -0.0187, -0.1981,\n","          0.1189],\n","        [-0.0157, -0.2750,  0.1488,  0.0856,  0.1541,  0.0564, -0.3062,  0.2723,\n","          0.2389],\n","        [-0.0453,  0.1183, -0.1926, -0.0839, -0.1384,  0.1213,  0.3295,  0.0666,\n","          0.2019]])\n","first_linear.bias tensor([ 0.3298,  0.0537, -0.0090,  0.1777, -0.1026, -0.0409,  0.1986,  0.2604,\n","        -0.0386,  0.0916])\n","linear_hidden.0.weight tensor([[ 0.2904,  0.0558,  0.2283, -0.1937,  0.1304, -0.1415,  0.0056,  0.0530,\n","          0.2955,  0.0765],\n","        [ 0.0552,  0.1666,  0.1920,  0.1970,  0.2357,  0.1342,  0.2073, -0.0044,\n","          0.1296, -0.2919],\n","        [ 0.1752,  0.1962, -0.2352,  0.1485, -0.0667, -0.0576, -0.2443,  0.2938,\n","         -0.2305,  0.0719],\n","        [ 0.2721,  0.1733,  0.0142, -0.0432, -0.0355,  0.2653,  0.2431,  0.0046,\n","         -0.0645,  0.2444],\n","        [ 0.1304, -0.0928, -0.1907, -0.1695,  0.1802,  0.0682,  0.2026,  0.0136,\n","         -0.2272,  0.1815],\n","        [-0.1458, -0.2160, -0.2515,  0.2270,  0.2050, -0.0713, -0.2515,  0.3091,\n","          0.0312, -0.1718],\n","        [-0.1532,  0.1488,  0.0456,  0.1618, -0.1266, -0.0686, -0.0836,  0.1333,\n","         -0.1575,  0.1250],\n","        [ 0.1958, -0.0657,  0.0251,  0.0893, -0.1558, -0.0461,  0.0158,  0.1599,\n","         -0.1237, -0.2007],\n","        [-0.2273, -0.0958, -0.3033, -0.3131,  0.1287, -0.1433,  0.2150, -0.2954,\n","         -0.0864, -0.0483],\n","        [-0.2113, -0.2696, -0.2843, -0.1668, -0.1349, -0.1923, -0.0030, -0.1881,\n","          0.0161,  0.2261]])\n","linear_hidden.0.bias tensor([-0.3148, -0.0140,  0.2042, -0.1564,  0.0242, -0.2784,  0.1795, -0.2752,\n","        -0.0418,  0.2075])\n","linear_output.weight tensor([[-0.2729,  0.2538, -0.1697,  0.2174,  0.0329, -0.1566,  0.0091,  0.0373,\n","         -0.1217,  0.2222]])\n","linear_output.bias tensor([-0.0420])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2987])) that is different to the input size (torch.Size([2987, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6824726760387421 \t Validate_Accuracy: 0.503\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.709624707698822 \t Validate_Accuracy: 0.502\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7031049132347107 \t Validate_Accuracy: 0.4385\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6944516897201538 \t Validate_Accuracy: 0.5565\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6823127865791321 \t Validate_Accuracy: 0.5115\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6832369863986969 \t Validate_Accuracy: 0.5125\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.729634165763855 \t Validate_Accuracy: 0.5135\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.7027681171894073 \t Validate_Accuracy: 0.534\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.687533438205719 \t Validate_Accuracy: 0.573\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6915014982223511 \t Validate_Accuracy: 0.552\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6909596025943756 \t Validate_Accuracy: 0.4685\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.684865802526474 \t Validate_Accuracy: 0.6225\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6641985177993774 \t Validate_Accuracy: 0.608\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.671281099319458 \t Validate_Accuracy: 0.623\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6567370891571045 \t Validate_Accuracy: 0.657\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6508715450763702 \t Validate_Accuracy: 0.6695\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6106084585189819 \t Validate_Accuracy: 0.7215\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.5729034841060638 \t Validate_Accuracy: 0.7755\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.5316705703735352 \t Validate_Accuracy: 0.792\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.5273006558418274 \t Validate_Accuracy: 0.801\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.4945930242538452 \t Validate_Accuracy: 0.801\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.5152228772640228 \t Validate_Accuracy: 0.7855\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.4868541806936264 \t Validate_Accuracy: 0.767\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.5551202744245529 \t Validate_Accuracy: 0.802\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3572760969400406 \t Validate_Accuracy: 0.8235\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.4322633743286133 \t Validate_Accuracy: 0.8175\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.5205516517162323 \t Validate_Accuracy: 0.8195\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.39782558381557465 \t Validate_Accuracy: 0.8\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.48913122713565826 \t Validate_Accuracy: 0.817\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3571332097053528 \t Validate_Accuracy: 0.8135\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.37073037028312683 \t Validate_Accuracy: 0.802\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.4008246213197708 \t Validate_Accuracy: 0.8095\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.47288647294044495 \t Validate_Accuracy: 0.814\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3632463067770004 \t Validate_Accuracy: 0.8115\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.44192489981651306 \t Validate_Accuracy: 0.8115\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.37569886445999146 \t Validate_Accuracy: 0.8195\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.4088384509086609 \t Validate_Accuracy: 0.823\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.406020849943161 \t Validate_Accuracy: 0.822\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.3152388632297516 \t Validate_Accuracy: 0.8325\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.3420724719762802 \t Validate_Accuracy: 0.828\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.33637407422065735 \t Validate_Accuracy: 0.82\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.3309914469718933 \t Validate_Accuracy: 0.8235\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.32388175278902054 \t Validate_Accuracy: 0.8185\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2869648411870003 \t Validate_Accuracy: 0.816\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.26911837607622147 \t Validate_Accuracy: 0.816\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.4651230275630951 \t Validate_Accuracy: 0.8195\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.37792378664016724 \t Validate_Accuracy: 0.8245\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.4876767247915268 \t Validate_Accuracy: 0.8315\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.3778858482837677 \t Validate_Accuracy: 0.8315\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.3369504064321518 \t Validate_Accuracy: 0.808\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.38899707794189453 \t Validate_Accuracy: 0.803\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3841552138328552 \t Validate_Accuracy: 0.8075\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.4122820198535919 \t Validate_Accuracy: 0.828\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.3561321049928665 \t Validate_Accuracy: 0.8215\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.38179725408554077 \t Validate_Accuracy: 0.821\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.3860141783952713 \t Validate_Accuracy: 0.823\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.3603147566318512 \t Validate_Accuracy: 0.8275\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.33610567450523376 \t Validate_Accuracy: 0.83\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.4199243485927582 \t Validate_Accuracy: 0.828\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.4073614329099655 \t Validate_Accuracy: 0.8325\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2801571786403656 \t Validate_Accuracy: 0.831\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.2916499674320221 \t Validate_Accuracy: 0.8355\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.3422747850418091 \t Validate_Accuracy: 0.8415\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.2967543378472328 \t Validate_Accuracy: 0.839\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.3750629276037216 \t Validate_Accuracy: 0.846\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.36575502157211304 \t Validate_Accuracy: 0.8545\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.45256026089191437 \t Validate_Accuracy: 0.8535\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.3367141783237457 \t Validate_Accuracy: 0.8385\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.32640981674194336 \t Validate_Accuracy: 0.8305\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.4222392737865448 \t Validate_Accuracy: 0.839\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.3600543737411499 \t Validate_Accuracy: 0.848\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.40022529661655426 \t Validate_Accuracy: 0.847\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.3611123114824295 \t Validate_Accuracy: 0.849\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2786346897482872 \t Validate_Accuracy: 0.8535\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.24176780134439468 \t Validate_Accuracy: 0.833\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.3659021407365799 \t Validate_Accuracy: 0.828\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.39764559268951416 \t Validate_Accuracy: 0.8425\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.3850689232349396 \t Validate_Accuracy: 0.8415\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.3699553459882736 \t Validate_Accuracy: 0.838\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.4811783581972122 \t Validate_Accuracy: 0.8415\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.3460680842399597 \t Validate_Accuracy: 0.836\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.3116210252046585 \t Validate_Accuracy: 0.821\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.3101997971534729 \t Validate_Accuracy: 0.804\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.3606998175382614 \t Validate_Accuracy: 0.8095\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.39146652817726135 \t Validate_Accuracy: 0.8295\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.48162081837654114 \t Validate_Accuracy: 0.8185\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.28394611179828644 \t Validate_Accuracy: 0.7905\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.4355613738298416 \t Validate_Accuracy: 0.7985\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.3637952208518982 \t Validate_Accuracy: 0.827\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2905181869864464 \t Validate_Accuracy: 0.828\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.327167272567749 \t Validate_Accuracy: 0.817\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.3407105505466461 \t Validate_Accuracy: 0.813\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.5301746726036072 \t Validate_Accuracy: 0.825\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.28489962220191956 \t Validate_Accuracy: 0.836\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.3058807849884033 \t Validate_Accuracy: 0.836\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.378380686044693 \t Validate_Accuracy: 0.839\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.371610090136528 \t Validate_Accuracy: 0.835\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.28816288709640503 \t Validate_Accuracy: 0.84\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.35757118463516235 \t Validate_Accuracy: 0.835\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.34357455372810364 \t Validate_Accuracy: 0.8285\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.28469205647706985 \t Validate_Accuracy: 0.831\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.24989599734544754 \t Validate_Accuracy: 0.8305\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.24694593995809555 \t Validate_Accuracy: 0.8355\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.3423152416944504 \t Validate_Accuracy: 0.8395\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.28152746707201004 \t Validate_Accuracy: 0.847\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.240165077149868 \t Validate_Accuracy: 0.8425\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.5328416377305984 \t Validate_Accuracy: 0.84\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.37930651009082794 \t Validate_Accuracy: 0.8415\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.4009310454130173 \t Validate_Accuracy: 0.8435\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2577579766511917 \t Validate_Accuracy: 0.8465\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2892952263355255 \t Validate_Accuracy: 0.8415\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.3051598072052002 \t Validate_Accuracy: 0.8445\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.34724804759025574 \t Validate_Accuracy: 0.849\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.34908005595207214 \t Validate_Accuracy: 0.849\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.32729649543762207 \t Validate_Accuracy: 0.8515\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.3638518154621124 \t Validate_Accuracy: 0.8495\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.25790367275476456 \t Validate_Accuracy: 0.841\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.3346702307462692 \t Validate_Accuracy: 0.8325\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.3227357864379883 \t Validate_Accuracy: 0.831\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.4902701675891876 \t Validate_Accuracy: 0.8335\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.33492597937583923 \t Validate_Accuracy: 0.837\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.24333617091178894 \t Validate_Accuracy: 0.8445\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2251989282667637 \t Validate_Accuracy: 0.8305\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.3086174428462982 \t Validate_Accuracy: 0.8265\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.28730903565883636 \t Validate_Accuracy: 0.839\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.3441518545150757 \t Validate_Accuracy: 0.856\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.4905, -0.2533, -0.3656],\n","          [ 0.0955, -0.0505,  0.0231],\n","          [-0.4499, -0.2014, -0.3264]]]])\n","conv1.bias tensor([-0.1072])\n","first_linear.weight tensor([[ 0.2149, -0.3389, -0.6741, -1.9380, -0.3297,  0.3136,  0.8950, -1.3732,\n","          0.1173],\n","        [-0.7903, -0.6186, -0.5494, -0.5017, -0.2418, -0.2929, -0.0985, -0.3406,\n","         -0.1487],\n","        [ 0.2540, -0.0432,  0.1548, -0.1412,  0.9412, -0.7056,  0.7305, -1.3952,\n","          1.1173],\n","        [-0.6387, -0.0570,  0.3463, -0.8312, -0.5070, -0.1248, -0.6001, -0.4642,\n","          0.3179],\n","        [-1.2083,  1.3863, -0.6000,  0.4585,  0.3570, -0.2801,  0.0972, -0.3918,\n","          0.4649],\n","        [-0.5528, -0.2308,  0.6301,  0.9487,  0.4136, -1.1844, -0.8327, -0.3663,\n","          1.2814],\n","        [-0.4406, -0.3007, -0.3976,  0.6197, -1.7669,  0.0480, -1.2196,  1.0663,\n","         -0.2523],\n","        [ 0.0390, -0.2930, -0.6580,  0.1499, -0.3660, -1.2555,  0.1797, -0.3530,\n","         -0.5883],\n","        [-0.1565, -0.0667,  0.0377,  0.2788,  0.7180,  0.4286,  0.3986,  0.3171,\n","          0.8792],\n","        [-0.4022,  0.9234, -1.4812,  0.0882, -0.3763,  1.3362,  0.5425, -0.4315,\n","         -0.0248]])\n","first_linear.bias tensor([-0.1629, -1.5449,  1.0077,  1.6350,  1.5593, -1.4757,  0.4535,  1.6425,\n","         1.3637, -1.6111])\n","linear_hidden.0.weight tensor([[ 0.7391,  0.1346,  0.7355,  0.5317, -0.4884,  1.1094, -0.4154,  0.6103,\n","          0.8509,  0.9250],\n","        [ 0.3561,  0.5994,  1.0585, -0.0776,  1.1828,  0.3357, -0.4581, -0.1566,\n","          0.1993, -1.5846],\n","        [ 0.5731, -0.0510, -0.2329,  0.5503, -0.9021,  0.7774, -1.1960,  0.9147,\n","          0.2775,  1.1845],\n","        [ 0.8087,  0.7551,  0.4736,  0.2369,  0.3029, -0.1587,  0.6443, -0.3556,\n","         -1.2091, -0.5146],\n","        [-0.8945,  0.2608, -0.1606, -0.0242, -0.4008,  2.1592,  0.6518,  0.5678,\n","          0.0917,  0.0759],\n","        [-0.0830, -0.9414,  0.1094,  0.7811, -0.7970,  0.5709, -0.1361,  1.1985,\n","          0.2861,  0.5578],\n","        [-1.0512,  0.1341,  0.3100, -1.4167,  0.4066, -0.3023, -1.0311, -1.1467,\n","         -0.2828, -0.1109],\n","        [ 0.6478, -0.3988,  0.2638,  0.6857, -0.1899, -0.1673,  0.9108,  1.4880,\n","          0.4491, -0.1625],\n","        [-1.2228, -0.6393, -1.4448,  0.1825, -0.6065,  0.9992,  1.1908,  0.2129,\n","         -0.2236,  0.6539],\n","        [-0.7004, -1.1200,  0.1519,  0.3582,  0.2322, -0.4399, -0.6147,  0.3447,\n","          0.9395, -0.0943]])\n","linear_hidden.0.bias tensor([-0.1855,  0.3160,  0.0313, -0.0509, -0.0740, -0.0679, -0.7835,  1.0046,\n","        -0.2915,  1.4420])\n","linear_output.weight tensor([[-0.7010,  0.8169, -0.7305,  0.1761, -0.5558, -0.6388,  0.6384, -0.9776,\n","         -0.8344, -1.0987]])\n","linear_output.bias tensor([0.0974])\n","Testing out: \n","batch_size:  2925\n","train_size:  3980\n","n_epochs:  170\n","lr:  0.02955725047136221\n","weight_decay:  2.787598499740914e-05\n","betas0:  0.8076423343558669\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1374, -0.2841,  0.3192],\n","          [-0.0832, -0.3096, -0.1184],\n","          [ 0.2378, -0.1120,  0.1150]]]])\n","conv1.bias tensor([0.1535])\n","first_linear.weight tensor([[ 0.0245, -0.2189, -0.1247,  0.2547,  0.1130,  0.0441,  0.3262, -0.3175,\n","         -0.2373],\n","        [ 0.1128, -0.3064, -0.1106, -0.1376,  0.2124,  0.0749,  0.2766,  0.1684,\n","          0.0467],\n","        [ 0.0768, -0.0233,  0.3096, -0.2059, -0.0817, -0.2782, -0.1577,  0.2639,\n","          0.1853],\n","        [-0.2288, -0.2301,  0.2100,  0.1852, -0.1511,  0.1306, -0.2700,  0.2620,\n","         -0.0828],\n","        [-0.1504,  0.0311,  0.2144,  0.2595,  0.0391, -0.0132, -0.0953,  0.2521,\n","          0.1477],\n","        [ 0.2498,  0.2536,  0.1526,  0.0676, -0.3318, -0.1926, -0.0512,  0.2982,\n","         -0.2981],\n","        [-0.2643, -0.2565,  0.1433,  0.2753, -0.0014, -0.2512, -0.1388, -0.0903,\n","         -0.1246],\n","        [ 0.0931,  0.1158,  0.1030, -0.1745, -0.2406, -0.0896, -0.3210,  0.2563,\n","          0.1911],\n","        [ 0.0750,  0.0289, -0.1855,  0.2364, -0.0567, -0.2441,  0.1916, -0.1510,\n","         -0.1765],\n","        [-0.2302, -0.2155, -0.1911,  0.1063,  0.2695,  0.0735,  0.0513,  0.0100,\n","         -0.0404]])\n","first_linear.bias tensor([-0.2201, -0.1153, -0.2832,  0.2678, -0.0352,  0.3257,  0.2415,  0.3020,\n","         0.2371, -0.3172])\n","linear_hidden.0.weight tensor([[-0.2418,  0.0868, -0.1870,  0.2621,  0.1155, -0.2812,  0.0601,  0.0603,\n","          0.2704,  0.2505],\n","        [ 0.0289,  0.0195, -0.2018,  0.0898,  0.2362, -0.0792, -0.1003,  0.0860,\n","          0.0818, -0.2987],\n","        [ 0.1346, -0.1707,  0.0683,  0.1676, -0.2408, -0.2150, -0.2281, -0.1262,\n","         -0.0122, -0.2458],\n","        [ 0.1058,  0.0851, -0.1483, -0.0299,  0.0859, -0.1992, -0.0514, -0.0614,\n","         -0.2058, -0.1217],\n","        [-0.1596,  0.1361, -0.1859, -0.2850, -0.2484,  0.1993,  0.1868,  0.0017,\n","          0.0823,  0.0869],\n","        [-0.0070,  0.1543, -0.2648, -0.0217, -0.2957, -0.0132,  0.1213, -0.2909,\n","          0.2987, -0.2635],\n","        [ 0.1263, -0.0142,  0.2233,  0.1150,  0.1518,  0.1878, -0.1181,  0.2043,\n","          0.2073,  0.2177],\n","        [ 0.0534,  0.0801,  0.2601, -0.2016, -0.2916,  0.1327, -0.2722, -0.2578,\n","         -0.2666,  0.2994],\n","        [-0.1519,  0.2874,  0.1653,  0.1459, -0.1836,  0.1640, -0.0935,  0.0212,\n","          0.0482, -0.0246],\n","        [ 0.1814,  0.0037, -0.2028, -0.2745,  0.1322, -0.2854,  0.1630,  0.3123,\n","          0.3127, -0.0695]])\n","linear_hidden.0.bias tensor([-0.1148,  0.0879,  0.2707, -0.0212,  0.2915,  0.0981,  0.2288, -0.1369,\n","         0.2369, -0.1701])\n","linear_output.weight tensor([[-0.1022, -0.1176, -0.2982, -0.1797, -0.2477,  0.1129, -0.2495,  0.2409,\n","         -0.1155, -0.2528]])\n","linear_output.bias tensor([-0.2984])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7202708721160889 \t Validate_Accuracy: 0.528\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2925])) that is different to the input size (torch.Size([2925, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1055])) that is different to the input size (torch.Size([1055, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.692890465259552 \t Validate_Accuracy: 0.498\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7000684142112732 \t Validate_Accuracy: 0.496\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6948840320110321 \t Validate_Accuracy: 0.5565\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6941920518875122 \t Validate_Accuracy: 0.594\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6883808672428131 \t Validate_Accuracy: 0.5715\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6818545758724213 \t Validate_Accuracy: 0.5605\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.676396369934082 \t Validate_Accuracy: 0.5875\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6602209210395813 \t Validate_Accuracy: 0.638\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6386017203330994 \t Validate_Accuracy: 0.6455\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6154312789440155 \t Validate_Accuracy: 0.644\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.5820437371730804 \t Validate_Accuracy: 0.773\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.5573694407939911 \t Validate_Accuracy: 0.7945\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.5296034514904022 \t Validate_Accuracy: 0.809\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.49764351546764374 \t Validate_Accuracy: 0.8035\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.48438067734241486 \t Validate_Accuracy: 0.816\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.47522060573101044 \t Validate_Accuracy: 0.8185\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.45307302474975586 \t Validate_Accuracy: 0.8115\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.44375449419021606 \t Validate_Accuracy: 0.824\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.43141601979732513 \t Validate_Accuracy: 0.8285\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.4234852194786072 \t Validate_Accuracy: 0.8255\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.41113682091236115 \t Validate_Accuracy: 0.834\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3987278640270233 \t Validate_Accuracy: 0.829\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.4011742025613785 \t Validate_Accuracy: 0.83\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3838714212179184 \t Validate_Accuracy: 0.8435\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3739842474460602 \t Validate_Accuracy: 0.8295\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3646094799041748 \t Validate_Accuracy: 0.837\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.3564416766166687 \t Validate_Accuracy: 0.848\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.347405344247818 \t Validate_Accuracy: 0.8425\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.3570263236761093 \t Validate_Accuracy: 0.845\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.343513160943985 \t Validate_Accuracy: 0.856\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.3307872861623764 \t Validate_Accuracy: 0.8505\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3360971361398697 \t Validate_Accuracy: 0.845\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3214492052793503 \t Validate_Accuracy: 0.858\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.3147745281457901 \t Validate_Accuracy: 0.858\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.30211855471134186 \t Validate_Accuracy: 0.8525\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.3089487850666046 \t Validate_Accuracy: 0.8535\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.31041939556598663 \t Validate_Accuracy: 0.8595\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.3015895336866379 \t Validate_Accuracy: 0.855\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.3046129196882248 \t Validate_Accuracy: 0.8605\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.28967222571372986 \t Validate_Accuracy: 0.861\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2932891696691513 \t Validate_Accuracy: 0.8605\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2888164222240448 \t Validate_Accuracy: 0.858\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2799345701932907 \t Validate_Accuracy: 0.8595\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2840587794780731 \t Validate_Accuracy: 0.867\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2769288122653961 \t Validate_Accuracy: 0.8615\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.26411522179841995 \t Validate_Accuracy: 0.867\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.2593243271112442 \t Validate_Accuracy: 0.8655\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.2643221616744995 \t Validate_Accuracy: 0.8715\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.2630094885826111 \t Validate_Accuracy: 0.866\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.25893761217594147 \t Validate_Accuracy: 0.8675\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.2574818879365921 \t Validate_Accuracy: 0.8715\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.25834354758262634 \t Validate_Accuracy: 0.8745\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2516467720270157 \t Validate_Accuracy: 0.871\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.25779175013303757 \t Validate_Accuracy: 0.8705\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.24766205251216888 \t Validate_Accuracy: 0.8695\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2497493177652359 \t Validate_Accuracy: 0.872\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.24781592935323715 \t Validate_Accuracy: 0.874\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.24227168411016464 \t Validate_Accuracy: 0.8735\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.24520041048526764 \t Validate_Accuracy: 0.8725\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.24337435513734818 \t Validate_Accuracy: 0.877\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.2417326122522354 \t Validate_Accuracy: 0.878\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.24474774301052094 \t Validate_Accuracy: 0.8775\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.24589572101831436 \t Validate_Accuracy: 0.8755\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2507350593805313 \t Validate_Accuracy: 0.8765\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.23804203420877457 \t Validate_Accuracy: 0.8835\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.2286990061402321 \t Validate_Accuracy: 0.879\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.23370740562677383 \t Validate_Accuracy: 0.8775\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.23769306391477585 \t Validate_Accuracy: 0.8755\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.23713186383247375 \t Validate_Accuracy: 0.865\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2376646250486374 \t Validate_Accuracy: 0.88\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.2471727430820465 \t Validate_Accuracy: 0.869\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.246736541390419 \t Validate_Accuracy: 0.8775\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.24140702188014984 \t Validate_Accuracy: 0.871\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.23472458124160767 \t Validate_Accuracy: 0.879\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2376391589641571 \t Validate_Accuracy: 0.8775\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.23314404487609863 \t Validate_Accuracy: 0.878\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.23873940855264664 \t Validate_Accuracy: 0.875\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.22601505368947983 \t Validate_Accuracy: 0.8815\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.22399578243494034 \t Validate_Accuracy: 0.879\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.22754332423210144 \t Validate_Accuracy: 0.884\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.23278836905956268 \t Validate_Accuracy: 0.8875\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.21966197341680527 \t Validate_Accuracy: 0.88\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.22058479487895966 \t Validate_Accuracy: 0.8815\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.22725189477205276 \t Validate_Accuracy: 0.8825\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.22635968774557114 \t Validate_Accuracy: 0.886\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.21367160230875015 \t Validate_Accuracy: 0.887\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2330893650650978 \t Validate_Accuracy: 0.883\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.22890421748161316 \t Validate_Accuracy: 0.8845\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.22436457127332687 \t Validate_Accuracy: 0.883\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.22201021760702133 \t Validate_Accuracy: 0.884\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.22028778493404388 \t Validate_Accuracy: 0.884\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.21981991082429886 \t Validate_Accuracy: 0.8845\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.21259695291519165 \t Validate_Accuracy: 0.8825\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.21477379649877548 \t Validate_Accuracy: 0.8825\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.22091323882341385 \t Validate_Accuracy: 0.886\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.22279059886932373 \t Validate_Accuracy: 0.8835\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.23036647588014603 \t Validate_Accuracy: 0.881\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.22475598752498627 \t Validate_Accuracy: 0.885\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.22156427800655365 \t Validate_Accuracy: 0.8825\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.22002214938402176 \t Validate_Accuracy: 0.89\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.22636426985263824 \t Validate_Accuracy: 0.8805\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.2315903827548027 \t Validate_Accuracy: 0.88\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.23092717677354813 \t Validate_Accuracy: 0.8795\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2211851105093956 \t Validate_Accuracy: 0.8855\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.22141965478658676 \t Validate_Accuracy: 0.884\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.21175319701433182 \t Validate_Accuracy: 0.8885\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.21368490159511566 \t Validate_Accuracy: 0.881\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2204318568110466 \t Validate_Accuracy: 0.886\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2167741134762764 \t Validate_Accuracy: 0.8845\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2154858112335205 \t Validate_Accuracy: 0.8825\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.22595789283514023 \t Validate_Accuracy: 0.8855\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.21659920364618301 \t Validate_Accuracy: 0.8825\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.21984729915857315 \t Validate_Accuracy: 0.879\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.22270309180021286 \t Validate_Accuracy: 0.8865\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.21980691701173782 \t Validate_Accuracy: 0.882\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.21880389750003815 \t Validate_Accuracy: 0.8855\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.22076459228992462 \t Validate_Accuracy: 0.8845\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.21106552332639694 \t Validate_Accuracy: 0.8855\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.21284202486276627 \t Validate_Accuracy: 0.887\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.21601273864507675 \t Validate_Accuracy: 0.8875\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.2212265059351921 \t Validate_Accuracy: 0.8865\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.21631252020597458 \t Validate_Accuracy: 0.886\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.2144039273262024 \t Validate_Accuracy: 0.8865\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.21342720091342926 \t Validate_Accuracy: 0.8875\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2172459438443184 \t Validate_Accuracy: 0.8875\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2102060243487358 \t Validate_Accuracy: 0.884\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.21444351971149445 \t Validate_Accuracy: 0.89\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.22182942926883698 \t Validate_Accuracy: 0.888\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.2138223871588707 \t Validate_Accuracy: 0.884\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.21496225148439407 \t Validate_Accuracy: 0.8895\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.21573930233716965 \t Validate_Accuracy: 0.885\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.21363715827465057 \t Validate_Accuracy: 0.887\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.22438790649175644 \t Validate_Accuracy: 0.8825\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.214581198990345 \t Validate_Accuracy: 0.8865\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.211954727768898 \t Validate_Accuracy: 0.8805\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.22634979337453842 \t Validate_Accuracy: 0.8885\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.21116572618484497 \t Validate_Accuracy: 0.883\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.21504224091768265 \t Validate_Accuracy: 0.8845\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.22087771445512772 \t Validate_Accuracy: 0.88\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2148708552122116 \t Validate_Accuracy: 0.887\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.21310605108737946 \t Validate_Accuracy: 0.8865\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.2108851745724678 \t Validate_Accuracy: 0.8885\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.21455887705087662 \t Validate_Accuracy: 0.8885\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.21079759299755096 \t Validate_Accuracy: 0.89\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.21530194580554962 \t Validate_Accuracy: 0.888\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.218855120241642 \t Validate_Accuracy: 0.887\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.20737836509943008 \t Validate_Accuracy: 0.885\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.21444083750247955 \t Validate_Accuracy: 0.883\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.2217206507921219 \t Validate_Accuracy: 0.885\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.21332145482301712 \t Validate_Accuracy: 0.8865\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.2108171582221985 \t Validate_Accuracy: 0.889\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2163987010717392 \t Validate_Accuracy: 0.886\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.21968261897563934 \t Validate_Accuracy: 0.8895\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.22596384584903717 \t Validate_Accuracy: 0.881\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.22751019150018692 \t Validate_Accuracy: 0.8895\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.21759303659200668 \t Validate_Accuracy: 0.885\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.21646694093942642 \t Validate_Accuracy: 0.8895\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.21346928924322128 \t Validate_Accuracy: 0.892\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.21346646547317505 \t Validate_Accuracy: 0.885\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.21541804820299149 \t Validate_Accuracy: 0.884\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.22479252517223358 \t Validate_Accuracy: 0.8825\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.21113763749599457 \t Validate_Accuracy: 0.8915\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.2239787057042122 \t Validate_Accuracy: 0.8895\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.20579249411821365 \t Validate_Accuracy: 0.8855\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.22068370878696442 \t Validate_Accuracy: 0.8815\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.22222787141799927 \t Validate_Accuracy: 0.887\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.2150312140583992 \t Validate_Accuracy: 0.8935\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.21849356591701508 \t Validate_Accuracy: 0.891\n","epoch 170\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:54:57] ax.service.managed_loop: Running optimization trial 7...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 170 \t Train Loss: 0.2149822860956192 \t Validate_Accuracy: 0.8775\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1135, -0.0510, -0.1311],\n","          [-0.0473, -0.0007, -0.0401],\n","          [-0.1127, -0.0382, -0.1217]]]])\n","conv1.bias tensor([0.0529])\n","first_linear.weight tensor([[-0.3048,  0.5040, -0.8363,  0.2056, -0.6515,  1.6392,  0.5418, -0.5786,\n","         -0.3359],\n","        [ 0.0392, -0.0097,  0.2556,  0.0790,  0.2364,  0.3961,  0.1564, -0.0389,\n","          0.2901],\n","        [ 0.2532,  0.4716,  0.2988,  0.1791,  0.2823, -0.0241,  0.0632,  0.0159,\n","          0.1322],\n","        [-0.3123, -0.4518,  0.1548, -0.1027, -0.1721, -0.2983, -0.2833, -0.1477,\n","          0.0298],\n","        [-0.7189,  0.1518,  0.2733,  1.4291, -0.4968, -0.4648, -0.7568,  0.0045,\n","          0.3763],\n","        [ 0.2184,  0.1362, -0.1450,  1.0131, -1.0833,  0.8786, -0.8525,  1.6878,\n","         -0.9503],\n","        [ 0.7321, -1.8668,  0.6041, -0.1708,  0.4348,  0.1146, -0.2390,  0.0853,\n","         -0.2233],\n","        [ 0.1283,  0.3147, -0.0683, -0.8729, -0.2995, -0.1155, -0.2547,  0.7531,\n","         -0.3990],\n","        [-0.1115, -0.3450, -0.1106, -0.1541, -0.1446, -0.0829, -0.0523, -0.0656,\n","         -0.1169],\n","        [ 0.2099, -0.0974, -0.0404,  0.1867,  0.2592,  0.2053,  0.2495,  0.5079,\n","          0.1912]])\n","first_linear.bias tensor([-1.0346, -0.9549,  0.6378, -0.6654, -0.7956,  1.1354, -0.9083, -0.4492,\n","        -0.8905,  0.8690])\n","linear_hidden.0.weight tensor([[ 0.2537, -0.7278,  0.5911, -0.4641,  1.2304, -1.0882,  0.9033,  0.5511,\n","         -0.5629,  1.0753],\n","        [-0.7256,  1.2302, -0.3766,  0.3551, -1.0417,  0.6050, -0.7158, -0.7516,\n","          0.5123, -0.6013],\n","        [ 0.0717, -0.4950,  0.1252, -0.0098, -1.7367, -0.4277, -0.7465,  0.7944,\n","         -0.2475, -0.1562],\n","        [ 1.0958, -0.7750,  0.4281, -0.6698,  0.3771, -0.9331,  0.8482,  0.9696,\n","         -0.9414,  0.4566],\n","        [-0.3367, -0.4560,  0.3898, -0.6864, -0.3261,  0.3176, -0.1769, -0.3314,\n","         -0.6496,  0.5801],\n","        [-0.6547,  1.2383, -0.3519,  0.2701, -1.0440,  0.6133, -0.6469, -0.7282,\n","          0.7048, -0.5648],\n","        [ 0.1141, -0.7291,  0.4886, -0.2251,  0.1502, -0.0110,  0.1499, -0.0490,\n","         -0.3542,  0.5427],\n","        [-0.5929,  1.0357, -0.4619,  0.4806, -1.0124,  0.6945, -0.5731, -0.7004,\n","          0.6076, -0.4110],\n","        [-0.6320,  1.2077, -0.2685,  0.5025, -0.9802,  0.6270, -0.6037, -0.6652,\n","          0.6409, -0.4414],\n","        [ 1.1334, -0.8157,  0.5862, -1.1745,  0.4654, -0.9871,  0.8841,  1.2975,\n","         -0.5345,  0.7085]])\n","linear_hidden.0.bias tensor([-0.1306, -0.5089,  0.0051, -0.0043,  0.5084, -0.4137,  0.3141, -0.3881,\n","        -0.3504, -0.1546])\n","linear_output.weight tensor([[-1.4995,  1.4914,  0.4033, -1.6314, -0.5190,  0.8082, -0.3118,  1.4428,\n","          1.4154, -1.7605]])\n","linear_output.bias tensor([0.0192])\n","Testing out: \n","batch_size:  1848\n","train_size:  3295\n","n_epochs:  145\n","lr:  0.028436671264106955\n","weight_decay:  0.0020445962926801597\n","betas0:  0.9986999552053045\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.3017, -0.0169,  0.1082],\n","          [-0.3127,  0.2351, -0.1143],\n","          [ 0.0975,  0.0010,  0.1394]]]])\n","conv1.bias tensor([0.3291])\n","first_linear.weight tensor([[-0.1095,  0.3264, -0.2694,  0.1626, -0.2852, -0.2770,  0.3071,  0.2510,\n","          0.0371],\n","        [-0.3130, -0.1519,  0.2286, -0.2934, -0.1525,  0.3207,  0.2158,  0.1801,\n","          0.1256],\n","        [ 0.2270,  0.3024,  0.0444, -0.0387, -0.2520,  0.0277,  0.1706, -0.1252,\n","         -0.2782],\n","        [-0.1980, -0.1445,  0.0404, -0.0732, -0.2693,  0.0409,  0.0718, -0.1582,\n","         -0.2806],\n","        [-0.0226,  0.2170,  0.2476, -0.2146, -0.1027,  0.0449, -0.0764,  0.2796,\n","          0.1936],\n","        [-0.3226,  0.0177,  0.0144,  0.2180, -0.2216,  0.2144,  0.2534,  0.2428,\n","          0.0712],\n","        [-0.3104,  0.2404, -0.0075, -0.1134, -0.0564,  0.2872,  0.1917, -0.3011,\n","         -0.0233],\n","        [ 0.1022, -0.0863, -0.0457,  0.3333,  0.0959, -0.2955,  0.1328, -0.2879,\n","          0.1876],\n","        [-0.1094, -0.2538,  0.1100, -0.2306, -0.0195, -0.0931,  0.2699, -0.2239,\n","          0.2903],\n","        [-0.2206,  0.0564, -0.2540, -0.1011,  0.0837,  0.1551, -0.0361, -0.2994,\n","          0.1052]])\n","first_linear.bias tensor([ 0.1946, -0.0682,  0.2180, -0.0649, -0.1196, -0.0954, -0.2595, -0.2808,\n","         0.2442,  0.2622])\n","linear_hidden.0.weight tensor([[ 0.2041,  0.0424, -0.0540, -0.0056, -0.1673, -0.0268,  0.2555,  0.2092,\n","         -0.0873, -0.0238],\n","        [-0.0047, -0.0912, -0.2959,  0.2067, -0.2676, -0.0230,  0.1542,  0.1617,\n","          0.0769, -0.3001],\n","        [-0.2449, -0.0160,  0.0016,  0.1288, -0.0667,  0.1645, -0.1377, -0.3094,\n","         -0.0016,  0.2740],\n","        [-0.0208,  0.2917, -0.0762, -0.0746, -0.1892, -0.2731, -0.1689, -0.3033,\n","          0.1691, -0.1910],\n","        [ 0.0474,  0.1439, -0.2959,  0.2084, -0.2482, -0.1566,  0.1362, -0.2696,\n","         -0.2081,  0.2066],\n","        [ 0.2394,  0.2924,  0.2899,  0.2559, -0.2787, -0.2374,  0.0841,  0.0475,\n","         -0.2071, -0.2255],\n","        [ 0.1005, -0.2281, -0.1434, -0.0293, -0.0038,  0.2382, -0.2567,  0.3098,\n","          0.0723,  0.3058],\n","        [ 0.1829, -0.0135,  0.1892,  0.0180,  0.2077, -0.3002, -0.2163,  0.2628,\n","          0.0638,  0.1852],\n","        [ 0.0440, -0.0886, -0.1214,  0.1541, -0.1601,  0.1491,  0.2357,  0.0516,\n","         -0.2740, -0.0478],\n","        [ 0.2747,  0.1208, -0.1259, -0.3027, -0.0085,  0.1393,  0.1506,  0.0031,\n","         -0.2845, -0.3151]])\n","linear_hidden.0.bias tensor([ 0.3034, -0.2345,  0.0240, -0.3076, -0.1321,  0.0068, -0.0293,  0.1450,\n","         0.0529,  0.0722])\n","linear_output.weight tensor([[ 0.2590,  0.1775, -0.0419, -0.1381,  0.1969,  0.0727,  0.1004, -0.2793,\n","          0.1985,  0.1421]])\n","linear_output.bias tensor([0.2830])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6996382772922516 \t Validate_Accuracy: 0.5095\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1848])) that is different to the input size (torch.Size([1848, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1447])) that is different to the input size (torch.Size([1447, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6925258040428162 \t Validate_Accuracy: 0.4965\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6946051716804504 \t Validate_Accuracy: 0.497\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6940310299396515 \t Validate_Accuracy: 0.496\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6928571462631226 \t Validate_Accuracy: 0.513\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6927207708358765 \t Validate_Accuracy: 0.5065\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.692971259355545 \t Validate_Accuracy: 0.5025\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6926013231277466 \t Validate_Accuracy: 0.5045\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6923686265945435 \t Validate_Accuracy: 0.53\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6925238370895386 \t Validate_Accuracy: 0.5155\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6926269829273224 \t Validate_Accuracy: 0.517\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6924044489860535 \t Validate_Accuracy: 0.534\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6921006143093109 \t Validate_Accuracy: 0.4855\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.691938042640686 \t Validate_Accuracy: 0.4735\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6918987929821014 \t Validate_Accuracy: 0.47\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6918250918388367 \t Validate_Accuracy: 0.4865\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6914684772491455 \t Validate_Accuracy: 0.5155\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6910529136657715 \t Validate_Accuracy: 0.543\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6903307437896729 \t Validate_Accuracy: 0.5565\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6884884238243103 \t Validate_Accuracy: 0.573\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6850023865699768 \t Validate_Accuracy: 0.6005\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6764643788337708 \t Validate_Accuracy: 0.675\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6618049740791321 \t Validate_Accuracy: 0.7525\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6366472840309143 \t Validate_Accuracy: 0.7965\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.5984967052936554 \t Validate_Accuracy: 0.801\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.5454801619052887 \t Validate_Accuracy: 0.8005\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.491748183965683 \t Validate_Accuracy: 0.773\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.4695105254650116 \t Validate_Accuracy: 0.8025\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.44925984740257263 \t Validate_Accuracy: 0.789\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.45012880861759186 \t Validate_Accuracy: 0.8085\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.4466250091791153 \t Validate_Accuracy: 0.791\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.4383203536272049 \t Validate_Accuracy: 0.8095\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.4340262860059738 \t Validate_Accuracy: 0.8\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.429384246468544 \t Validate_Accuracy: 0.815\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.4261407405138016 \t Validate_Accuracy: 0.804\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.42022550106048584 \t Validate_Accuracy: 0.814\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.4061627686023712 \t Validate_Accuracy: 0.82\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.4012340158224106 \t Validate_Accuracy: 0.8145\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.39434391260147095 \t Validate_Accuracy: 0.821\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.3854965716600418 \t Validate_Accuracy: 0.8325\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.3771584928035736 \t Validate_Accuracy: 0.832\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.3711741268634796 \t Validate_Accuracy: 0.837\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.3665626347064972 \t Validate_Accuracy: 0.84\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.3619137704372406 \t Validate_Accuracy: 0.8395\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.35552969574928284 \t Validate_Accuracy: 0.8385\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.3492843210697174 \t Validate_Accuracy: 0.8425\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.3482948839664459 \t Validate_Accuracy: 0.8465\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.3432764858007431 \t Validate_Accuracy: 0.851\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.3417357951402664 \t Validate_Accuracy: 0.842\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.33788996934890747 \t Validate_Accuracy: 0.84\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.3346857875585556 \t Validate_Accuracy: 0.8455\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3307834267616272 \t Validate_Accuracy: 0.8525\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.3306882083415985 \t Validate_Accuracy: 0.8465\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.3263012021780014 \t Validate_Accuracy: 0.8545\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.32631437480449677 \t Validate_Accuracy: 0.8555\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.32442498207092285 \t Validate_Accuracy: 0.8535\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.32327012717723846 \t Validate_Accuracy: 0.856\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.3225066661834717 \t Validate_Accuracy: 0.8545\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.31845398247241974 \t Validate_Accuracy: 0.8565\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.31922687590122223 \t Validate_Accuracy: 0.863\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.31614460051059723 \t Validate_Accuracy: 0.8555\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.31411561369895935 \t Validate_Accuracy: 0.856\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.3151770234107971 \t Validate_Accuracy: 0.8635\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.31207388639450073 \t Validate_Accuracy: 0.858\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.30821821093559265 \t Validate_Accuracy: 0.8595\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.3064337819814682 \t Validate_Accuracy: 0.8565\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3062773793935776 \t Validate_Accuracy: 0.8585\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.30606718361377716 \t Validate_Accuracy: 0.854\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.30190393328666687 \t Validate_Accuracy: 0.8605\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2969217449426651 \t Validate_Accuracy: 0.862\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2947472631931305 \t Validate_Accuracy: 0.8625\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.28897254168987274 \t Validate_Accuracy: 0.86\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.2874111384153366 \t Validate_Accuracy: 0.8585\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2876038998365402 \t Validate_Accuracy: 0.8675\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.28207458555698395 \t Validate_Accuracy: 0.871\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2771146595478058 \t Validate_Accuracy: 0.861\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.2753722071647644 \t Validate_Accuracy: 0.871\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.2730291485786438 \t Validate_Accuracy: 0.8755\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2683197408914566 \t Validate_Accuracy: 0.863\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2685908377170563 \t Validate_Accuracy: 0.876\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2647896558046341 \t Validate_Accuracy: 0.8705\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.25935980677604675 \t Validate_Accuracy: 0.875\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.25788672268390656 \t Validate_Accuracy: 0.8685\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.25389964133501053 \t Validate_Accuracy: 0.874\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.25491076707839966 \t Validate_Accuracy: 0.874\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.25238748639822006 \t Validate_Accuracy: 0.876\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.24862040579319 \t Validate_Accuracy: 0.8715\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2504999563097954 \t Validate_Accuracy: 0.8735\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.24818193912506104 \t Validate_Accuracy: 0.874\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2441495805978775 \t Validate_Accuracy: 0.8755\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.24717526882886887 \t Validate_Accuracy: 0.872\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.24439668655395508 \t Validate_Accuracy: 0.877\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.24330853670835495 \t Validate_Accuracy: 0.8765\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2416207641363144 \t Validate_Accuracy: 0.8755\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.24232149124145508 \t Validate_Accuracy: 0.8745\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.24109526723623276 \t Validate_Accuracy: 0.8755\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.2410888373851776 \t Validate_Accuracy: 0.878\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.24315068870782852 \t Validate_Accuracy: 0.8705\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.23910357058048248 \t Validate_Accuracy: 0.8745\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.23965933918952942 \t Validate_Accuracy: 0.877\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.23950330913066864 \t Validate_Accuracy: 0.878\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2368461787700653 \t Validate_Accuracy: 0.8765\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.23518936336040497 \t Validate_Accuracy: 0.877\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.23679566383361816 \t Validate_Accuracy: 0.8805\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.23274806141853333 \t Validate_Accuracy: 0.882\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.23002824187278748 \t Validate_Accuracy: 0.879\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2316204011440277 \t Validate_Accuracy: 0.883\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.23335663974285126 \t Validate_Accuracy: 0.8825\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.22817011177539825 \t Validate_Accuracy: 0.8805\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.22675568610429764 \t Validate_Accuracy: 0.881\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.22791601717472076 \t Validate_Accuracy: 0.877\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.22499041259288788 \t Validate_Accuracy: 0.886\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.22501393407583237 \t Validate_Accuracy: 0.882\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2257152944803238 \t Validate_Accuracy: 0.8775\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.23267312347888947 \t Validate_Accuracy: 0.881\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.2344127669930458 \t Validate_Accuracy: 0.8845\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2226823791861534 \t Validate_Accuracy: 0.89\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.22153784334659576 \t Validate_Accuracy: 0.885\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.21668216586112976 \t Validate_Accuracy: 0.8835\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.21843425929546356 \t Validate_Accuracy: 0.889\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.21946492791175842 \t Validate_Accuracy: 0.886\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.21773988008499146 \t Validate_Accuracy: 0.8865\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2156115174293518 \t Validate_Accuracy: 0.8865\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.2193642184138298 \t Validate_Accuracy: 0.889\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.21586737036705017 \t Validate_Accuracy: 0.8895\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.21457258611917496 \t Validate_Accuracy: 0.8895\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.21260570734739304 \t Validate_Accuracy: 0.887\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.21759448945522308 \t Validate_Accuracy: 0.882\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.21518733352422714 \t Validate_Accuracy: 0.885\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.21355822682380676 \t Validate_Accuracy: 0.887\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.21346507221460342 \t Validate_Accuracy: 0.887\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.21390879899263382 \t Validate_Accuracy: 0.887\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.21283898502588272 \t Validate_Accuracy: 0.889\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.21370191127061844 \t Validate_Accuracy: 0.8855\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.21192745864391327 \t Validate_Accuracy: 0.8855\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.21376996487379074 \t Validate_Accuracy: 0.889\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.21258629858493805 \t Validate_Accuracy: 0.8895\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.20956506580114365 \t Validate_Accuracy: 0.8895\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.20892610400915146 \t Validate_Accuracy: 0.8865\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.2081935927271843 \t Validate_Accuracy: 0.886\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.2099980190396309 \t Validate_Accuracy: 0.8855\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.2075405865907669 \t Validate_Accuracy: 0.886\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.2105240374803543 \t Validate_Accuracy: 0.89\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.20972416549921036 \t Validate_Accuracy: 0.888\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.20607511699199677 \t Validate_Accuracy: 0.8845\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1850,  0.0842,  0.2356],\n","          [ 0.0967, -0.0107,  0.0606],\n","          [ 0.1881,  0.0670,  0.2124]]]])\n","conv1.bias tensor([-0.0653])\n","first_linear.weight tensor([[ 0.4092,  0.1020, -0.3828, -0.1680, -0.5577,  1.0600,  0.0267,  0.4846,\n","         -0.7297],\n","        [ 0.1591, -0.0505, -0.0090,  0.2075,  0.1807,  0.1653,  0.1562,  0.1733,\n","          0.2109],\n","        [-0.1974, -0.4475,  0.2030,  0.3505,  0.0776,  0.1813, -0.0071, -0.2024,\n","         -0.6828],\n","        [-0.1691, -0.1259, -0.2284, -0.0825, -0.1757, -0.3095, -0.0334, -0.0322,\n","         -0.1104],\n","        [ 0.3809,  0.2252, -0.0949, -1.5667,  0.4035, -0.1570,  0.5317, -0.1196,\n","         -0.0912],\n","        [-0.6466,  1.1298, -0.5949,  0.4620, -0.6788,  0.4046,  0.0689, -0.0722,\n","          0.0292],\n","        [ 0.1258, -0.0234,  0.2847,  0.3765, -0.5732, -0.2359, -0.8619,  1.2509,\n","         -0.2219],\n","        [ 0.4968, -0.8065, -0.1118,  0.0522, -0.0286,  0.5643,  0.1293, -0.1250,\n","          0.2530],\n","        [-0.1559, -0.1615, -0.2135, -0.0560, -0.1219, -0.1617, -0.0074, -0.0659,\n","         -0.1227],\n","        [ 0.0633, -0.0037, -0.0855, -0.0542, -0.1035, -0.0982, -0.0573, -0.2973,\n","         -0.2716]])\n","first_linear.bias tensor([ 0.9409,  0.8679, -0.3605, -1.0989, -0.9737, -0.9806, -1.0219, -0.4693,\n","         0.9721,  0.8289])\n","linear_hidden.0.weight tensor([[ 4.2552e-01, -3.1675e-01, -2.8692e-01,  4.0877e-01, -4.3446e-01,\n","         -4.6881e-01, -3.9248e-01, -3.1534e-01, -3.6510e-01, -3.6052e-01],\n","        [ 4.7926e-01, -3.6608e-01, -3.1023e-01,  5.5462e-01, -4.8721e-01,\n","         -5.1102e-01, -4.6317e-01, -3.4525e-01, -4.5878e-01, -4.1310e-01],\n","        [ 4.3079e-01, -3.2286e-01, -2.8950e-01,  4.2006e-01, -4.3956e-01,\n","         -4.7344e-01, -3.9960e-01, -3.1873e-01, -3.7343e-01, -3.6558e-01],\n","        [ 4.5721e-01, -3.4980e-01, -3.0230e-01,  4.7805e-01, -4.6650e-01,\n","         -4.9618e-01, -4.3289e-01, -3.3560e-01, -4.1455e-01, -3.9306e-01],\n","        [ 5.0508e-01, -3.9576e-01, -3.2486e-01,  5.9353e-01, -5.1431e-01,\n","         -5.3434e-01, -4.9389e-01, -3.6292e-01, -4.9186e-01, -4.3943e-01],\n","        [ 1.7230e-04,  3.9564e-05, -6.3642e-05, -3.1579e-05, -1.1229e-04,\n","         -1.4726e-04, -4.0063e-05, -3.2352e-05, -1.0224e-04, -1.1304e-04],\n","        [ 5.0167e-01, -4.0909e-01, -3.2334e-01,  5.8180e-01, -5.1032e-01,\n","         -5.3088e-01, -4.9140e-01, -3.6073e-01, -4.9964e-01, -4.4259e-01],\n","        [-4.7662e-01,  3.2069e-01,  3.0792e-01, -5.5008e-01,  4.8105e-01,\n","          5.1258e-01,  4.5802e-01,  3.4930e-01,  4.4352e-01,  4.0453e-01],\n","        [ 5.0096e-01, -4.0195e-01, -3.2227e-01,  5.8993e-01, -5.0992e-01,\n","         -5.2968e-01, -4.9065e-01, -3.5934e-01, -4.9613e-01, -4.4116e-01],\n","        [ 3.2271e-01, -2.1085e-01, -2.3061e-01,  2.6699e-01, -3.2672e-01,\n","         -3.6613e-01, -2.7905e-01, -2.3938e-01, -2.4058e-01, -2.5901e-01]])\n","linear_hidden.0.bias tensor([-2.2526e-01, -1.2935e-01, -2.1841e-01, -1.8365e-01, -1.1278e-01,\n","        -1.9413e-06, -9.5852e-02,  1.7057e-01, -9.5764e-02, -2.7022e-01])\n","linear_output.weight tensor([[ 1.2438e+00,  1.3552e+00,  1.2554e+00,  1.3283e+00,  1.4450e+00,\n","          1.3941e-04,  1.4548e+00, -1.3573e+00,  1.4376e+00,  9.2308e-01]])\n","linear_output.bias tensor([-0.0148])\n","Testing out: \n","batch_size:  2541\n","train_size:  4066\n","n_epochs:  85\n","lr:  0.06993466490569425\n","weight_decay:  0.005170408472724954\n","betas0:  0.9991961466769799\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1977, -0.2002, -0.2187],\n","          [-0.1898, -0.2247,  0.1424],\n","          [-0.0391,  0.0469,  0.1323]]]])\n","conv1.bias tensor([0.0357])\n","first_linear.weight tensor([[ 1.6815e-01,  5.1855e-02,  2.4775e-01, -1.8653e-01, -2.4676e-01,\n","         -6.6234e-02, -2.6324e-02,  1.0090e-01, -3.3203e-01],\n","        [-1.6219e-01,  1.1794e-01, -1.8437e-01, -8.4026e-02, -1.7311e-01,\n","          2.1803e-01, -1.6068e-01, -7.5377e-02,  2.9985e-01],\n","        [ 8.6724e-02,  2.0979e-01,  9.5284e-03, -1.1716e-01, -2.8846e-01,\n","         -1.0848e-01, -1.1344e-01, -2.5128e-01,  3.1877e-01],\n","        [-1.7292e-01,  1.4709e-01, -1.1029e-01,  3.0491e-01,  1.5213e-01,\n","          9.0737e-02,  2.0445e-01,  1.0216e-01,  2.6990e-01],\n","        [ 7.5957e-03, -2.3812e-01,  2.9532e-01, -2.0068e-01, -2.3297e-01,\n","         -1.7810e-02, -8.6891e-02, -2.3040e-03, -5.2471e-02],\n","        [-3.1284e-01,  8.1471e-02, -1.2765e-01,  1.9414e-01,  1.5873e-01,\n","          1.6021e-01,  2.2749e-01, -1.5364e-02,  1.9652e-01],\n","        [ 2.0957e-01, -8.3827e-02,  9.4229e-02, -2.1106e-01, -5.5161e-02,\n","          2.1450e-01,  1.2262e-01, -6.7433e-02,  1.1640e-01],\n","        [-5.2741e-02,  3.2474e-01, -5.3092e-02, -1.8424e-01, -3.3170e-05,\n","          1.4365e-01, -8.5799e-02,  1.3683e-01,  2.7109e-01],\n","        [-4.6662e-02, -6.9146e-02,  3.0982e-01,  1.7440e-01, -2.8937e-01,\n","          2.5677e-01,  3.2246e-01, -5.7289e-02,  1.7595e-01],\n","        [-7.3742e-02,  2.9492e-01, -7.4274e-02, -2.3652e-01, -1.3779e-01,\n","          1.8826e-02, -2.6949e-01,  2.7359e-01, -1.3615e-01]])\n","first_linear.bias tensor([-0.0789,  0.0293, -0.2469, -0.0996, -0.1408,  0.0806,  0.2928,  0.0713,\n","        -0.1957,  0.2438])\n","linear_hidden.0.weight tensor([[ 0.1311, -0.0981, -0.1947, -0.1659, -0.1446,  0.2079,  0.3081,  0.0719,\n","         -0.0493,  0.1012],\n","        [ 0.0506, -0.2426,  0.2124,  0.0865, -0.3132,  0.2290,  0.0735,  0.2232,\n","         -0.1320,  0.1037],\n","        [ 0.1395, -0.2956,  0.1944,  0.2770, -0.0734, -0.0416, -0.2113,  0.2044,\n","         -0.1759, -0.2183],\n","        [-0.2754,  0.2704, -0.3060,  0.3135,  0.0600,  0.0694, -0.2885, -0.2805,\n","          0.2714,  0.0393],\n","        [ 0.3152,  0.0141,  0.0831, -0.0569, -0.0059,  0.0297, -0.2939,  0.0480,\n","         -0.2547, -0.0113],\n","        [-0.2723,  0.2780, -0.1235, -0.1197, -0.2731,  0.2676,  0.0409,  0.2481,\n","         -0.2672,  0.0577],\n","        [ 0.1430, -0.2482, -0.1230, -0.1583, -0.1926, -0.0194, -0.0140, -0.0138,\n","         -0.1848, -0.1817],\n","        [ 0.2989,  0.0859,  0.2751,  0.2814,  0.0280, -0.0027,  0.2735, -0.0738,\n","          0.0275,  0.2657],\n","        [ 0.1849,  0.0790, -0.1491, -0.2953, -0.0656, -0.1074,  0.2687,  0.1401,\n","          0.1858,  0.2655],\n","        [-0.2788, -0.2965, -0.0399, -0.1896, -0.2683, -0.1255, -0.0718, -0.0818,\n","         -0.2248,  0.1033]])\n","linear_hidden.0.bias tensor([ 0.1938,  0.1591, -0.1680, -0.1525,  0.0685, -0.2111, -0.2684, -0.1704,\n","         0.0227,  0.0635])\n","linear_output.weight tensor([[-0.0288, -0.2504, -0.2639,  0.2550,  0.0622, -0.0261,  0.2900, -0.2796,\n","          0.0421,  0.2028]])\n","linear_output.bias tensor([0.0237])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6985494196414948 \t Validate_Accuracy: 0.412\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2541])) that is different to the input size (torch.Size([2541, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1525])) that is different to the input size (torch.Size([1525, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6941915452480316 \t Validate_Accuracy: 0.497\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6933342814445496 \t Validate_Accuracy: 0.503\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.693218857049942 \t Validate_Accuracy: 0.4465\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6931290030479431 \t Validate_Accuracy: 0.494\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6931050717830658 \t Validate_Accuracy: 0.497\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.693179577589035 \t Validate_Accuracy: 0.497\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.693174809217453 \t Validate_Accuracy: 0.503\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.693169504404068 \t Validate_Accuracy: 0.5085\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6931382715702057 \t Validate_Accuracy: 0.503\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6932027041912079 \t Validate_Accuracy: 0.503\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6931695938110352 \t Validate_Accuracy: 0.5\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6931570768356323 \t Validate_Accuracy: 0.497\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.693150520324707 \t Validate_Accuracy: 0.497\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6931498050689697 \t Validate_Accuracy: 0.503\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6931724548339844 \t Validate_Accuracy: 0.503\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6931888163089752 \t Validate_Accuracy: 0.503\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6931501924991608 \t Validate_Accuracy: 0.497\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6931621134281158 \t Validate_Accuracy: 0.497\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6932067573070526 \t Validate_Accuracy: 0.497\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6931796669960022 \t Validate_Accuracy: 0.503\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6931323409080505 \t Validate_Accuracy: 0.503\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6932444274425507 \t Validate_Accuracy: 0.503\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6931521594524384 \t Validate_Accuracy: 0.503\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6931547224521637 \t Validate_Accuracy: 0.503\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6931582391262054 \t Validate_Accuracy: 0.503\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6931723654270172 \t Validate_Accuracy: 0.503\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6931508481502533 \t Validate_Accuracy: 0.503\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6932035982608795 \t Validate_Accuracy: 0.503\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6931332051753998 \t Validate_Accuracy: 0.503\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6931504011154175 \t Validate_Accuracy: 0.503\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6932124495506287 \t Validate_Accuracy: 0.503\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6931999623775482 \t Validate_Accuracy: 0.503\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6931582987308502 \t Validate_Accuracy: 0.503\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6931639313697815 \t Validate_Accuracy: 0.503\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6931535303592682 \t Validate_Accuracy: 0.503\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6931871473789215 \t Validate_Accuracy: 0.503\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.6931256949901581 \t Validate_Accuracy: 0.503\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6931570768356323 \t Validate_Accuracy: 0.503\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.693154901266098 \t Validate_Accuracy: 0.503\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6931604743003845 \t Validate_Accuracy: 0.503\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6931643187999725 \t Validate_Accuracy: 0.497\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.6931433975696564 \t Validate_Accuracy: 0.497\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.6931629180908203 \t Validate_Accuracy: 0.497\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6931564807891846 \t Validate_Accuracy: 0.503\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6931866407394409 \t Validate_Accuracy: 0.503\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6931229531764984 \t Validate_Accuracy: 0.503\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6931414008140564 \t Validate_Accuracy: 0.503\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.6931570172309875 \t Validate_Accuracy: 0.503\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.6931808888912201 \t Validate_Accuracy: 0.503\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.6931536793708801 \t Validate_Accuracy: 0.503\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.6931768655776978 \t Validate_Accuracy: 0.503\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.6931534111499786 \t Validate_Accuracy: 0.503\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.6931428015232086 \t Validate_Accuracy: 0.503\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.6931216716766357 \t Validate_Accuracy: 0.503\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.693131685256958 \t Validate_Accuracy: 0.503\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.693213015794754 \t Validate_Accuracy: 0.503\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.6931588649749756 \t Validate_Accuracy: 0.503\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.6931486129760742 \t Validate_Accuracy: 0.497\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.6934073269367218 \t Validate_Accuracy: 0.497\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.6931882500648499 \t Validate_Accuracy: 0.503\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.6932238936424255 \t Validate_Accuracy: 0.503\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.6932530403137207 \t Validate_Accuracy: 0.503\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.6931750178337097 \t Validate_Accuracy: 0.497\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.6931508779525757 \t Validate_Accuracy: 0.497\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.6933258771896362 \t Validate_Accuracy: 0.497\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.6931947767734528 \t Validate_Accuracy: 0.503\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.6931061446666718 \t Validate_Accuracy: 0.503\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.6933108866214752 \t Validate_Accuracy: 0.503\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.6931772530078888 \t Validate_Accuracy: 0.503\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.6931661367416382 \t Validate_Accuracy: 0.503\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.6931485831737518 \t Validate_Accuracy: 0.503\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.6931849420070648 \t Validate_Accuracy: 0.497\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.6931359171867371 \t Validate_Accuracy: 0.497\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.6932632029056549 \t Validate_Accuracy: 0.497\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.6931992471218109 \t Validate_Accuracy: 0.497\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.6931373775005341 \t Validate_Accuracy: 0.503\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.6931897103786469 \t Validate_Accuracy: 0.503\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.6931667625904083 \t Validate_Accuracy: 0.503\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.6931607127189636 \t Validate_Accuracy: 0.503\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.6931683421134949 \t Validate_Accuracy: 0.503\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.6931531727313995 \t Validate_Accuracy: 0.497\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.6932629346847534 \t Validate_Accuracy: 0.497\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.6932076513767242 \t Validate_Accuracy: 0.503\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.693093478679657 \t Validate_Accuracy: 0.503\n","model parameters! \n","\n","conv1.weight tensor([[[[-6.1947e-05, -4.2682e-06,  2.4035e-05],\n","          [ 1.7642e-05, -5.5984e-06, -3.3028e-05],\n","          [-1.6990e-05, -6.5944e-06, -4.7925e-05]]]])\n","conv1.bias tensor([-3.8583e-05])\n","first_linear.weight tensor([[ 1.7806e-05,  1.7766e-06, -2.3927e-05,  1.3950e-05,  7.9103e-06,\n","          3.9335e-06, -2.1670e-05,  9.9379e-06, -1.7567e-05],\n","        [-1.7601e-07,  8.6362e-06,  2.4195e-05,  1.8157e-05, -4.5508e-06,\n","         -1.3741e-05,  3.5864e-06, -2.0876e-05,  6.1194e-06],\n","        [ 1.6622e-05, -7.8497e-06, -7.6678e-06, -1.4541e-06,  1.9887e-05,\n","         -3.7202e-05, -4.0527e-05, -1.1984e-05,  1.5766e-05],\n","        [-2.0821e-05, -1.3213e-05,  3.9271e-06,  2.9954e-05, -1.6282e-05,\n","         -1.1299e-05,  2.7972e-05,  9.0370e-06,  1.4979e-05],\n","        [ 3.2242e-06,  3.3075e-05, -1.7799e-05,  2.3605e-05,  2.2274e-05,\n","         -8.9795e-06, -2.3115e-05,  7.1286e-07, -1.4132e-06],\n","        [-3.2114e-05, -2.9767e-05, -7.9381e-06, -3.3658e-05, -6.1664e-08,\n","         -3.2654e-06,  2.1967e-06,  3.1489e-05, -2.6234e-05],\n","        [-1.7367e-05, -1.4600e-05,  1.4282e-05,  1.4554e-05, -1.7932e-06,\n","         -2.2454e-06,  9.2018e-06, -3.7388e-06, -4.2732e-05],\n","        [ 7.9987e-06, -2.0345e-05,  1.0326e-05, -1.3314e-05, -1.8055e-05,\n","          2.6676e-05,  7.2747e-06, -1.5411e-05,  3.1210e-05],\n","        [ 1.1567e-05, -3.0434e-06,  2.4812e-05,  3.0217e-05, -1.1928e-05,\n","         -3.3648e-05, -4.3335e-06, -1.6092e-05, -2.7849e-06],\n","        [-9.6943e-07, -1.4740e-05, -9.5844e-06,  1.7335e-05, -5.5086e-06,\n","         -1.4787e-05,  2.7946e-05,  2.2744e-05, -4.7374e-07]])\n","first_linear.bias tensor([ 2.0431e-05, -1.2711e-07,  2.9221e-05,  3.8135e-06,  2.8541e-05,\n","         9.6332e-06, -1.8987e-05, -3.7460e-05, -1.3419e-05,  1.8192e-05])\n","linear_hidden.0.weight tensor([[ 1.2216e-05, -1.0640e-05,  7.9701e-06,  1.0312e-05,  2.5221e-05,\n","          1.6944e-05,  2.0819e-05,  3.6395e-07,  6.0571e-06, -1.6091e-05],\n","        [ 1.0592e-05, -1.2250e-05, -1.7300e-05,  9.9106e-06, -7.6732e-07,\n","          7.3886e-06,  2.6088e-05,  3.6474e-05, -3.4256e-05,  1.4867e-05],\n","        [-9.8008e-06,  2.4785e-05, -3.1910e-05, -7.6474e-06, -2.0934e-05,\n","         -3.2039e-05, -2.2821e-05,  1.2251e-05, -4.3181e-05, -5.3328e-06],\n","        [ 2.5164e-05, -3.5929e-05, -1.5275e-05,  2.2017e-05, -1.8508e-05,\n","          1.6009e-05, -4.0753e-05,  3.1097e-05,  8.7126e-06, -9.9453e-06],\n","        [-4.3826e-05,  1.7204e-05,  1.1824e-05, -1.1653e-07, -8.6419e-06,\n","         -2.9169e-06,  1.6148e-05, -1.1849e-05,  2.9791e-05,  6.0995e-06],\n","        [-9.9129e-06, -1.8424e-05,  1.6536e-05, -4.7289e-06, -1.6106e-05,\n","         -2.3936e-05,  1.1216e-06,  2.2491e-05,  1.3547e-05,  4.7122e-07],\n","        [-9.9397e-06, -1.9843e-05,  3.6716e-06, -2.3406e-05, -1.4767e-05,\n","          2.8102e-06, -2.6021e-06,  8.6508e-07,  1.3179e-05,  1.9897e-05],\n","        [-4.3958e-05, -3.2538e-06,  4.3107e-05,  6.7888e-06, -1.8838e-05,\n","         -2.8812e-05,  2.4359e-05, -3.1141e-05, -2.3743e-05,  9.3185e-06],\n","        [ 1.0825e-05,  1.9802e-05,  5.8735e-06,  2.3650e-05, -5.4140e-06,\n","         -6.2892e-06,  3.9621e-05,  2.5977e-05, -5.6000e-06, -3.0133e-05],\n","        [ 3.4094e-05, -2.6626e-05,  2.7417e-06,  3.5361e-05, -1.8899e-05,\n","          8.4404e-07,  5.1718e-06,  3.0492e-05, -1.5498e-06,  6.9699e-06]])\n","linear_hidden.0.bias tensor([ 4.7172e-04, -1.0733e-04,  2.0176e-07,  6.5859e-06, -1.3612e-03,\n","        -5.3813e-04, -3.3841e-05, -1.1592e-05,  1.9497e-03,  5.7059e-05])\n","linear_output.weight tensor([[-2.7192e-04,  7.5492e-05, -9.1441e-06, -3.3629e-05,  1.7269e-03,\n","         -1.9956e-03,  4.9225e-05, -3.1881e-05, -2.9663e-04,  1.7953e-05]])\n","linear_output.bias tensor([-0.0254])\n","Testing out: \n","batch_size:  1949\n","train_size:  3624\n","n_epochs:  243\n","lr:  0.031672266066625876\n","weight_decay:  7.837422921949513e-05\n","betas0:  0.9721796643378373\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1336, -0.2450,  0.0516],\n","          [ 0.0296, -0.3268, -0.2761],\n","          [-0.1890, -0.1126, -0.2436]]]])\n","conv1.bias tensor([-0.2792])\n","first_linear.weight tensor([[-0.0356, -0.2132, -0.0921, -0.3010, -0.2810,  0.2140, -0.0237, -0.0387,\n","          0.1526],\n","        [-0.1308,  0.1489,  0.2378,  0.1159,  0.3183,  0.2395,  0.0160, -0.3068,\n","         -0.1873],\n","        [-0.1887,  0.2161, -0.2137, -0.1303,  0.0512,  0.0274,  0.1102,  0.1176,\n","          0.0349],\n","        [-0.2136,  0.0863,  0.2395,  0.1335,  0.0795,  0.3082,  0.3029, -0.0836,\n","          0.3239],\n","        [ 0.0441, -0.1250,  0.1176, -0.1462, -0.1613, -0.3330,  0.1486,  0.0798,\n","         -0.1551],\n","        [-0.0977, -0.0998, -0.2829,  0.0605,  0.0942,  0.0866,  0.0812, -0.0892,\n","          0.2785],\n","        [ 0.0568, -0.3264,  0.0790, -0.1728, -0.2311, -0.2047, -0.1628,  0.3330,\n","         -0.0386],\n","        [ 0.2629,  0.3138, -0.1599, -0.2621,  0.1298,  0.0993,  0.0027, -0.0064,\n","          0.3237],\n","        [ 0.1242,  0.2431, -0.0380,  0.0964, -0.2906,  0.2767,  0.2816, -0.3329,\n","          0.1854],\n","        [-0.0142,  0.2804,  0.0694,  0.2608,  0.0890, -0.2270,  0.2564,  0.0820,\n","         -0.0485]])\n","first_linear.bias tensor([ 0.1590, -0.3196, -0.0369, -0.2951, -0.2618,  0.3020,  0.3240, -0.3077,\n","        -0.2739, -0.0036])\n","linear_hidden.0.weight tensor([[ 0.2909, -0.2261,  0.0052, -0.1226,  0.1462,  0.2147,  0.2419,  0.2361,\n","         -0.0125, -0.1756],\n","        [ 0.2388, -0.1976,  0.1836, -0.0680,  0.1819,  0.2783,  0.1533, -0.0363,\n","          0.2639,  0.3016],\n","        [-0.0878, -0.0797, -0.0894, -0.0582, -0.1203,  0.2543, -0.0678, -0.0893,\n","         -0.1873, -0.1568],\n","        [ 0.0296, -0.0682,  0.2979,  0.1308, -0.1569,  0.1791,  0.0995, -0.2523,\n","         -0.2114, -0.0872],\n","        [ 0.0343,  0.2814, -0.1013,  0.1913, -0.1407,  0.1823,  0.2319,  0.2975,\n","         -0.0984,  0.2334],\n","        [-0.1552,  0.1605,  0.2130,  0.1145,  0.0486, -0.1060, -0.0785, -0.0437,\n","         -0.1774,  0.2829],\n","        [ 0.0788,  0.1780,  0.2682,  0.1616,  0.1847, -0.3079, -0.2600, -0.0968,\n","          0.1601, -0.0955],\n","        [ 0.0441, -0.1715,  0.0070, -0.2640, -0.2595,  0.0078, -0.3010, -0.1851,\n","          0.2688,  0.3141],\n","        [-0.1612, -0.3112, -0.0430, -0.1840, -0.0630, -0.2694,  0.2838, -0.2350,\n","         -0.2050,  0.1548],\n","        [-0.1555, -0.0589, -0.3073,  0.2859, -0.0346, -0.2092, -0.0812, -0.0896,\n","         -0.2246, -0.2544]])\n","linear_hidden.0.bias tensor([-0.1441, -0.1263,  0.0724, -0.1102,  0.0013, -0.2573,  0.1630,  0.0978,\n","         0.0394,  0.1680])\n","linear_output.weight tensor([[-0.0187, -0.0393, -0.0921,  0.2701, -0.0431, -0.0516, -0.0181, -0.0060,\n","          0.1642, -0.1807]])\n","linear_output.bias tensor([-0.1832])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1949])) that is different to the input size (torch.Size([1949, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1675])) that is different to the input size (torch.Size([1675, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6932500004768372 \t Validate_Accuracy: 0.5345\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6888712346553802 \t Validate_Accuracy: 0.5\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6816347241401672 \t Validate_Accuracy: 0.541\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6640518009662628 \t Validate_Accuracy: 0.68\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6387974917888641 \t Validate_Accuracy: 0.7245\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.5981154143810272 \t Validate_Accuracy: 0.751\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.5524817109107971 \t Validate_Accuracy: 0.7805\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5009320676326752 \t Validate_Accuracy: 0.795\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.4545120447874069 \t Validate_Accuracy: 0.8075\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.41921354830265045 \t Validate_Accuracy: 0.8155\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.39772625267505646 \t Validate_Accuracy: 0.8135\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.3844502866268158 \t Validate_Accuracy: 0.8085\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.3781917989253998 \t Validate_Accuracy: 0.8225\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.36135339736938477 \t Validate_Accuracy: 0.8355\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.35266976058483124 \t Validate_Accuracy: 0.8285\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.34316885471343994 \t Validate_Accuracy: 0.8435\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3268427848815918 \t Validate_Accuracy: 0.8485\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3174609839916229 \t Validate_Accuracy: 0.85\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.3071739375591278 \t Validate_Accuracy: 0.862\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.29636314511299133 \t Validate_Accuracy: 0.8615\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.28964442014694214 \t Validate_Accuracy: 0.8675\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.27578383684158325 \t Validate_Accuracy: 0.8745\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.27388742566108704 \t Validate_Accuracy: 0.8665\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.2717928886413574 \t Validate_Accuracy: 0.872\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.2664622664451599 \t Validate_Accuracy: 0.8745\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.25681136548519135 \t Validate_Accuracy: 0.8755\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.2527751624584198 \t Validate_Accuracy: 0.883\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.25011199712753296 \t Validate_Accuracy: 0.8875\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.24339140206575394 \t Validate_Accuracy: 0.888\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.23819898813962936 \t Validate_Accuracy: 0.893\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.2356555238366127 \t Validate_Accuracy: 0.892\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.23142047226428986 \t Validate_Accuracy: 0.894\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.23122669011354446 \t Validate_Accuracy: 0.8915\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.2254183292388916 \t Validate_Accuracy: 0.8925\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.22190187871456146 \t Validate_Accuracy: 0.891\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2189485728740692 \t Validate_Accuracy: 0.8955\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.21690989285707474 \t Validate_Accuracy: 0.8965\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.21323972940444946 \t Validate_Accuracy: 0.897\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.21097828447818756 \t Validate_Accuracy: 0.899\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.20742124319076538 \t Validate_Accuracy: 0.901\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.20520775020122528 \t Validate_Accuracy: 0.9005\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.20361410826444626 \t Validate_Accuracy: 0.896\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.20819132775068283 \t Validate_Accuracy: 0.898\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2027759626507759 \t Validate_Accuracy: 0.902\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.20238959789276123 \t Validate_Accuracy: 0.8985\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.20093903690576553 \t Validate_Accuracy: 0.897\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.20534153282642365 \t Validate_Accuracy: 0.905\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.19798412919044495 \t Validate_Accuracy: 0.9035\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.19616376608610153 \t Validate_Accuracy: 0.9015\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.19384873658418655 \t Validate_Accuracy: 0.9055\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.19523459672927856 \t Validate_Accuracy: 0.907\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.19357717782258987 \t Validate_Accuracy: 0.899\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.19300556927919388 \t Validate_Accuracy: 0.902\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.19672514498233795 \t Validate_Accuracy: 0.9\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.20175135880708694 \t Validate_Accuracy: 0.902\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.1923641562461853 \t Validate_Accuracy: 0.9035\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.1961139217019081 \t Validate_Accuracy: 0.8885\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.21681009978055954 \t Validate_Accuracy: 0.901\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.2105085328221321 \t Validate_Accuracy: 0.9025\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2036699801683426 \t Validate_Accuracy: 0.904\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.20190741121768951 \t Validate_Accuracy: 0.9005\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.19351688772439957 \t Validate_Accuracy: 0.904\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.19249416887760162 \t Validate_Accuracy: 0.9035\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.1914074420928955 \t Validate_Accuracy: 0.906\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.19163943827152252 \t Validate_Accuracy: 0.905\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.18918875604867935 \t Validate_Accuracy: 0.9045\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.18910802900791168 \t Validate_Accuracy: 0.9\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.19460750371217728 \t Validate_Accuracy: 0.9045\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.18691480159759521 \t Validate_Accuracy: 0.9045\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.19025643169879913 \t Validate_Accuracy: 0.9025\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.1908663585782051 \t Validate_Accuracy: 0.9075\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.1886395514011383 \t Validate_Accuracy: 0.9045\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.1880161091685295 \t Validate_Accuracy: 0.905\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.18686994165182114 \t Validate_Accuracy: 0.907\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.18698587268590927 \t Validate_Accuracy: 0.906\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.18737328797578812 \t Validate_Accuracy: 0.904\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.18740978837013245 \t Validate_Accuracy: 0.9035\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.18799155950546265 \t Validate_Accuracy: 0.9045\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.1860240325331688 \t Validate_Accuracy: 0.904\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.18565059453248978 \t Validate_Accuracy: 0.9025\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.19202739000320435 \t Validate_Accuracy: 0.9085\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.18583738803863525 \t Validate_Accuracy: 0.9065\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.1869698241353035 \t Validate_Accuracy: 0.907\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.1848059594631195 \t Validate_Accuracy: 0.9075\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.18367481976747513 \t Validate_Accuracy: 0.9075\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.18377715349197388 \t Validate_Accuracy: 0.9065\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.18652266263961792 \t Validate_Accuracy: 0.905\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.18379805982112885 \t Validate_Accuracy: 0.905\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.18371527642011642 \t Validate_Accuracy: 0.9045\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.18460481613874435 \t Validate_Accuracy: 0.9045\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.18493620306253433 \t Validate_Accuracy: 0.9045\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.186844140291214 \t Validate_Accuracy: 0.8995\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.191244438290596 \t Validate_Accuracy: 0.904\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.18596644699573517 \t Validate_Accuracy: 0.9035\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.18585612624883652 \t Validate_Accuracy: 0.9035\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.19273998588323593 \t Validate_Accuracy: 0.9095\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.19286400824785233 \t Validate_Accuracy: 0.9055\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.1894368678331375 \t Validate_Accuracy: 0.902\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.18509967625141144 \t Validate_Accuracy: 0.9065\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.18264612555503845 \t Validate_Accuracy: 0.908\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.18356506526470184 \t Validate_Accuracy: 0.903\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.1841135248541832 \t Validate_Accuracy: 0.905\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.1831427589058876 \t Validate_Accuracy: 0.9065\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.18475250899791718 \t Validate_Accuracy: 0.9045\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.18771032243967056 \t Validate_Accuracy: 0.9045\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.18492484837770462 \t Validate_Accuracy: 0.905\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.18875926733016968 \t Validate_Accuracy: 0.9055\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.18412423133850098 \t Validate_Accuracy: 0.905\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.18267757445573807 \t Validate_Accuracy: 0.911\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.18661388009786606 \t Validate_Accuracy: 0.907\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.18659022450447083 \t Validate_Accuracy: 0.906\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.18665548413991928 \t Validate_Accuracy: 0.8995\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.19248852878808975 \t Validate_Accuracy: 0.9045\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.18311332911252975 \t Validate_Accuracy: 0.904\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.1868860051035881 \t Validate_Accuracy: 0.906\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.18243416398763657 \t Validate_Accuracy: 0.9085\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.18247298151254654 \t Validate_Accuracy: 0.9065\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.18260683119297028 \t Validate_Accuracy: 0.907\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.18000271171331406 \t Validate_Accuracy: 0.906\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.18244829773902893 \t Validate_Accuracy: 0.9075\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.1812640056014061 \t Validate_Accuracy: 0.91\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.180191308259964 \t Validate_Accuracy: 0.9075\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.17964265495538712 \t Validate_Accuracy: 0.908\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.18212498724460602 \t Validate_Accuracy: 0.908\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.1816411018371582 \t Validate_Accuracy: 0.905\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.18403863161802292 \t Validate_Accuracy: 0.9075\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.185627318918705 \t Validate_Accuracy: 0.9065\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.185355544090271 \t Validate_Accuracy: 0.9095\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.181616872549057 \t Validate_Accuracy: 0.9055\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.18088627606630325 \t Validate_Accuracy: 0.9045\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.1792408674955368 \t Validate_Accuracy: 0.909\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.1807931810617447 \t Validate_Accuracy: 0.9105\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.1798667535185814 \t Validate_Accuracy: 0.905\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.17996717244386673 \t Validate_Accuracy: 0.9055\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.1826617568731308 \t Validate_Accuracy: 0.9065\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.1792125105857849 \t Validate_Accuracy: 0.9095\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.17943426966667175 \t Validate_Accuracy: 0.9065\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.18121667206287384 \t Validate_Accuracy: 0.9105\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.1785668432712555 \t Validate_Accuracy: 0.908\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.1805863231420517 \t Validate_Accuracy: 0.9065\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.1787668839097023 \t Validate_Accuracy: 0.9085\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.18047871440649033 \t Validate_Accuracy: 0.909\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.17884687334299088 \t Validate_Accuracy: 0.907\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.18348713964223862 \t Validate_Accuracy: 0.9095\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.17885910719633102 \t Validate_Accuracy: 0.901\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.19530683755874634 \t Validate_Accuracy: 0.9065\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.18428350985050201 \t Validate_Accuracy: 0.9025\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.18330401927232742 \t Validate_Accuracy: 0.9025\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.18800681829452515 \t Validate_Accuracy: 0.904\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.1840006336569786 \t Validate_Accuracy: 0.908\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.18259509652853012 \t Validate_Accuracy: 0.903\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.18118758499622345 \t Validate_Accuracy: 0.907\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.18234699964523315 \t Validate_Accuracy: 0.9065\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.18248187750577927 \t Validate_Accuracy: 0.903\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.18019425123929977 \t Validate_Accuracy: 0.908\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.18266334384679794 \t Validate_Accuracy: 0.9075\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.17828556895256042 \t Validate_Accuracy: 0.907\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.1815921887755394 \t Validate_Accuracy: 0.9075\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.17826303839683533 \t Validate_Accuracy: 0.908\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.18318632990121841 \t Validate_Accuracy: 0.9045\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.18213564157485962 \t Validate_Accuracy: 0.9025\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.18323085457086563 \t Validate_Accuracy: 0.9015\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.1848064661026001 \t Validate_Accuracy: 0.8985\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.18406318873167038 \t Validate_Accuracy: 0.8965\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.18961197137832642 \t Validate_Accuracy: 0.897\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.19773581624031067 \t Validate_Accuracy: 0.904\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.18903730809688568 \t Validate_Accuracy: 0.902\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.18613869696855545 \t Validate_Accuracy: 0.903\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.1852484568953514 \t Validate_Accuracy: 0.9005\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.18412131816148758 \t Validate_Accuracy: 0.9065\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.18436895310878754 \t Validate_Accuracy: 0.903\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.1812828630208969 \t Validate_Accuracy: 0.901\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.18300645053386688 \t Validate_Accuracy: 0.9035\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.18571198731660843 \t Validate_Accuracy: 0.909\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.1795462667942047 \t Validate_Accuracy: 0.902\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.18881855905056 \t Validate_Accuracy: 0.9045\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.18726161867380142 \t Validate_Accuracy: 0.907\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.1874842569231987 \t Validate_Accuracy: 0.8985\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.19048362225294113 \t Validate_Accuracy: 0.9055\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.18043381720781326 \t Validate_Accuracy: 0.9075\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.18017342686653137 \t Validate_Accuracy: 0.906\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.17833925038576126 \t Validate_Accuracy: 0.907\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.17936613410711288 \t Validate_Accuracy: 0.9055\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.18128549307584763 \t Validate_Accuracy: 0.909\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.17906218767166138 \t Validate_Accuracy: 0.903\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.1797267273068428 \t Validate_Accuracy: 0.907\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.18293225020170212 \t Validate_Accuracy: 0.9095\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.18112250417470932 \t Validate_Accuracy: 0.902\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.17914913594722748 \t Validate_Accuracy: 0.905\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.17977388203144073 \t Validate_Accuracy: 0.9065\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.1775432974100113 \t Validate_Accuracy: 0.909\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.17898307740688324 \t Validate_Accuracy: 0.905\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.1775592416524887 \t Validate_Accuracy: 0.9075\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.17811336368322372 \t Validate_Accuracy: 0.908\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.17863617092370987 \t Validate_Accuracy: 0.9075\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.1771550476551056 \t Validate_Accuracy: 0.9095\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.17858440428972244 \t Validate_Accuracy: 0.9085\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.1774589791893959 \t Validate_Accuracy: 0.9035\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.1805676519870758 \t Validate_Accuracy: 0.903\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.17979160696268082 \t Validate_Accuracy: 0.896\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.18880823999643326 \t Validate_Accuracy: 0.8965\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.1929672583937645 \t Validate_Accuracy: 0.9045\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.18212690204381943 \t Validate_Accuracy: 0.9065\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.18312285095453262 \t Validate_Accuracy: 0.9055\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.1777261346578598 \t Validate_Accuracy: 0.9055\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.176978200674057 \t Validate_Accuracy: 0.907\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.17796356230974197 \t Validate_Accuracy: 0.9025\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.18034550547599792 \t Validate_Accuracy: 0.9095\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.17929431051015854 \t Validate_Accuracy: 0.904\n","epoch 210\n","Epoch: 210 \t Train Loss: 0.17784669995307922 \t Validate_Accuracy: 0.908\n","epoch 211\n","Epoch: 211 \t Train Loss: 0.18094311654567719 \t Validate_Accuracy: 0.904\n","epoch 212\n","Epoch: 212 \t Train Loss: 0.180420882999897 \t Validate_Accuracy: 0.9035\n","epoch 213\n","Epoch: 213 \t Train Loss: 0.17725781351327896 \t Validate_Accuracy: 0.9065\n","epoch 214\n","Epoch: 214 \t Train Loss: 0.17736664414405823 \t Validate_Accuracy: 0.909\n","epoch 215\n","Epoch: 215 \t Train Loss: 0.1776503473520279 \t Validate_Accuracy: 0.907\n","epoch 216\n","Epoch: 216 \t Train Loss: 0.1790412738919258 \t Validate_Accuracy: 0.908\n","epoch 217\n","Epoch: 217 \t Train Loss: 0.17906504124403 \t Validate_Accuracy: 0.9095\n","epoch 218\n","Epoch: 218 \t Train Loss: 0.1762639805674553 \t Validate_Accuracy: 0.9055\n","epoch 219\n","Epoch: 219 \t Train Loss: 0.1771642044186592 \t Validate_Accuracy: 0.9055\n","epoch 220\n","Epoch: 220 \t Train Loss: 0.17616697400808334 \t Validate_Accuracy: 0.91\n","epoch 221\n","Epoch: 221 \t Train Loss: 0.17828620970249176 \t Validate_Accuracy: 0.9065\n","epoch 222\n","Epoch: 222 \t Train Loss: 0.18031111359596252 \t Validate_Accuracy: 0.901\n","epoch 223\n","Epoch: 223 \t Train Loss: 0.1842619776725769 \t Validate_Accuracy: 0.9015\n","epoch 224\n","Epoch: 224 \t Train Loss: 0.18528500944375992 \t Validate_Accuracy: 0.8995\n","epoch 225\n","Epoch: 225 \t Train Loss: 0.1908825784921646 \t Validate_Accuracy: 0.904\n","epoch 226\n","Epoch: 226 \t Train Loss: 0.18440785259008408 \t Validate_Accuracy: 0.9075\n","epoch 227\n","Epoch: 227 \t Train Loss: 0.18487537652254105 \t Validate_Accuracy: 0.9\n","epoch 228\n","Epoch: 228 \t Train Loss: 0.18187522888183594 \t Validate_Accuracy: 0.9005\n","epoch 229\n","Epoch: 229 \t Train Loss: 0.1813398003578186 \t Validate_Accuracy: 0.9055\n","epoch 230\n","Epoch: 230 \t Train Loss: 0.17778823524713516 \t Validate_Accuracy: 0.9035\n","epoch 231\n","Epoch: 231 \t Train Loss: 0.18031372129917145 \t Validate_Accuracy: 0.9025\n","epoch 232\n","Epoch: 232 \t Train Loss: 0.1831211969256401 \t Validate_Accuracy: 0.905\n","epoch 233\n","Epoch: 233 \t Train Loss: 0.17953340709209442 \t Validate_Accuracy: 0.9075\n","epoch 234\n","Epoch: 234 \t Train Loss: 0.17989876866340637 \t Validate_Accuracy: 0.905\n","epoch 235\n","Epoch: 235 \t Train Loss: 0.18003693968057632 \t Validate_Accuracy: 0.904\n","epoch 236\n","Epoch: 236 \t Train Loss: 0.18093642592430115 \t Validate_Accuracy: 0.904\n","epoch 237\n","Epoch: 237 \t Train Loss: 0.18092182278633118 \t Validate_Accuracy: 0.9005\n","epoch 238\n","Epoch: 238 \t Train Loss: 0.18252183496952057 \t Validate_Accuracy: 0.9015\n","epoch 239\n","Epoch: 239 \t Train Loss: 0.1845172941684723 \t Validate_Accuracy: 0.9005\n","epoch 240\n","Epoch: 240 \t Train Loss: 0.18443208187818527 \t Validate_Accuracy: 0.9025\n","epoch 241\n","Epoch: 241 \t Train Loss: 0.18135787546634674 \t Validate_Accuracy: 0.9015\n","epoch 242\n","Epoch: 242 \t Train Loss: 0.18303948640823364 \t Validate_Accuracy: 0.9025\n","epoch 243\n","Epoch: 243 \t Train Loss: 0.1837371215224266 \t Validate_Accuracy: 0.9055\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1143, -0.0485, -0.1361],\n","          [-0.0538,  0.0226, -0.0350],\n","          [-0.1138, -0.0424, -0.1261]]]])\n","conv1.bias tensor([0.0310])\n","first_linear.weight tensor([[-0.0632, -0.2192, -0.2408, -0.2339,  0.2430,  0.0653, -0.2199, -0.0781,\n","          0.0973],\n","        [-0.1691, -0.0851, -0.7890,  0.0584,  1.2886,  1.0556,  0.1698, -0.6181,\n","         -0.5298],\n","        [ 0.0266,  0.5899, -0.3054, -1.0092, -0.0134,  0.6400,  1.0656, -0.1552,\n","         -0.7513],\n","        [ 0.0706,  0.2638,  0.0801,  0.1340,  0.0813,  0.3074,  0.1645,  0.1388,\n","         -0.0863],\n","        [ 0.4594, -1.0184,  0.8660, -0.3646,  0.3568, -0.3812,  0.8580, -0.5105,\n","          0.1075],\n","        [-0.1070, -0.2853, -0.4818, -0.4729,  0.2615, -0.2673,  0.5151, -1.4872,\n","          1.1361],\n","        [ 1.1460, -1.5730,  0.1852, -0.6888,  0.0774,  0.5358, -0.5462,  0.5567,\n","         -0.6083],\n","        [ 0.6470,  0.3779, -0.0897, -1.7797,  0.5856, -0.3659,  0.1224,  0.1177,\n","          0.3312],\n","        [ 0.1876,  0.5886, -0.5234,  0.2955, -1.5850,  1.3044,  0.3473,  0.1741,\n","         -0.1670],\n","        [ 0.2070,  0.4026,  0.3491,  0.0857,  0.2210,  0.4212,  0.2632,  0.3436,\n","          0.0406]])\n","first_linear.bias tensor([ 1.3701, -0.8097,  0.6968, -1.3567, -0.7044,  0.7798,  1.0069, -0.8882,\n","        -0.9056, -1.1014])\n","linear_hidden.0.weight tensor([[ 0.4882, -0.3124, -0.0118, -0.4093, -0.0768,  0.4009,  0.1442, -0.0815,\n","         -0.3614, -0.4201],\n","        [ 1.0882,  0.3247, -0.6578, -0.8267,  0.8693, -0.4010, -0.3429,  0.7365,\n","          0.9215, -0.5802],\n","        [ 0.3824, -0.3103, -0.0541, -0.4016, -0.0523,  0.5240,  0.0842,  0.0066,\n","         -0.4395, -0.4541],\n","        [-0.7825, -0.4941,  0.9000,  1.0430, -0.8583,  0.6104,  0.4185, -0.7709,\n","         -0.7601,  0.6873],\n","        [-0.6627,  0.0945,  0.8468,  0.6224,  0.0091,  0.7948,  1.0040, -0.3228,\n","         -0.8282,  0.9165],\n","        [ 0.6229,  0.8309, -0.6016, -0.6115,  0.9856, -0.7046, -0.5530,  0.7434,\n","         -0.5303, -0.3823],\n","        [ 0.8406,  0.8934, -0.4003, -0.6257,  0.8683, -0.9549, -0.7472,  0.7664,\n","          0.8584, -0.8173],\n","        [ 0.7979,  0.4172, -0.7493, -0.7517,  0.1723, -0.8626, -0.6766,  0.4616,\n","          1.0044, -0.4967],\n","        [-0.8513, -1.1578, -0.4072,  0.3674, -1.5319, -0.2410,  1.0234, -1.2528,\n","         -0.7246,  0.7212],\n","        [ 0.6664,  0.7445, -0.7987, -0.5907,  0.7651, -0.7955, -0.6747,  0.7229,\n","          0.5440, -1.0773]])\n","linear_hidden.0.bias tensor([ 0.4555,  0.4794,  0.4848, -0.7423, -0.4648,  0.1527,  0.8374,  0.4965,\n","        -0.2044,  0.6116])\n","linear_output.weight tensor([[-0.9419, -1.6673, -0.8267,  1.9432,  1.5846, -1.1946, -1.7320, -1.5971,\n","          1.6416, -1.6294]])\n","linear_output.bias tensor([-0.5238])\n","Testing out: \n","batch_size:  1918\n","train_size:  4158\n","n_epochs:  156\n","lr:  0.03034030157844494\n","weight_decay:  0.0005049023440983478\n","betas0:  0.7999999999999999\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.2492, -0.2658, -0.0374],\n","          [ 0.2652,  0.0826,  0.2256],\n","          [-0.1404,  0.2955, -0.2455]]]])\n","conv1.bias tensor([-0.1691])\n","first_linear.weight tensor([[ 0.0848, -0.2251,  0.1784,  0.1259, -0.1054,  0.3051, -0.2378, -0.2276,\n","          0.0248],\n","        [ 0.1203,  0.2249,  0.0009,  0.0472, -0.2555,  0.1864,  0.2257, -0.2176,\n","         -0.0192],\n","        [ 0.0756, -0.2108,  0.1566,  0.2796,  0.2979,  0.2035,  0.1296,  0.1367,\n","         -0.0254],\n","        [-0.1572, -0.2635, -0.1795, -0.0633, -0.0534, -0.2909, -0.0737,  0.0841,\n","         -0.2550],\n","        [-0.1342,  0.1967,  0.0398, -0.2141,  0.2764,  0.0152,  0.0788, -0.1352,\n","          0.0571],\n","        [ 0.1919, -0.2515, -0.1178,  0.1674,  0.2412,  0.0722, -0.0485,  0.1774,\n","          0.1107],\n","        [ 0.2988, -0.0917, -0.2375,  0.0600, -0.2701, -0.3031, -0.0047,  0.0569,\n","          0.1555],\n","        [-0.2459,  0.1959, -0.3245, -0.0383, -0.1740,  0.1775,  0.3006,  0.2829,\n","          0.1104],\n","        [-0.2019,  0.3080,  0.2652,  0.0723,  0.2001, -0.2971,  0.1289,  0.0037,\n","         -0.0221],\n","        [ 0.1548,  0.1337, -0.2910,  0.2475,  0.1604, -0.1881, -0.3139,  0.2483,\n","         -0.0383]])\n","first_linear.bias tensor([-0.1560,  0.2041,  0.1409,  0.0888, -0.1733,  0.1129,  0.0681, -0.1760,\n","        -0.0630,  0.0098])\n","linear_hidden.0.weight tensor([[ 0.3122, -0.0927,  0.2914,  0.2663,  0.2378,  0.2404, -0.2085, -0.1380,\n","          0.0604, -0.1048],\n","        [ 0.0083, -0.1641,  0.3077,  0.0418, -0.1675, -0.0776,  0.2538,  0.1379,\n","         -0.2598,  0.2251],\n","        [ 0.1041, -0.0160, -0.2429,  0.1254,  0.2989,  0.3015, -0.2627,  0.2490,\n","          0.1301, -0.3024],\n","        [ 0.2130, -0.1834, -0.0982, -0.3016, -0.1625, -0.0671,  0.0492, -0.1247,\n","         -0.0202, -0.0665],\n","        [-0.2497,  0.2879,  0.0255,  0.2937,  0.2013, -0.2900,  0.0367, -0.1562,\n","         -0.1189,  0.2515],\n","        [ 0.0069,  0.1416, -0.0629, -0.1407, -0.0007,  0.2734, -0.2218,  0.2150,\n","         -0.3094,  0.1440],\n","        [-0.1959, -0.1975,  0.0934,  0.1623, -0.0113, -0.2815,  0.1685, -0.0219,\n","          0.0436, -0.1479],\n","        [-0.0654,  0.2749,  0.3068, -0.2531, -0.1121, -0.1270, -0.2412, -0.3099,\n","          0.2053,  0.0251],\n","        [ 0.2667, -0.1869,  0.1326, -0.2355, -0.1029,  0.0917, -0.0105,  0.2721,\n","         -0.3106, -0.0558],\n","        [-0.2047,  0.0431,  0.0182,  0.0640, -0.1776,  0.2751, -0.2891,  0.0834,\n","         -0.1437,  0.3023]])\n","linear_hidden.0.bias tensor([-0.1601, -0.1958, -0.2523,  0.2383,  0.0456, -0.1804,  0.0430, -0.2580,\n","        -0.0358,  0.0531])\n","linear_output.weight tensor([[ 0.1702, -0.0394,  0.0460,  0.2890, -0.0211, -0.1857, -0.3147,  0.1535,\n","          0.2820,  0.0857]])\n","linear_output.bias tensor([-0.0940])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.6971251567204794 \t Validate_Accuracy: 0.502\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1918])) that is different to the input size (torch.Size([1918, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([322])) that is different to the input size (torch.Size([322, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6925364931424459 \t Validate_Accuracy: 0.5035\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.695292075475057 \t Validate_Accuracy: 0.4955\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6930740078290304 \t Validate_Accuracy: 0.4895\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6933409372965494 \t Validate_Accuracy: 0.499\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6925608913103739 \t Validate_Accuracy: 0.5115\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6918178002039591 \t Validate_Accuracy: 0.4925\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6918812990188599 \t Validate_Accuracy: 0.4845\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6910069187482198 \t Validate_Accuracy: 0.511\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6886135141054789 \t Validate_Accuracy: 0.531\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6853172183036804 \t Validate_Accuracy: 0.56\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6828149358431498 \t Validate_Accuracy: 0.602\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6630253593126932 \t Validate_Accuracy: 0.7045\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6286829511324564 \t Validate_Accuracy: 0.8095\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.5650298992792765 \t Validate_Accuracy: 0.8135\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.4825517733891805 \t Validate_Accuracy: 0.8165\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.44308361411094666 \t Validate_Accuracy: 0.814\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.4096881151199341 \t Validate_Accuracy: 0.8225\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.3957458237806956 \t Validate_Accuracy: 0.819\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.3815983633200328 \t Validate_Accuracy: 0.8255\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3723991811275482 \t Validate_Accuracy: 0.829\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3655600349108378 \t Validate_Accuracy: 0.821\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3629622161388397 \t Validate_Accuracy: 0.8425\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.35177069902420044 \t Validate_Accuracy: 0.8385\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.34403473138809204 \t Validate_Accuracy: 0.8415\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.33280322949091595 \t Validate_Accuracy: 0.843\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3469088673591614 \t Validate_Accuracy: 0.854\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.33084651827812195 \t Validate_Accuracy: 0.841\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.32747910420099896 \t Validate_Accuracy: 0.8405\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.32381481925646466 \t Validate_Accuracy: 0.851\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.2960854023694992 \t Validate_Accuracy: 0.8565\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.32128749291102093 \t Validate_Accuracy: 0.853\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.3101767996946971 \t Validate_Accuracy: 0.838\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.31686123212178546 \t Validate_Accuracy: 0.8505\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.3031461238861084 \t Validate_Accuracy: 0.8475\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.31084203720092773 \t Validate_Accuracy: 0.856\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.3038785358270009 \t Validate_Accuracy: 0.846\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.30309293667475384 \t Validate_Accuracy: 0.8545\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.30172858635584515 \t Validate_Accuracy: 0.849\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2937205930550893 \t Validate_Accuracy: 0.856\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.29511407017707825 \t Validate_Accuracy: 0.855\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2911263306935628 \t Validate_Accuracy: 0.85\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2932839592297872 \t Validate_Accuracy: 0.8545\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.29486080010732013 \t Validate_Accuracy: 0.8555\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.30329317847887677 \t Validate_Accuracy: 0.8515\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2963302830855052 \t Validate_Accuracy: 0.856\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.2711467792590459 \t Validate_Accuracy: 0.859\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.29201172788937885 \t Validate_Accuracy: 0.857\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.28461042046546936 \t Validate_Accuracy: 0.8505\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.28007108966509503 \t Validate_Accuracy: 0.86\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2817516128222148 \t Validate_Accuracy: 0.8575\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.28385395805040997 \t Validate_Accuracy: 0.8515\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.28601767619450885 \t Validate_Accuracy: 0.855\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2921561698118846 \t Validate_Accuracy: 0.8545\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.29769550760587055 \t Validate_Accuracy: 0.8605\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.30952222148577374 \t Validate_Accuracy: 0.8425\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.29140997926394147 \t Validate_Accuracy: 0.8605\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2869853079319 \t Validate_Accuracy: 0.8625\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.27834861477216083 \t Validate_Accuracy: 0.866\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.29536770780881244 \t Validate_Accuracy: 0.864\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.27585790554682416 \t Validate_Accuracy: 0.865\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.28022822737693787 \t Validate_Accuracy: 0.858\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.27275296052296955 \t Validate_Accuracy: 0.8705\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.28755417466163635 \t Validate_Accuracy: 0.8575\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2831827600797017 \t Validate_Accuracy: 0.8605\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.27413148681322735 \t Validate_Accuracy: 0.866\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.2696710030237834 \t Validate_Accuracy: 0.865\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.27506693204243976 \t Validate_Accuracy: 0.8595\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.27403920888900757 \t Validate_Accuracy: 0.8725\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.28545641899108887 \t Validate_Accuracy: 0.864\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.276930292447408 \t Validate_Accuracy: 0.8685\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.2673308650652568 \t Validate_Accuracy: 0.8715\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.26904404163360596 \t Validate_Accuracy: 0.868\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.24763614435990652 \t Validate_Accuracy: 0.875\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.27486129601796466 \t Validate_Accuracy: 0.871\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.26546144485473633 \t Validate_Accuracy: 0.8595\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.2616126934687297 \t Validate_Accuracy: 0.8665\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.25960180163383484 \t Validate_Accuracy: 0.873\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2585372080405553 \t Validate_Accuracy: 0.8785\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2658475438753764 \t Validate_Accuracy: 0.8645\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.27243029077847797 \t Validate_Accuracy: 0.8565\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.29116501410802204 \t Validate_Accuracy: 0.8565\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2720351964235306 \t Validate_Accuracy: 0.869\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.26171136895815533 \t Validate_Accuracy: 0.865\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.26722416281700134 \t Validate_Accuracy: 0.868\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.257920374472936 \t Validate_Accuracy: 0.8715\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2691120207309723 \t Validate_Accuracy: 0.868\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.26775698363780975 \t Validate_Accuracy: 0.864\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.2553812811772029 \t Validate_Accuracy: 0.87\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2568783015012741 \t Validate_Accuracy: 0.8685\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.26712257663408917 \t Validate_Accuracy: 0.8705\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2700860599676768 \t Validate_Accuracy: 0.8705\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.26573512951533 \t Validate_Accuracy: 0.8615\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.25475968917210895 \t Validate_Accuracy: 0.866\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.26056426763534546 \t Validate_Accuracy: 0.869\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.24759222567081451 \t Validate_Accuracy: 0.8765\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.24919691681861877 \t Validate_Accuracy: 0.875\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2573997875054677 \t Validate_Accuracy: 0.871\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.25447405378023785 \t Validate_Accuracy: 0.868\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.257538378238678 \t Validate_Accuracy: 0.868\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2696437289317449 \t Validate_Accuracy: 0.871\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.24557082851727804 \t Validate_Accuracy: 0.864\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.25121281544367474 \t Validate_Accuracy: 0.8735\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.25878116488456726 \t Validate_Accuracy: 0.8695\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2580419182777405 \t Validate_Accuracy: 0.8765\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2706923186779022 \t Validate_Accuracy: 0.8685\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2525751789410909 \t Validate_Accuracy: 0.8715\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2664017975330353 \t Validate_Accuracy: 0.868\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2472524493932724 \t Validate_Accuracy: 0.8755\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2581942280133565 \t Validate_Accuracy: 0.8655\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.25157032410303753 \t Validate_Accuracy: 0.876\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.2585589239994685 \t Validate_Accuracy: 0.871\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2520669847726822 \t Validate_Accuracy: 0.877\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.255654772122701 \t Validate_Accuracy: 0.876\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.24317369361718497 \t Validate_Accuracy: 0.878\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.2359246015548706 \t Validate_Accuracy: 0.8855\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.23729253808657327 \t Validate_Accuracy: 0.8725\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.23570166528224945 \t Validate_Accuracy: 0.88\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.23648273944854736 \t Validate_Accuracy: 0.87\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.22737551728884378 \t Validate_Accuracy: 0.8855\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.24082662661870322 \t Validate_Accuracy: 0.8855\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.2548205554485321 \t Validate_Accuracy: 0.883\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.25396499037742615 \t Validate_Accuracy: 0.8745\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.23840401073296866 \t Validate_Accuracy: 0.8835\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.23782949646313986 \t Validate_Accuracy: 0.882\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.23763637244701385 \t Validate_Accuracy: 0.877\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.21405644218126932 \t Validate_Accuracy: 0.8865\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.23866329590479532 \t Validate_Accuracy: 0.881\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23207445442676544 \t Validate_Accuracy: 0.883\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.23132182161013284 \t Validate_Accuracy: 0.8765\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.24763384958108267 \t Validate_Accuracy: 0.8835\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.22465425233046213 \t Validate_Accuracy: 0.879\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.22779079775015512 \t Validate_Accuracy: 0.8865\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.2442863086859385 \t Validate_Accuracy: 0.885\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.22752953072388968 \t Validate_Accuracy: 0.8845\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.216364194949468 \t Validate_Accuracy: 0.8835\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.22520758211612701 \t Validate_Accuracy: 0.883\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.23101859788099924 \t Validate_Accuracy: 0.8885\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.22014722228050232 \t Validate_Accuracy: 0.8795\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.24020084738731384 \t Validate_Accuracy: 0.882\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.23398240904013315 \t Validate_Accuracy: 0.873\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.24086899061997732 \t Validate_Accuracy: 0.8845\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.2299560010433197 \t Validate_Accuracy: 0.878\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.23720763127009073 \t Validate_Accuracy: 0.878\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.23062420388062796 \t Validate_Accuracy: 0.8875\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.22244286040465036 \t Validate_Accuracy: 0.885\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.22672944267590842 \t Validate_Accuracy: 0.8845\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.24091671903928122 \t Validate_Accuracy: 0.8865\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.22426560521125793 \t Validate_Accuracy: 0.8845\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.22751426696777344 \t Validate_Accuracy: 0.8885\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.21340630948543549 \t Validate_Accuracy: 0.8855\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.21534030139446259 \t Validate_Accuracy: 0.8825\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.22149076561133066 \t Validate_Accuracy: 0.8845\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.2274387131134669 \t Validate_Accuracy: 0.8895\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.2277605136235555 \t Validate_Accuracy: 0.8845\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.24345965683460236 \t Validate_Accuracy: 0.8865\n","model parameters! \n","\n","conv1.weight tensor([[[[0.1643, 0.0594, 0.1535],\n","          [0.0744, 0.0008, 0.0666],\n","          [0.1143, 0.0512, 0.1330]]]])\n","conv1.bias tensor([0.0166])\n","first_linear.weight tensor([[-0.0386, -0.1389, -0.1547, -0.3001, -0.3019, -0.4566, -0.3295, -0.4723,\n","         -0.3128],\n","        [-0.0325,  0.0526,  0.0282,  0.1986,  0.0849,  0.4819,  0.1492,  0.2341,\n","          0.1909],\n","        [-0.5314, -0.4706, -0.5033, -0.0088, -0.3543, -0.0292, -0.2014, -0.2101,\n","         -0.1184],\n","        [ 0.6506,  0.1974, -0.5071, -0.9882, -0.4506,  1.1242,  0.2677,  0.6996,\n","         -0.9788],\n","        [-0.1283, -0.4021, -0.6553,  0.1948, -0.3166, -0.3133,  0.3634,  0.4440,\n","          0.0816],\n","        [ 0.2086, -0.2097, -0.2509,  0.5933, -0.3135,  1.5256,  0.2492, -0.1160,\n","         -0.4037],\n","        [ 0.6427, -1.6229,  0.5703,  0.2571,  0.5023, -0.2425, -0.4069,  0.1311,\n","         -0.0905],\n","        [-0.7554,  0.5086, -0.3265,  1.5164, -1.0558,  0.4293, -0.4959,  0.4160,\n","         -0.1367],\n","        [-0.1961, -0.0794, -0.0847, -0.1344, -0.1454, -0.2097, -0.1014, -0.1029,\n","         -0.0909],\n","        [-0.4346,  0.1055, -0.5599, -0.4557,  0.4866,  0.2508,  0.9804, -1.3929,\n","          0.4162]])\n","first_linear.bias tensor([-1.1642, -0.8051, -1.0354, -0.9952, -0.9337,  0.7485, -0.9726, -1.0534,\n","         1.3336, -0.8991])\n","linear_hidden.0.weight tensor([[ 0.2493, -0.3445,  0.2607, -0.5258,  0.2633,  0.3233, -0.2909, -0.1310,\n","         -0.5788, -0.3142],\n","        [ 0.3747,  0.2623,  0.2024, -0.2938,  0.1709,  0.5897, -0.6431, -0.5670,\n","         -0.5871, -0.4006],\n","        [-0.4554, -0.2752, -0.2163,  0.2405, -0.2147, -0.6146,  0.6923,  0.6393,\n","          0.6864,  0.4715],\n","        [ 0.4991, -0.3853, -0.2346, -0.8918,  0.3127, -0.1859,  0.6256, -0.8287,\n","         -0.4612, -0.7220],\n","        [-0.5432, -0.4766, -0.2397,  0.5777, -0.1593, -0.3645,  0.4178,  0.5619,\n","          0.7339,  0.4773],\n","        [-0.4348, -0.1165, -0.1392,  0.5222, -0.1849, -0.3808,  0.1115,  0.4632,\n","          0.5823,  0.4669],\n","        [-0.5799, -0.3452, -0.1787,  0.5843, -0.2275, -0.5293,  0.6015,  0.6780,\n","          0.8173,  0.5206],\n","        [ 0.3917,  0.4812,  0.2674, -0.5328,  0.1094,  0.3089, -0.3364, -0.5015,\n","         -0.7109, -0.4406],\n","        [ 0.5771,  0.3837,  0.2699, -0.6069,  0.1023,  0.4071, -0.4694, -0.6201,\n","         -0.8772, -0.5043],\n","        [-0.5696, -0.3260, -0.2371,  0.3262, -0.1934, -0.6094,  0.7119,  0.7225,\n","          0.7630,  0.4812]])\n","linear_hidden.0.bias tensor([-0.6072, -0.1167, -0.0108,  0.2037,  0.0661,  0.0372,  0.0661, -0.2390,\n","        -0.0138, -0.0608])\n","linear_output.weight tensor([[ 0.9767,  1.0214, -1.1118,  1.0985, -1.1735, -0.6371, -1.4380,  1.1060,\n","          1.2624, -1.2891]])\n","linear_output.bias tensor([-0.3233])\n","Testing out: \n","batch_size:  2931\n","train_size:  2309\n","n_epochs:  243\n","lr:  0.011591738504744735\n","weight_decay:  0.0004961830114386383\n","betas0:  0.9992189277737973\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.0205,  0.0465, -0.1383],\n","          [-0.1503,  0.0043,  0.1715],\n","          [ 0.0308,  0.1694,  0.1911]]]])\n","conv1.bias tensor([0.0767])\n","first_linear.weight tensor([[ 0.2166, -0.1936, -0.0236, -0.0683,  0.0864,  0.2738,  0.2704,  0.2072,\n","         -0.1156],\n","        [-0.0668,  0.0238, -0.0326, -0.0121, -0.1970, -0.0182,  0.0993, -0.2755,\n","          0.2479],\n","        [-0.2782, -0.2654,  0.0924,  0.2704, -0.1036,  0.2676,  0.0957, -0.2056,\n","         -0.2365],\n","        [ 0.3019,  0.3285, -0.1371,  0.1600, -0.0744, -0.1363,  0.1472,  0.1764,\n","         -0.2286],\n","        [ 0.2880,  0.2967, -0.1615,  0.3325, -0.1445, -0.2906, -0.2830, -0.2395,\n","         -0.1384],\n","        [-0.0906,  0.1478,  0.2741, -0.0426, -0.1065, -0.2835, -0.0161,  0.2800,\n","         -0.0051],\n","        [-0.0403, -0.0488,  0.1614, -0.0210,  0.0450, -0.2095,  0.2742,  0.3176,\n","          0.0911],\n","        [-0.1080, -0.1883, -0.1970, -0.0240,  0.1208, -0.3229,  0.1316, -0.2455,\n","         -0.0828],\n","        [-0.2375, -0.0120, -0.1809,  0.0202,  0.1250,  0.0517,  0.1623,  0.2198,\n","         -0.2881],\n","        [-0.2838, -0.0029, -0.2389,  0.2247,  0.2268, -0.0583,  0.2136,  0.0464,\n","          0.2965]])\n","first_linear.bias tensor([-0.0568, -0.1881,  0.2118, -0.0262,  0.1866, -0.0159,  0.0286, -0.2257,\n","         0.0739,  0.2749])\n","linear_hidden.0.weight tensor([[ 0.0792, -0.0439, -0.2309, -0.0096,  0.0042, -0.2596,  0.1897,  0.1514,\n","         -0.3051,  0.1646],\n","        [ 0.1507,  0.2005,  0.1023, -0.1814,  0.1512,  0.0763, -0.1505,  0.3059,\n","          0.1653,  0.0477],\n","        [-0.1474,  0.1377,  0.1789,  0.3092, -0.0947,  0.1008, -0.1283, -0.0533,\n","          0.1154,  0.3047],\n","        [-0.0894,  0.2940,  0.1184,  0.1789,  0.0143,  0.2568, -0.1184, -0.0005,\n","          0.3020,  0.0368],\n","        [-0.0737,  0.1910, -0.2542, -0.0250,  0.0098,  0.1965,  0.3140, -0.0879,\n","          0.1727,  0.3010],\n","        [-0.2133, -0.2858, -0.0584, -0.0768,  0.1006,  0.2911,  0.2983,  0.1469,\n","         -0.1726, -0.0670],\n","        [-0.2160, -0.0980, -0.0670,  0.0277,  0.1167,  0.2329,  0.1823, -0.3023,\n","          0.2093,  0.0145],\n","        [ 0.2324,  0.1684,  0.3156,  0.2563,  0.0262,  0.1722, -0.0632,  0.1031,\n","         -0.1758, -0.2818],\n","        [-0.2260, -0.2638,  0.0067, -0.1720,  0.2052,  0.0295,  0.3031,  0.0630,\n","         -0.2421, -0.1032],\n","        [ 0.1341,  0.2326, -0.0488, -0.1254, -0.1997,  0.2540,  0.0282,  0.1195,\n","          0.2474,  0.0367]])\n","linear_hidden.0.bias tensor([ 0.1286, -0.2158,  0.3124,  0.1703,  0.2098,  0.1255,  0.3092, -0.2611,\n","         0.0416, -0.1875])\n","linear_output.weight tensor([[ 0.2492,  0.3094, -0.1666,  0.2755, -0.2920, -0.2079,  0.1696,  0.1216,\n","         -0.2202,  0.2338]])\n","linear_output.bias tensor([-0.0282])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7010908722877502 \t Validate_Accuracy: 0.503\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6957131624221802 \t Validate_Accuracy: 0.474\n","epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2309])) that is different to the input size (torch.Size([2309, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3 \t Train Loss: 0.6931222677230835 \t Validate_Accuracy: 0.5115\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6923925280570984 \t Validate_Accuracy: 0.516\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6924601793289185 \t Validate_Accuracy: 0.5185\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6924355626106262 \t Validate_Accuracy: 0.4995\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.691952645778656 \t Validate_Accuracy: 0.4605\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.69105064868927 \t Validate_Accuracy: 0.466\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6898775696754456 \t Validate_Accuracy: 0.479\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.688535749912262 \t Validate_Accuracy: 0.4945\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6870445013046265 \t Validate_Accuracy: 0.518\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6853300333023071 \t Validate_Accuracy: 0.538\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.683255672454834 \t Validate_Accuracy: 0.5505\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6806706190109253 \t Validate_Accuracy: 0.5575\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6774618625640869 \t Validate_Accuracy: 0.5645\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6735909581184387 \t Validate_Accuracy: 0.5695\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.669085681438446 \t Validate_Accuracy: 0.5785\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6640021800994873 \t Validate_Accuracy: 0.678\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6583911776542664 \t Validate_Accuracy: 0.715\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6522725224494934 \t Validate_Accuracy: 0.7425\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6456489562988281 \t Validate_Accuracy: 0.7575\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6385132074356079 \t Validate_Accuracy: 0.7695\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6308474540710449 \t Validate_Accuracy: 0.7825\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6226181983947754 \t Validate_Accuracy: 0.7935\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.613781750202179 \t Validate_Accuracy: 0.799\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6043162941932678 \t Validate_Accuracy: 0.8035\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.5942693948745728 \t Validate_Accuracy: 0.811\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.5837637782096863 \t Validate_Accuracy: 0.8135\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.5729794502258301 \t Validate_Accuracy: 0.8205\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.5620957016944885 \t Validate_Accuracy: 0.82\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.5512150526046753 \t Validate_Accuracy: 0.8185\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.5403127670288086 \t Validate_Accuracy: 0.8165\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.5293077230453491 \t Validate_Accuracy: 0.818\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.5181864500045776 \t Validate_Accuracy: 0.816\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.5070582628250122 \t Validate_Accuracy: 0.818\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.4961344599723816 \t Validate_Accuracy: 0.8195\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.485622763633728 \t Validate_Accuracy: 0.8205\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.475628137588501 \t Validate_Accuracy: 0.821\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.4661298096179962 \t Validate_Accuracy: 0.8215\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.4570677876472473 \t Validate_Accuracy: 0.824\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.4484146535396576 \t Validate_Accuracy: 0.823\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.44016972184181213 \t Validate_Accuracy: 0.8225\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.4323309361934662 \t Validate_Accuracy: 0.8245\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.4248914122581482 \t Validate_Accuracy: 0.826\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.4178261458873749 \t Validate_Accuracy: 0.827\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.41106709837913513 \t Validate_Accuracy: 0.826\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.404544472694397 \t Validate_Accuracy: 0.8265\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.39822301268577576 \t Validate_Accuracy: 0.8275\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.39210590720176697 \t Validate_Accuracy: 0.8295\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.38621851801872253 \t Validate_Accuracy: 0.8305\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.3805951178073883 \t Validate_Accuracy: 0.8325\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3752293884754181 \t Validate_Accuracy: 0.835\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.3700838088989258 \t Validate_Accuracy: 0.837\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.3651164770126343 \t Validate_Accuracy: 0.8405\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.36027422547340393 \t Validate_Accuracy: 0.8425\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.35545778274536133 \t Validate_Accuracy: 0.8445\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.3505687713623047 \t Validate_Accuracy: 0.8435\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.34557777643203735 \t Validate_Accuracy: 0.8445\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.34056389331817627 \t Validate_Accuracy: 0.8445\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.3356599807739258 \t Validate_Accuracy: 0.848\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.3309265673160553 \t Validate_Accuracy: 0.8525\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.3263881802558899 \t Validate_Accuracy: 0.856\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.32209059596061707 \t Validate_Accuracy: 0.857\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.31801652908325195 \t Validate_Accuracy: 0.8565\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.3140970766544342 \t Validate_Accuracy: 0.857\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.3102974593639374 \t Validate_Accuracy: 0.8585\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3066132068634033 \t Validate_Accuracy: 0.86\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.3029955327510834 \t Validate_Accuracy: 0.8625\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.29939448833465576 \t Validate_Accuracy: 0.864\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.29581576585769653 \t Validate_Accuracy: 0.867\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.29227134585380554 \t Validate_Accuracy: 0.866\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.28878554701805115 \t Validate_Accuracy: 0.868\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.28539469838142395 \t Validate_Accuracy: 0.8665\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2821192741394043 \t Validate_Accuracy: 0.869\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2789779007434845 \t Validate_Accuracy: 0.871\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2759701907634735 \t Validate_Accuracy: 0.873\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.27306869626045227 \t Validate_Accuracy: 0.878\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.27024024724960327 \t Validate_Accuracy: 0.8805\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2674655020236969 \t Validate_Accuracy: 0.8805\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2647257149219513 \t Validate_Accuracy: 0.8805\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2619990408420563 \t Validate_Accuracy: 0.883\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.25925159454345703 \t Validate_Accuracy: 0.8825\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2564692199230194 \t Validate_Accuracy: 0.8825\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.253665030002594 \t Validate_Accuracy: 0.883\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2508564889431 \t Validate_Accuracy: 0.8825\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.24806459248065948 \t Validate_Accuracy: 0.882\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.24531535804271698 \t Validate_Accuracy: 0.8845\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.24264268577098846 \t Validate_Accuracy: 0.886\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.24007299542427063 \t Validate_Accuracy: 0.884\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.23761717975139618 \t Validate_Accuracy: 0.884\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.23529841005802155 \t Validate_Accuracy: 0.8845\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.23313964903354645 \t Validate_Accuracy: 0.886\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.23113976418972015 \t Validate_Accuracy: 0.885\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.22927325963974 \t Validate_Accuracy: 0.8825\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.22751730680465698 \t Validate_Accuracy: 0.8845\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.22587816417217255 \t Validate_Accuracy: 0.8855\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.22436189651489258 \t Validate_Accuracy: 0.8855\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2229650467634201 \t Validate_Accuracy: 0.8865\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.2216656655073166 \t Validate_Accuracy: 0.8865\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2204369753599167 \t Validate_Accuracy: 0.887\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.21927347779273987 \t Validate_Accuracy: 0.8865\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2181892842054367 \t Validate_Accuracy: 0.8875\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.21719308197498322 \t Validate_Accuracy: 0.8875\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.21626992523670197 \t Validate_Accuracy: 0.887\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.21540071070194244 \t Validate_Accuracy: 0.8875\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.21458637714385986 \t Validate_Accuracy: 0.8885\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.21384061872959137 \t Validate_Accuracy: 0.889\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.21316349506378174 \t Validate_Accuracy: 0.89\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.21254095435142517 \t Validate_Accuracy: 0.89\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.21196500957012177 \t Validate_Accuracy: 0.891\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2114374190568924 \t Validate_Accuracy: 0.8905\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.21095509827136993 \t Validate_Accuracy: 0.8895\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.21050980687141418 \t Validate_Accuracy: 0.889\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2100977748632431 \t Validate_Accuracy: 0.8905\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.2097189724445343 \t Validate_Accuracy: 0.891\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.20937122404575348 \t Validate_Accuracy: 0.891\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.20905369520187378 \t Validate_Accuracy: 0.8895\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.2087792456150055 \t Validate_Accuracy: 0.89\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.2086358666419983 \t Validate_Accuracy: 0.8895\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.20899443328380585 \t Validate_Accuracy: 0.89\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.209797665476799 \t Validate_Accuracy: 0.8895\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.20939743518829346 \t Validate_Accuracy: 0.889\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2076374888420105 \t Validate_Accuracy: 0.8905\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.2087947428226471 \t Validate_Accuracy: 0.8875\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.20823770761489868 \t Validate_Accuracy: 0.8885\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.2073080837726593 \t Validate_Accuracy: 0.8885\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.20828846096992493 \t Validate_Accuracy: 0.888\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.206985205411911 \t Validate_Accuracy: 0.888\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.20742976665496826 \t Validate_Accuracy: 0.8885\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.20716214179992676 \t Validate_Accuracy: 0.89\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.2066381275653839 \t Validate_Accuracy: 0.889\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.207090824842453 \t Validate_Accuracy: 0.889\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.2063135802745819 \t Validate_Accuracy: 0.8895\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.20673991739749908 \t Validate_Accuracy: 0.8905\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.20630671083927155 \t Validate_Accuracy: 0.891\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.20624308288097382 \t Validate_Accuracy: 0.8905\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.20631477236747742 \t Validate_Accuracy: 0.8895\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.20587418973445892 \t Validate_Accuracy: 0.892\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.2061442881822586 \t Validate_Accuracy: 0.8895\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.2057369351387024 \t Validate_Accuracy: 0.8905\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.20585621893405914 \t Validate_Accuracy: 0.8905\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.20571964979171753 \t Validate_Accuracy: 0.889\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.20555248856544495 \t Validate_Accuracy: 0.8905\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.2056528627872467 \t Validate_Accuracy: 0.8895\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2053745537996292 \t Validate_Accuracy: 0.8905\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.20546478033065796 \t Validate_Accuracy: 0.8895\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.2053123116493225 \t Validate_Accuracy: 0.8905\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.20523619651794434 \t Validate_Accuracy: 0.891\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.2052561640739441 \t Validate_Accuracy: 0.8905\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.20507508516311646 \t Validate_Accuracy: 0.8905\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.20512166619300842 \t Validate_Accuracy: 0.8895\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.20500768721103668 \t Validate_Accuracy: 0.888\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2049381583929062 \t Validate_Accuracy: 0.8905\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.20494550466537476 \t Validate_Accuracy: 0.8895\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.20481109619140625 \t Validate_Accuracy: 0.8895\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.2048119753599167 \t Validate_Accuracy: 0.89\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.20474813878536224 \t Validate_Accuracy: 0.89\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.2046670764684677 \t Validate_Accuracy: 0.8895\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.20466434955596924 \t Validate_Accuracy: 0.89\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.2045762538909912 \t Validate_Accuracy: 0.89\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2045372724533081 \t Validate_Accuracy: 0.889\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.20451010763645172 \t Validate_Accuracy: 0.8885\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.204428032040596 \t Validate_Accuracy: 0.89\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.20440629124641418 \t Validate_Accuracy: 0.8885\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.2043577879667282 \t Validate_Accuracy: 0.8885\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.2042940855026245 \t Validate_Accuracy: 0.8895\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.20427149534225464 \t Validate_Accuracy: 0.889\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.20421580970287323 \t Validate_Accuracy: 0.889\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.20416390895843506 \t Validate_Accuracy: 0.89\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.20413775742053986 \t Validate_Accuracy: 0.889\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.20408160984516144 \t Validate_Accuracy: 0.889\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.204036146402359 \t Validate_Accuracy: 0.8895\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.20400463044643402 \t Validate_Accuracy: 0.889\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.20395150780677795 \t Validate_Accuracy: 0.889\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.20390798151493073 \t Validate_Accuracy: 0.89\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2038736194372177 \t Validate_Accuracy: 0.889\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.20382292568683624 \t Validate_Accuracy: 0.889\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.20377996563911438 \t Validate_Accuracy: 0.89\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.2037438601255417 \t Validate_Accuracy: 0.8895\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.20369550585746765 \t Validate_Accuracy: 0.8895\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.20365159213542938 \t Validate_Accuracy: 0.89\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.20361365377902985 \t Validate_Accuracy: 0.889\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.20356746017932892 \t Validate_Accuracy: 0.889\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.20352202653884888 \t Validate_Accuracy: 0.8895\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.20348261296749115 \t Validate_Accuracy: 0.8895\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.20343756675720215 \t Validate_Accuracy: 0.889\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.2033906728029251 \t Validate_Accuracy: 0.89\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.20334890484809875 \t Validate_Accuracy: 0.8895\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.2033047378063202 \t Validate_Accuracy: 0.8905\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.20325709879398346 \t Validate_Accuracy: 0.89\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.20321224629878998 \t Validate_Accuracy: 0.8895\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.20316770672798157 \t Validate_Accuracy: 0.8905\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.20311929285526276 \t Validate_Accuracy: 0.8905\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.20307116210460663 \t Validate_Accuracy: 0.8905\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.20302452147006989 \t Validate_Accuracy: 0.89\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.20297616720199585 \t Validate_Accuracy: 0.89\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.20292504131793976 \t Validate_Accuracy: 0.89\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.20287542045116425 \t Validate_Accuracy: 0.89\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.20282481610774994 \t Validate_Accuracy: 0.8905\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.202772319316864 \t Validate_Accuracy: 0.89\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.20271846652030945 \t Validate_Accuracy: 0.89\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.20266465842723846 \t Validate_Accuracy: 0.891\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.20260930061340332 \t Validate_Accuracy: 0.8915\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.20255213975906372 \t Validate_Accuracy: 0.8915\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.2024935781955719 \t Validate_Accuracy: 0.8915\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.20243456959724426 \t Validate_Accuracy: 0.8905\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.2023736983537674 \t Validate_Accuracy: 0.892\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.20231042802333832 \t Validate_Accuracy: 0.891\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.2022458165884018 \t Validate_Accuracy: 0.891\n","epoch 210\n","Epoch: 210 \t Train Loss: 0.20217959582805634 \t Validate_Accuracy: 0.891\n","epoch 211\n","Epoch: 211 \t Train Loss: 0.2021113485097885 \t Validate_Accuracy: 0.891\n","epoch 212\n","Epoch: 212 \t Train Loss: 0.20204044878482819 \t Validate_Accuracy: 0.891\n","epoch 213\n","Epoch: 213 \t Train Loss: 0.2019670456647873 \t Validate_Accuracy: 0.891\n","epoch 214\n","Epoch: 214 \t Train Loss: 0.20189185440540314 \t Validate_Accuracy: 0.8915\n","epoch 215\n","Epoch: 215 \t Train Loss: 0.20181408524513245 \t Validate_Accuracy: 0.891\n","epoch 216\n","Epoch: 216 \t Train Loss: 0.20173320174217224 \t Validate_Accuracy: 0.891\n","epoch 217\n","Epoch: 217 \t Train Loss: 0.20164938271045685 \t Validate_Accuracy: 0.891\n","epoch 218\n","Epoch: 218 \t Train Loss: 0.20156222581863403 \t Validate_Accuracy: 0.891\n","epoch 219\n","Epoch: 219 \t Train Loss: 0.2014722228050232 \t Validate_Accuracy: 0.891\n","epoch 220\n","Epoch: 220 \t Train Loss: 0.20137858390808105 \t Validate_Accuracy: 0.8905\n","epoch 221\n","Epoch: 221 \t Train Loss: 0.20128078758716583 \t Validate_Accuracy: 0.891\n","epoch 222\n","Epoch: 222 \t Train Loss: 0.2011791318655014 \t Validate_Accuracy: 0.8905\n","epoch 223\n","Epoch: 223 \t Train Loss: 0.2010732740163803 \t Validate_Accuracy: 0.89\n","epoch 224\n","Epoch: 224 \t Train Loss: 0.20096293091773987 \t Validate_Accuracy: 0.89\n","epoch 225\n","Epoch: 225 \t Train Loss: 0.20084750652313232 \t Validate_Accuracy: 0.89\n","epoch 226\n","Epoch: 226 \t Train Loss: 0.20072732865810394 \t Validate_Accuracy: 0.89\n","epoch 227\n","Epoch: 227 \t Train Loss: 0.200601264834404 \t Validate_Accuracy: 0.89\n","epoch 228\n","Epoch: 228 \t Train Loss: 0.20046944916248322 \t Validate_Accuracy: 0.89\n","epoch 229\n","Epoch: 229 \t Train Loss: 0.20033200085163116 \t Validate_Accuracy: 0.89\n","epoch 230\n","Epoch: 230 \t Train Loss: 0.2001877725124359 \t Validate_Accuracy: 0.89\n","epoch 231\n","Epoch: 231 \t Train Loss: 0.2000364363193512 \t Validate_Accuracy: 0.8895\n","epoch 232\n","Epoch: 232 \t Train Loss: 0.19987812638282776 \t Validate_Accuracy: 0.8895\n","epoch 233\n","Epoch: 233 \t Train Loss: 0.19971148669719696 \t Validate_Accuracy: 0.8895\n","epoch 234\n","Epoch: 234 \t Train Loss: 0.19953665137290955 \t Validate_Accuracy: 0.89\n","epoch 235\n","Epoch: 235 \t Train Loss: 0.19935333728790283 \t Validate_Accuracy: 0.89\n","epoch 236\n","Epoch: 236 \t Train Loss: 0.1991603523492813 \t Validate_Accuracy: 0.89\n","epoch 237\n","Epoch: 237 \t Train Loss: 0.19895759224891663 \t Validate_Accuracy: 0.89\n","epoch 238\n","Epoch: 238 \t Train Loss: 0.19874420762062073 \t Validate_Accuracy: 0.8895\n","epoch 239\n","Epoch: 239 \t Train Loss: 0.19852009415626526 \t Validate_Accuracy: 0.89\n","epoch 240\n","Epoch: 240 \t Train Loss: 0.19828426837921143 \t Validate_Accuracy: 0.8905\n","epoch 241\n","Epoch: 241 \t Train Loss: 0.19803640246391296 \t Validate_Accuracy: 0.8905\n","epoch 242\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:57:15] ax.service.managed_loop: Running optimization trial 8...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 242 \t Train Loss: 0.19777637720108032 \t Validate_Accuracy: 0.89\n","epoch 243\n","Epoch: 243 \t Train Loss: 0.19750283658504486 \t Validate_Accuracy: 0.8905\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1322,  0.0541,  0.1804],\n","          [ 0.0816, -0.0078,  0.0553],\n","          [ 0.1306,  0.0389,  0.1701]]]])\n","conv1.bias tensor([-0.0740])\n","first_linear.weight tensor([[ 0.2347,  0.1755, -0.5483, -0.6577,  0.9013,  0.3845,  0.1335, -0.2598,\n","         -0.0707],\n","        [ 0.3960,  0.2303,  0.1437, -0.0758,  0.4074,  0.4668,  0.2596,  0.1019,\n","          0.1069],\n","        [-0.2735, -0.4181, -0.1838,  1.3452, -0.6502,  0.7070, -0.4228,  0.1218,\n","         -0.2435],\n","        [ 0.1565, -0.0329,  0.0324,  0.0170,  0.1007,  0.2624,  0.0800,  0.1904,\n","          0.0267],\n","        [-0.7871,  1.1368, -0.5226,  1.0808, -0.4981,  0.0866, -0.3655,  0.0958,\n","         -0.0275],\n","        [ 0.0745,  0.0451,  0.4692,  0.2442, -0.5961, -0.5447, -0.7652,  1.2989,\n","         -0.1641],\n","        [-0.4742,  1.1893, -0.6705, -0.4517, -0.7145,  0.6102,  0.7588, -0.2547,\n","          0.0151],\n","        [-0.0917,  0.0086, -0.0692,  0.0043, -0.0753, -0.1759, -0.0493, -0.1194,\n","         -0.0578],\n","        [-0.3721,  0.2939,  0.2781,  0.1934, -0.0406,  0.0436,  0.1155,  0.1993,\n","          0.5247],\n","        [-0.7801, -0.0542,  0.4581,  0.4572,  0.4576, -1.2373,  0.0483, -0.9960,\n","          1.0332]])\n","first_linear.bias tensor([ 0.2471, -0.7827,  0.9118,  1.0035, -0.9871,  0.7992,  0.9897, -1.1041,\n","         0.6184,  1.0525])\n","linear_hidden.0.weight tensor([[ 1.1837,  0.8943,  0.4237, -0.3387, -0.8072,  0.3764,  0.6970,  0.7413,\n","         -0.7889,  0.6513],\n","        [-0.1266,  0.1187, -0.0836, -0.5258,  0.2217, -0.1453, -0.2316,  0.5782,\n","         -0.3458, -0.1740],\n","        [ 0.1148, -0.1019,  0.0728,  0.4617, -0.1766,  0.0664,  0.1552, -0.3443,\n","          0.3341,  0.2099],\n","        [-0.2480,  0.5406,  0.7635, -0.6655,  0.3779,  0.5941,  0.4609,  0.5116,\n","          0.0713,  0.6714],\n","        [ 0.1676, -0.1301,  0.0848,  0.4070, -0.2354,  0.1156,  0.1939, -0.4320,\n","          0.4863,  0.2615],\n","        [-0.4698, -0.8498, -0.8268,  0.7055,  0.8151, -0.8608, -0.6506, -0.7280,\n","          0.3375, -0.8121],\n","        [-0.3370,  0.6258,  0.6163, -0.4244,  0.6435,  0.5953,  1.0810,  0.1211,\n","          0.4951,  0.1250],\n","        [ 0.5468,  0.8090,  0.8615, -0.6147, -0.7582,  0.8599,  0.6244,  0.8069,\n","         -0.4535,  0.7738],\n","        [-0.4800, -0.9091, -0.8140,  0.7390,  0.8018, -0.8672, -0.6498, -0.8091,\n","          0.3467, -0.7909],\n","        [ 0.4899,  0.8301,  0.7634, -0.6723, -0.8197,  0.8677,  0.6561,  0.7882,\n","         -0.2666,  0.7620]])\n","linear_hidden.0.bias tensor([ 0.2846, -0.5067,  0.5350,  0.3781,  0.5026,  0.6524,  0.3801, -0.5808,\n","         0.4911, -0.6096])\n","linear_output.weight tensor([[ 1.3317,  0.8542, -0.6852,  1.0430, -0.8008, -1.6896,  0.9190,  1.4682,\n","         -1.6678,  1.5737]])\n","linear_output.bias tensor([0.2981])\n","Testing out: \n","batch_size:  1828\n","train_size:  3863\n","n_epochs:  224\n","lr:  0.017208694042933067\n","weight_decay:  0.0003307960051334996\n","betas0:  0.967815690478712\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.2000, -0.1111, -0.0529],\n","          [-0.0538,  0.0630, -0.1566],\n","          [ 0.0166, -0.2796, -0.1322]]]])\n","conv1.bias tensor([0.0753])\n","first_linear.weight tensor([[ 0.0151, -0.0735, -0.1570, -0.0895, -0.2196, -0.3009,  0.0159,  0.2569,\n","         -0.2268],\n","        [-0.1524,  0.0255, -0.3137, -0.0408, -0.1599,  0.0373,  0.1114,  0.1000,\n","         -0.1320],\n","        [ 0.2875, -0.2178,  0.0980, -0.1508,  0.2311, -0.1377,  0.1835,  0.2342,\n","          0.0664],\n","        [-0.0201,  0.2723,  0.0429, -0.1532, -0.2247,  0.1572,  0.3019, -0.1131,\n","         -0.2442],\n","        [ 0.1651,  0.2529,  0.1670,  0.0696,  0.1798, -0.1541,  0.3206,  0.2301,\n","          0.2003],\n","        [ 0.0367, -0.2130,  0.2220,  0.1919, -0.1528, -0.0207,  0.2217,  0.0090,\n","          0.1438],\n","        [ 0.1989, -0.0507, -0.2409,  0.3111,  0.3248,  0.3308, -0.2111,  0.1443,\n","         -0.0567],\n","        [ 0.1692, -0.2451, -0.2669,  0.2293, -0.2039,  0.2119,  0.0355, -0.2607,\n","          0.1565],\n","        [ 0.2677,  0.1720,  0.3188, -0.3114, -0.2460,  0.0626, -0.2757,  0.1414,\n","         -0.0199],\n","        [ 0.0392,  0.1027, -0.2374,  0.0460, -0.0250,  0.0484, -0.1685, -0.2693,\n","         -0.0971]])\n","first_linear.bias tensor([-0.1052,  0.2063,  0.0362, -0.2121, -0.1395,  0.2740, -0.0958,  0.1045,\n","         0.0574,  0.0283])\n","linear_hidden.0.weight tensor([[-0.0782, -0.2283, -0.2100, -0.1322,  0.0233,  0.1082,  0.0937,  0.0309,\n","         -0.0391, -0.2689],\n","        [ 0.2216,  0.0363,  0.0751,  0.1029, -0.0044, -0.1590, -0.0080, -0.2194,\n","          0.1133, -0.2450],\n","        [-0.0915, -0.3150,  0.0178,  0.0273,  0.0507, -0.0316,  0.1894,  0.2694,\n","          0.2860, -0.0040],\n","        [ 0.1631, -0.0674, -0.0910, -0.1510, -0.0602, -0.0672, -0.1665, -0.0788,\n","         -0.1338,  0.2223],\n","        [ 0.1503, -0.0626, -0.3033, -0.0874, -0.2262,  0.2029,  0.2936,  0.3073,\n","          0.1742, -0.0719],\n","        [ 0.3048,  0.0854, -0.2889,  0.2871, -0.0765,  0.0663, -0.0259,  0.1737,\n","          0.0866,  0.2885],\n","        [ 0.1801, -0.0023, -0.1734,  0.0216, -0.1558,  0.0552,  0.2765,  0.1825,\n","         -0.2850,  0.0491],\n","        [-0.1383,  0.2576,  0.0634,  0.3024, -0.1713, -0.2426,  0.1297,  0.2163,\n","         -0.0417,  0.2711],\n","        [-0.1240,  0.1390, -0.2844,  0.2868, -0.3053, -0.0427,  0.1887, -0.1070,\n","          0.1673, -0.2997],\n","        [-0.1569, -0.0924,  0.1647,  0.1322,  0.0303,  0.1084,  0.2495,  0.2500,\n","          0.0451,  0.1513]])\n","linear_hidden.0.bias tensor([-0.0249, -0.2336,  0.0613,  0.0376, -0.0336,  0.0512, -0.2104, -0.2246,\n","        -0.1413, -0.0229])\n","linear_output.weight tensor([[ 0.0370, -0.0524,  0.2033, -0.2821, -0.2117,  0.2416, -0.2333, -0.2461,\n","          0.2646, -0.1633]])\n","linear_output.bias tensor([0.2392])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7002997398376465 \t Validate_Accuracy: 0.5375\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1828])) that is different to the input size (torch.Size([1828, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([207])) that is different to the input size (torch.Size([207, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.6895996332168579 \t Validate_Accuracy: 0.454\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6837972601254781 \t Validate_Accuracy: 0.5275\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6751007040341696 \t Validate_Accuracy: 0.5895\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6543176770210266 \t Validate_Accuracy: 0.7435\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6348650654157003 \t Validate_Accuracy: 0.785\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6078921755154928 \t Validate_Accuracy: 0.798\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5672086079915365 \t Validate_Accuracy: 0.8175\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.5439449350039164 \t Validate_Accuracy: 0.817\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.5016112228234609 \t Validate_Accuracy: 0.8185\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.47978221376736957 \t Validate_Accuracy: 0.8135\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.47996848821640015 \t Validate_Accuracy: 0.81\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.46595872441927594 \t Validate_Accuracy: 0.811\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.46049246191978455 \t Validate_Accuracy: 0.8145\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.4538413981596629 \t Validate_Accuracy: 0.816\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.4408673743406932 \t Validate_Accuracy: 0.819\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.42670424779256183 \t Validate_Accuracy: 0.8245\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.43500985701878864 \t Validate_Accuracy: 0.819\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.41976000865300495 \t Validate_Accuracy: 0.822\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.41042659680048627 \t Validate_Accuracy: 0.824\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.4073861042658488 \t Validate_Accuracy: 0.827\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3887813687324524 \t Validate_Accuracy: 0.8245\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.3875988225142161 \t Validate_Accuracy: 0.831\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3919943571090698 \t Validate_Accuracy: 0.831\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.38048725326855976 \t Validate_Accuracy: 0.8245\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.3723789652188619 \t Validate_Accuracy: 0.834\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.36125893394152325 \t Validate_Accuracy: 0.8365\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.38233207662900287 \t Validate_Accuracy: 0.836\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.35686737298965454 \t Validate_Accuracy: 0.83\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.35516002774238586 \t Validate_Accuracy: 0.839\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.3543000817298889 \t Validate_Accuracy: 0.843\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.338457852602005 \t Validate_Accuracy: 0.844\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.33504872520764667 \t Validate_Accuracy: 0.8445\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.3436974883079529 \t Validate_Accuracy: 0.848\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.33639365434646606 \t Validate_Accuracy: 0.8475\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.3462524314721425 \t Validate_Accuracy: 0.844\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.3378598888715108 \t Validate_Accuracy: 0.846\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.3357374270757039 \t Validate_Accuracy: 0.841\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.3303683598836263 \t Validate_Accuracy: 0.8475\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.3085547884305318 \t Validate_Accuracy: 0.8495\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.3150195777416229 \t Validate_Accuracy: 0.853\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.3407698671023051 \t Validate_Accuracy: 0.852\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.31959958871205646 \t Validate_Accuracy: 0.854\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.313860426346461 \t Validate_Accuracy: 0.8545\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.30466973781585693 \t Validate_Accuracy: 0.854\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.3112071951230367 \t Validate_Accuracy: 0.8565\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.30604403217633563 \t Validate_Accuracy: 0.8545\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.32610313097635907 \t Validate_Accuracy: 0.8555\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.29754239320755005 \t Validate_Accuracy: 0.859\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.29878706733385724 \t Validate_Accuracy: 0.8535\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.30716914931933087 \t Validate_Accuracy: 0.855\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3038400411605835 \t Validate_Accuracy: 0.856\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.3340704043706258 \t Validate_Accuracy: 0.853\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.3036147157351176 \t Validate_Accuracy: 0.8565\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.3072059551874797 \t Validate_Accuracy: 0.853\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.3000858227411906 \t Validate_Accuracy: 0.854\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.3028355538845062 \t Validate_Accuracy: 0.854\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.33034202456474304 \t Validate_Accuracy: 0.854\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.30166741212209064 \t Validate_Accuracy: 0.8555\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.30909640590349835 \t Validate_Accuracy: 0.853\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.3011454741160075 \t Validate_Accuracy: 0.8545\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.31453728675842285 \t Validate_Accuracy: 0.8595\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.3002334535121918 \t Validate_Accuracy: 0.8585\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.3134697874387105 \t Validate_Accuracy: 0.86\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2827022274335225 \t Validate_Accuracy: 0.861\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2987141211827596 \t Validate_Accuracy: 0.863\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.28420425454775494 \t Validate_Accuracy: 0.8605\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.2777076264222463 \t Validate_Accuracy: 0.8565\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.28933151563008624 \t Validate_Accuracy: 0.856\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.2895294427871704 \t Validate_Accuracy: 0.862\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.2993260125319163 \t Validate_Accuracy: 0.864\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.27986132105191547 \t Validate_Accuracy: 0.8595\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.29500146706899005 \t Validate_Accuracy: 0.8625\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2813018163045247 \t Validate_Accuracy: 0.864\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.2693614761034648 \t Validate_Accuracy: 0.8635\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2775711417198181 \t Validate_Accuracy: 0.867\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.3046518762906392 \t Validate_Accuracy: 0.8625\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.27656052509943646 \t Validate_Accuracy: 0.8625\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2878790895144145 \t Validate_Accuracy: 0.861\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2848888039588928 \t Validate_Accuracy: 0.8555\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.2915441195170085 \t Validate_Accuracy: 0.861\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.27540871500968933 \t Validate_Accuracy: 0.8615\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.2873603105545044 \t Validate_Accuracy: 0.8635\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.2814200321833293 \t Validate_Accuracy: 0.862\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.2753102084000905 \t Validate_Accuracy: 0.868\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.2785719732443492 \t Validate_Accuracy: 0.863\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2703714569409688 \t Validate_Accuracy: 0.864\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2802670399347941 \t Validate_Accuracy: 0.863\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.27154144644737244 \t Validate_Accuracy: 0.8675\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2778778274854024 \t Validate_Accuracy: 0.874\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.27325137456258136 \t Validate_Accuracy: 0.8645\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.27491561571757 \t Validate_Accuracy: 0.863\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.24854514002799988 \t Validate_Accuracy: 0.8655\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2706013520558675 \t Validate_Accuracy: 0.8605\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.2809130648771922 \t Validate_Accuracy: 0.8615\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.29122911890347797 \t Validate_Accuracy: 0.8635\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.2676904797554016 \t Validate_Accuracy: 0.87\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.26060883204142254 \t Validate_Accuracy: 0.873\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.27490893999735516 \t Validate_Accuracy: 0.872\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2682022551695506 \t Validate_Accuracy: 0.861\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2724761366844177 \t Validate_Accuracy: 0.8705\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.282997061808904 \t Validate_Accuracy: 0.871\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.24863797426223755 \t Validate_Accuracy: 0.8615\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.2548576941092809 \t Validate_Accuracy: 0.865\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.2665459215641022 \t Validate_Accuracy: 0.865\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2560467670361201 \t Validate_Accuracy: 0.864\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2683705488840739 \t Validate_Accuracy: 0.8675\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2781916558742523 \t Validate_Accuracy: 0.87\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.25451990962028503 \t Validate_Accuracy: 0.8675\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.26887457569440204 \t Validate_Accuracy: 0.867\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.26610638201236725 \t Validate_Accuracy: 0.863\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.24934407571951547 \t Validate_Accuracy: 0.866\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2723168234030406 \t Validate_Accuracy: 0.861\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2810225486755371 \t Validate_Accuracy: 0.8615\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.26154640316963196 \t Validate_Accuracy: 0.862\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.23521738747755686 \t Validate_Accuracy: 0.869\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2619377324978511 \t Validate_Accuracy: 0.8705\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.2620273431142171 \t Validate_Accuracy: 0.862\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.2883842885494232 \t Validate_Accuracy: 0.864\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.26450255513191223 \t Validate_Accuracy: 0.8665\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.2723344812790553 \t Validate_Accuracy: 0.8605\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.26642688115437824 \t Validate_Accuracy: 0.8655\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.26005754868189496 \t Validate_Accuracy: 0.8705\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.25598181287447613 \t Validate_Accuracy: 0.8645\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.2634797791639964 \t Validate_Accuracy: 0.8675\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.25413347283999127 \t Validate_Accuracy: 0.874\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2609078884124756 \t Validate_Accuracy: 0.8765\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.24013867477575937 \t Validate_Accuracy: 0.8625\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.25423123439153034 \t Validate_Accuracy: 0.8745\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.2483421415090561 \t Validate_Accuracy: 0.8685\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.27998186151186627 \t Validate_Accuracy: 0.8735\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.266903152068456 \t Validate_Accuracy: 0.8715\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.24448511004447937 \t Validate_Accuracy: 0.874\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.27838551501433056 \t Validate_Accuracy: 0.8765\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.24763494233290353 \t Validate_Accuracy: 0.8655\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2452462762594223 \t Validate_Accuracy: 0.873\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.2654995272556941 \t Validate_Accuracy: 0.8735\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.26011739671230316 \t Validate_Accuracy: 0.8695\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.24948938687642416 \t Validate_Accuracy: 0.8645\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.2427007108926773 \t Validate_Accuracy: 0.8715\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.24725544452667236 \t Validate_Accuracy: 0.876\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.26435404022534686 \t Validate_Accuracy: 0.8685\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.24754668275515238 \t Validate_Accuracy: 0.8675\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.2633059273163478 \t Validate_Accuracy: 0.8705\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.26225994527339935 \t Validate_Accuracy: 0.871\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.23576035102208456 \t Validate_Accuracy: 0.8695\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.26588884989420575 \t Validate_Accuracy: 0.866\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.23829937477906546 \t Validate_Accuracy: 0.8625\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.2762286265691121 \t Validate_Accuracy: 0.871\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.23730850219726562 \t Validate_Accuracy: 0.8745\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.2449335257212321 \t Validate_Accuracy: 0.8755\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.2402075876792272 \t Validate_Accuracy: 0.8735\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.24588397641976675 \t Validate_Accuracy: 0.8755\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.2407905012369156 \t Validate_Accuracy: 0.8665\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.2666715780893962 \t Validate_Accuracy: 0.8715\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.24042877554893494 \t Validate_Accuracy: 0.8715\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.2535669455925624 \t Validate_Accuracy: 0.8765\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.2419042686621348 \t Validate_Accuracy: 0.879\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.25518113374710083 \t Validate_Accuracy: 0.8755\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.23349707822004953 \t Validate_Accuracy: 0.8665\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.24762255946795145 \t Validate_Accuracy: 0.8725\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.24945896367232004 \t Validate_Accuracy: 0.8705\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.25475358466307324 \t Validate_Accuracy: 0.875\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.25007085998853046 \t Validate_Accuracy: 0.875\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.2557162245114644 \t Validate_Accuracy: 0.8675\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.2494197835524877 \t Validate_Accuracy: 0.8725\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.23758667707443237 \t Validate_Accuracy: 0.8725\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.25906384984652203 \t Validate_Accuracy: 0.8745\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.231496532758077 \t Validate_Accuracy: 0.8755\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.2282729148864746 \t Validate_Accuracy: 0.876\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.25189144412676495 \t Validate_Accuracy: 0.87\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.22629919151465097 \t Validate_Accuracy: 0.874\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.24997247258822122 \t Validate_Accuracy: 0.874\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.2418336719274521 \t Validate_Accuracy: 0.872\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.24675623079140982 \t Validate_Accuracy: 0.875\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2194266269604365 \t Validate_Accuracy: 0.8755\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.23397697508335114 \t Validate_Accuracy: 0.88\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.24792235593001047 \t Validate_Accuracy: 0.8815\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.22391984860102335 \t Validate_Accuracy: 0.875\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.24156097571055093 \t Validate_Accuracy: 0.8715\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.23495377600193024 \t Validate_Accuracy: 0.871\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.23641858994960785 \t Validate_Accuracy: 0.875\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.2494481404622396 \t Validate_Accuracy: 0.8755\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.24738352497418722 \t Validate_Accuracy: 0.875\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.2328495333592097 \t Validate_Accuracy: 0.8685\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.2274826020002365 \t Validate_Accuracy: 0.869\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.2578642765680949 \t Validate_Accuracy: 0.8725\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.23969280223051706 \t Validate_Accuracy: 0.8735\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.25942810376485187 \t Validate_Accuracy: 0.866\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.24406960606575012 \t Validate_Accuracy: 0.8725\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.24462517102559408 \t Validate_Accuracy: 0.871\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.24027380843957266 \t Validate_Accuracy: 0.8705\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.26968108117580414 \t Validate_Accuracy: 0.871\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.24283319214979807 \t Validate_Accuracy: 0.8665\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.24298537770907083 \t Validate_Accuracy: 0.868\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.23305476208527884 \t Validate_Accuracy: 0.8755\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.21975348393122354 \t Validate_Accuracy: 0.8745\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.242418110370636 \t Validate_Accuracy: 0.873\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.2593935529390971 \t Validate_Accuracy: 0.875\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.25002236167589825 \t Validate_Accuracy: 0.8755\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.24954197307427725 \t Validate_Accuracy: 0.8695\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.25513384242852527 \t Validate_Accuracy: 0.8725\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.24290316800276437 \t Validate_Accuracy: 0.8735\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.2509891490141551 \t Validate_Accuracy: 0.869\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.2307732254266739 \t Validate_Accuracy: 0.8735\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.26025886833667755 \t Validate_Accuracy: 0.876\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.24663330614566803 \t Validate_Accuracy: 0.8725\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.24478410681088766 \t Validate_Accuracy: 0.8785\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.23543153703212738 \t Validate_Accuracy: 0.878\n","epoch 210\n","Epoch: 210 \t Train Loss: 0.2501794596513112 \t Validate_Accuracy: 0.875\n","epoch 211\n","Epoch: 211 \t Train Loss: 0.23622478048006693 \t Validate_Accuracy: 0.872\n","epoch 212\n","Epoch: 212 \t Train Loss: 0.2464418113231659 \t Validate_Accuracy: 0.8765\n","epoch 213\n","Epoch: 213 \t Train Loss: 0.23013719419638315 \t Validate_Accuracy: 0.874\n","epoch 214\n","Epoch: 214 \t Train Loss: 0.23551135261853537 \t Validate_Accuracy: 0.878\n","epoch 215\n","Epoch: 215 \t Train Loss: 0.2308581918478012 \t Validate_Accuracy: 0.879\n","epoch 216\n","Epoch: 216 \t Train Loss: 0.24015084405740103 \t Validate_Accuracy: 0.8805\n","epoch 217\n","Epoch: 217 \t Train Loss: 0.2528088092803955 \t Validate_Accuracy: 0.878\n","epoch 218\n","Epoch: 218 \t Train Loss: 0.2537432014942169 \t Validate_Accuracy: 0.875\n","epoch 219\n","Epoch: 219 \t Train Loss: 0.2306850254535675 \t Validate_Accuracy: 0.874\n","epoch 220\n","Epoch: 220 \t Train Loss: 0.23359918097654978 \t Validate_Accuracy: 0.877\n","epoch 221\n","Epoch: 221 \t Train Loss: 0.23690113921960196 \t Validate_Accuracy: 0.8795\n","epoch 222\n","Epoch: 222 \t Train Loss: 0.2413328488667806 \t Validate_Accuracy: 0.879\n","epoch 223\n","Epoch: 223 \t Train Loss: 0.24141107499599457 \t Validate_Accuracy: 0.88\n","epoch 224\n","Epoch: 224 \t Train Loss: 0.24725725750128427 \t Validate_Accuracy: 0.8755\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1676, -0.0706, -0.1698],\n","          [-0.0740, -0.0049, -0.0420],\n","          [-0.1601, -0.0612, -0.1501]]]])\n","conv1.bias tensor([0.0312])\n","first_linear.weight tensor([[-0.7079, -0.0234,  0.3077,  0.4113, -0.7369, -1.0180, -0.5867,  0.5588,\n","         -0.1259],\n","        [ 0.1882, -0.6268, -0.3967,  0.1825, -0.0871, -0.9202, -0.1848, -0.2864,\n","         -0.2521],\n","        [-0.0176, -0.0150,  0.0808,  0.4320,  0.1397, -0.0663,  0.2135,  0.4826,\n","          0.4592],\n","        [-0.4876, -0.4855, -0.5349, -0.4622, -0.3785,  0.3247,  0.1108,  0.1145,\n","         -0.0749],\n","        [ 0.3597,  0.0506, -0.3485,  0.3038,  0.2043,  0.1799,  0.3849,  0.5148,\n","         -0.1245],\n","        [ 0.5096, -0.9281,  0.7521, -0.4261,  1.0106, -1.3872, -0.1392, -0.3259,\n","          0.4353],\n","        [ 0.6386, -0.5493,  0.8040,  0.5830,  0.4186,  0.1689, -0.8141,  0.8375,\n","          0.0641],\n","        [-0.1241, -0.0699,  0.1293, -0.5161,  0.7833, -0.3948,  0.5802, -1.2556,\n","          0.6858],\n","        [ 0.7422, -0.5590, -0.0358, -1.4280,  0.0783,  0.2226,  0.5943,  0.3482,\n","         -0.8431],\n","        [ 0.5531,  0.5498, -0.1483,  0.0639, -0.0456, -0.0459,  0.6183, -0.2972,\n","         -0.6719]])\n","first_linear.bias tensor([-0.3957,  0.8405,  0.6933, -0.8074, -1.0577,  1.1290, -0.5102, -0.8223,\n","        -0.9763, -0.5438])\n","linear_hidden.0.weight tensor([[-0.6586, -0.4628, -0.8609,  0.6302,  0.4624,  1.2197,  0.2142, -1.1152,\n","         -0.1714,  0.2780],\n","        [ 0.0499,  0.6256,  0.2419, -0.5126, -0.1632, -0.9469,  1.3026, -0.2742,\n","          1.4167, -0.3766],\n","        [-0.1128, -0.7245, -0.1838,  0.2856,  0.3840,  0.0549,  0.3783,  0.0507,\n","         -0.5802,  0.4378],\n","        [ 0.2736,  0.8627,  0.0159, -0.2535, -0.5527, -0.1437, -0.3458,  0.1385,\n","          0.7198, -0.2675],\n","        [ 0.2392,  0.7585, -0.0224, -0.1822, -0.4650, -0.1203, -0.3008,  0.1097,\n","          0.6592, -0.3122],\n","        [ 0.6677, -0.0033, -0.4205,  1.1129,  0.6013,  0.1311, -0.4958, -0.2327,\n","          0.2812,  0.0523],\n","        [ 1.1322,  0.5687,  0.6167, -0.0927, -0.5304, -1.3767,  0.6423, -0.0108,\n","         -0.3828, -0.6245],\n","        [-1.0369,  0.3916,  0.3177, -0.1219, -0.2713, -1.2273, -0.5511,  1.5242,\n","          0.7862, -0.0103],\n","        [ 0.2132,  0.1777, -0.3244,  0.4167,  0.3165,  0.3596, -0.6446, -0.4703,\n","         -0.2375, -0.4948],\n","        [-0.4073,  0.0733,  0.9302, -0.6175, -0.4425, -0.3080,  0.0799,  0.0703,\n","          0.0255, -0.4226]])\n","linear_hidden.0.bias tensor([ 0.3073, -0.1670, -0.6900,  0.7274,  0.7298, -0.7662, -0.8229, -0.3075,\n","        -0.8599,  0.5389])\n","linear_output.weight tensor([[ 1.7994, -2.2697,  1.0643, -1.1932, -1.0121,  1.3542, -1.7706, -2.1733,\n","          1.2218, -1.1209]])\n","linear_output.bias tensor([0.3909])\n","Testing out: \n","batch_size:  2257\n","train_size:  2498\n","n_epochs:  161\n","lr:  0.0053913441285685275\n","weight_decay:  0.0009504910500690459\n","betas0:  0.9994766949828744\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1149, -0.2053, -0.2080],\n","          [ 0.0030, -0.0927,  0.1720],\n","          [-0.1827, -0.2309, -0.1501]]]])\n","conv1.bias tensor([0.1650])\n","first_linear.weight tensor([[-0.0865,  0.1435, -0.2071,  0.1215,  0.1810, -0.0063, -0.0228,  0.2473,\n","          0.0553],\n","        [ 0.0857,  0.1677, -0.0523, -0.2266, -0.1217,  0.2680, -0.1547, -0.2761,\n","         -0.2423],\n","        [-0.0986,  0.0754,  0.3213,  0.1958,  0.0993,  0.1326,  0.2227, -0.0544,\n","          0.3137],\n","        [-0.2874,  0.1587,  0.2642,  0.2783, -0.0741,  0.2743,  0.2993,  0.0010,\n","         -0.1062],\n","        [-0.0604, -0.0352,  0.3106, -0.0030, -0.0092,  0.3168, -0.0405,  0.2131,\n","         -0.2959],\n","        [-0.1774, -0.0856,  0.1757, -0.2672,  0.1791, -0.0674, -0.0613, -0.2180,\n","         -0.3022],\n","        [-0.2910,  0.2492, -0.0955, -0.0689,  0.2669,  0.1418, -0.2098, -0.0836,\n","          0.1191],\n","        [-0.3112, -0.0826,  0.1272, -0.1780,  0.0763,  0.1856,  0.2851, -0.2394,\n","          0.2133],\n","        [-0.0333,  0.0454,  0.3244,  0.1036, -0.0058, -0.3101,  0.1805, -0.1410,\n","          0.1269],\n","        [-0.0510, -0.2037, -0.1595,  0.2289, -0.0442,  0.1540, -0.1263, -0.0522,\n","          0.0926]])\n","first_linear.bias tensor([ 0.2264,  0.2676, -0.2905, -0.0867,  0.0766,  0.2061,  0.2788,  0.2092,\n","         0.2772, -0.2814])\n","linear_hidden.0.weight tensor([[-0.0636,  0.2212, -0.2239,  0.1911,  0.2241, -0.2643, -0.0602,  0.1578,\n","         -0.2103,  0.1484],\n","        [-0.1167, -0.2629, -0.0249, -0.1949, -0.1426, -0.2281, -0.0509, -0.0111,\n","          0.0469, -0.2609],\n","        [ 0.0257, -0.2560,  0.1463,  0.0070, -0.0993, -0.0517,  0.2260, -0.2663,\n","          0.0591,  0.0663],\n","        [-0.1314, -0.0534, -0.0031,  0.0467,  0.0057, -0.1127, -0.0223,  0.1076,\n","          0.1167, -0.2273],\n","        [ 0.2344, -0.1725, -0.2249,  0.0825, -0.2567, -0.0310,  0.0018, -0.1610,\n","         -0.2733,  0.2762],\n","        [ 0.1887,  0.0232, -0.2177, -0.1109, -0.1827,  0.1652, -0.0794, -0.3116,\n","         -0.1941,  0.1909],\n","        [-0.2561, -0.2527, -0.1522,  0.1247, -0.1989,  0.0286, -0.1651, -0.2544,\n","         -0.2434,  0.1840],\n","        [ 0.1068,  0.1521, -0.0156,  0.1270,  0.2254,  0.2980,  0.2188,  0.2972,\n","          0.2351, -0.1795],\n","        [-0.0832, -0.1146, -0.0427, -0.1612, -0.1337, -0.2452,  0.0675,  0.1045,\n","         -0.0274, -0.0135],\n","        [ 0.2360, -0.1396,  0.0928, -0.1354, -0.3131, -0.0705, -0.1046, -0.2675,\n","         -0.2061, -0.2983]])\n","linear_hidden.0.bias tensor([-0.2817, -0.1346, -0.0849, -0.1788,  0.2074,  0.0381, -0.2738,  0.1367,\n","        -0.1234, -0.2967])\n","linear_output.weight tensor([[ 0.0016,  0.2614, -0.2136,  0.2060, -0.0879, -0.0708,  0.0803, -0.2297,\n","          0.1397,  0.1148]])\n","linear_output.bias tensor([-0.3071])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2257])) that is different to the input size (torch.Size([2257, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([241])) that is different to the input size (torch.Size([241, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.7384031414985657 \t Validate_Accuracy: 0.503\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.7153958976268768 \t Validate_Accuracy: 0.503\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7082675993442535 \t Validate_Accuracy: 0.503\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.7077440619468689 \t Validate_Accuracy: 0.503\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.7018306851387024 \t Validate_Accuracy: 0.503\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6949267685413361 \t Validate_Accuracy: 0.481\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6910946369171143 \t Validate_Accuracy: 0.524\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.68907031416893 \t Validate_Accuracy: 0.5305\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6904495060443878 \t Validate_Accuracy: 0.5495\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6872432231903076 \t Validate_Accuracy: 0.5835\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6867666244506836 \t Validate_Accuracy: 0.594\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6847354769706726 \t Validate_Accuracy: 0.5965\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.6822404265403748 \t Validate_Accuracy: 0.5985\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.681932270526886 \t Validate_Accuracy: 0.5985\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6859467923641205 \t Validate_Accuracy: 0.597\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6875994801521301 \t Validate_Accuracy: 0.5955\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6791512370109558 \t Validate_Accuracy: 0.5895\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.677631288766861 \t Validate_Accuracy: 0.587\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6784475147724152 \t Validate_Accuracy: 0.5875\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.674026221036911 \t Validate_Accuracy: 0.5945\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6801290810108185 \t Validate_Accuracy: 0.6015\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6764094531536102 \t Validate_Accuracy: 0.5995\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6728710532188416 \t Validate_Accuracy: 0.6\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6663309335708618 \t Validate_Accuracy: 0.6095\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6690936088562012 \t Validate_Accuracy: 0.62\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.6674930453300476 \t Validate_Accuracy: 0.6245\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6635959446430206 \t Validate_Accuracy: 0.627\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6575557589530945 \t Validate_Accuracy: 0.637\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6531630158424377 \t Validate_Accuracy: 0.6435\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6475739181041718 \t Validate_Accuracy: 0.6555\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6448233723640442 \t Validate_Accuracy: 0.6695\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6272565722465515 \t Validate_Accuracy: 0.68\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6303017735481262 \t Validate_Accuracy: 0.695\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6217493712902069 \t Validate_Accuracy: 0.7175\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6057455241680145 \t Validate_Accuracy: 0.742\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.5870970487594604 \t Validate_Accuracy: 0.759\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.5829911828041077 \t Validate_Accuracy: 0.772\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.5711411833763123 \t Validate_Accuracy: 0.7905\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.5555452704429626 \t Validate_Accuracy: 0.8025\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.5427277088165283 \t Validate_Accuracy: 0.8105\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.541602611541748 \t Validate_Accuracy: 0.8215\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.525318443775177 \t Validate_Accuracy: 0.826\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.5061517953872681 \t Validate_Accuracy: 0.8315\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.490685299038887 \t Validate_Accuracy: 0.833\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.4651607424020767 \t Validate_Accuracy: 0.8375\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.4609289765357971 \t Validate_Accuracy: 0.835\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.47856009006500244 \t Validate_Accuracy: 0.839\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.4411729723215103 \t Validate_Accuracy: 0.838\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.429277628660202 \t Validate_Accuracy: 0.8355\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.4115075469017029 \t Validate_Accuracy: 0.837\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.4003536254167557 \t Validate_Accuracy: 0.84\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.3914933204650879 \t Validate_Accuracy: 0.8415\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.4044128954410553 \t Validate_Accuracy: 0.836\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.38633449375629425 \t Validate_Accuracy: 0.835\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.3979215919971466 \t Validate_Accuracy: 0.837\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.3965580612421036 \t Validate_Accuracy: 0.834\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.36078542470932007 \t Validate_Accuracy: 0.84\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.36723048985004425 \t Validate_Accuracy: 0.84\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.3518080711364746 \t Validate_Accuracy: 0.842\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.37049591541290283 \t Validate_Accuracy: 0.841\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.36375920474529266 \t Validate_Accuracy: 0.8425\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.35173967480659485 \t Validate_Accuracy: 0.845\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.371295303106308 \t Validate_Accuracy: 0.8475\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.3486792743206024 \t Validate_Accuracy: 0.8495\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.34127376973629 \t Validate_Accuracy: 0.851\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.3280526101589203 \t Validate_Accuracy: 0.851\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.33024053275585175 \t Validate_Accuracy: 0.8515\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.36204689741134644 \t Validate_Accuracy: 0.851\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.3510080873966217 \t Validate_Accuracy: 0.8505\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.30149537324905396 \t Validate_Accuracy: 0.852\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.34420888125896454 \t Validate_Accuracy: 0.8545\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.3236815333366394 \t Validate_Accuracy: 0.856\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.3163122683763504 \t Validate_Accuracy: 0.854\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.3161536455154419 \t Validate_Accuracy: 0.8545\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.30082041025161743 \t Validate_Accuracy: 0.8595\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.29078733921051025 \t Validate_Accuracy: 0.859\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.32062506675720215 \t Validate_Accuracy: 0.8605\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.30038952827453613 \t Validate_Accuracy: 0.86\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2980660945177078 \t Validate_Accuracy: 0.861\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.3022974729537964 \t Validate_Accuracy: 0.862\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.3004506230354309 \t Validate_Accuracy: 0.864\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.2736665606498718 \t Validate_Accuracy: 0.863\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.29054848849773407 \t Validate_Accuracy: 0.8595\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.26975254714488983 \t Validate_Accuracy: 0.86\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.3005959391593933 \t Validate_Accuracy: 0.863\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.29845069348812103 \t Validate_Accuracy: 0.861\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.27458009123802185 \t Validate_Accuracy: 0.8645\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.28175824880599976 \t Validate_Accuracy: 0.8675\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.28988099098205566 \t Validate_Accuracy: 0.864\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2843942940235138 \t Validate_Accuracy: 0.864\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.30089040100574493 \t Validate_Accuracy: 0.8665\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2734946310520172 \t Validate_Accuracy: 0.866\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.28559403121471405 \t Validate_Accuracy: 0.8665\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.26370571553707123 \t Validate_Accuracy: 0.866\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.28482238948345184 \t Validate_Accuracy: 0.8685\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.2699064612388611 \t Validate_Accuracy: 0.868\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.27949364483356476 \t Validate_Accuracy: 0.8665\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2615623250603676 \t Validate_Accuracy: 0.866\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.2498670592904091 \t Validate_Accuracy: 0.868\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2587393969297409 \t Validate_Accuracy: 0.8705\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2988351285457611 \t Validate_Accuracy: 0.868\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.27059654891490936 \t Validate_Accuracy: 0.867\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.28196871280670166 \t Validate_Accuracy: 0.8655\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.27254965901374817 \t Validate_Accuracy: 0.8695\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.26152874529361725 \t Validate_Accuracy: 0.868\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2732491046190262 \t Validate_Accuracy: 0.8685\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2549150139093399 \t Validate_Accuracy: 0.867\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.26355843245983124 \t Validate_Accuracy: 0.8715\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.2724006026983261 \t Validate_Accuracy: 0.872\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.26288117468357086 \t Validate_Accuracy: 0.866\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2362043783068657 \t Validate_Accuracy: 0.8685\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.2619708925485611 \t Validate_Accuracy: 0.864\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2534213289618492 \t Validate_Accuracy: 0.8665\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.26521384716033936 \t Validate_Accuracy: 0.866\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.25019872933626175 \t Validate_Accuracy: 0.863\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.25159625709056854 \t Validate_Accuracy: 0.865\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2521643117070198 \t Validate_Accuracy: 0.867\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.27228110283613205 \t Validate_Accuracy: 0.8635\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.26772183179855347 \t Validate_Accuracy: 0.8645\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.24230092018842697 \t Validate_Accuracy: 0.8665\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.25687234103679657 \t Validate_Accuracy: 0.876\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.23450036346912384 \t Validate_Accuracy: 0.8745\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.22289080172777176 \t Validate_Accuracy: 0.8725\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.25691458582878113 \t Validate_Accuracy: 0.8705\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.24105890095233917 \t Validate_Accuracy: 0.869\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.25954119861125946 \t Validate_Accuracy: 0.8685\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.23787549138069153 \t Validate_Accuracy: 0.8715\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.27468081563711166 \t Validate_Accuracy: 0.8685\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.25804150104522705 \t Validate_Accuracy: 0.8735\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.2534494623541832 \t Validate_Accuracy: 0.874\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.24872423708438873 \t Validate_Accuracy: 0.8765\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.25541166961193085 \t Validate_Accuracy: 0.874\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.23636440932750702 \t Validate_Accuracy: 0.875\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.2548440024256706 \t Validate_Accuracy: 0.873\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.25412438809871674 \t Validate_Accuracy: 0.8725\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.23499581962823868 \t Validate_Accuracy: 0.8745\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.243977390229702 \t Validate_Accuracy: 0.87\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.2565321773290634 \t Validate_Accuracy: 0.8725\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.2316967025399208 \t Validate_Accuracy: 0.8745\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.23689629882574081 \t Validate_Accuracy: 0.8715\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.21709265559911728 \t Validate_Accuracy: 0.873\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.25074394047260284 \t Validate_Accuracy: 0.8715\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.25743579119443893 \t Validate_Accuracy: 0.8735\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.24792438000440598 \t Validate_Accuracy: 0.8735\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.23140326142311096 \t Validate_Accuracy: 0.8735\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.24644284695386887 \t Validate_Accuracy: 0.871\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.24527215212583542 \t Validate_Accuracy: 0.8715\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.2351963147521019 \t Validate_Accuracy: 0.8735\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.23108859360218048 \t Validate_Accuracy: 0.8725\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.2189643606543541 \t Validate_Accuracy: 0.872\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.21560156345367432 \t Validate_Accuracy: 0.8675\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.23169313371181488 \t Validate_Accuracy: 0.8705\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.2422437146306038 \t Validate_Accuracy: 0.8675\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.2487378939986229 \t Validate_Accuracy: 0.868\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.2586381733417511 \t Validate_Accuracy: 0.871\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.23091135919094086 \t Validate_Accuracy: 0.8745\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.25294229388237 \t Validate_Accuracy: 0.8715\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.24045950919389725 \t Validate_Accuracy: 0.874\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.23464922606945038 \t Validate_Accuracy: 0.8755\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.2322346642613411 \t Validate_Accuracy: 0.8775\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2282712459564209 \t Validate_Accuracy: 0.8765\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1734, -0.0980, -0.2365],\n","          [-0.1197,  0.0197, -0.0855],\n","          [-0.1687, -0.0804, -0.2285]]]])\n","conv1.bias tensor([0.1117])\n","first_linear.weight tensor([[ 2.7512e-01,  5.3142e-01,  2.9792e-01,  4.6817e-02,  3.0341e-01,\n","          2.1575e-01,  4.6451e-02, -3.8487e-02,  1.0657e-01],\n","        [-1.1371e-02,  3.9179e-01, -4.3400e-01,  2.7440e-01, -5.5519e-01,\n","          9.9717e-01, -1.3625e-01, -1.0975e-01, -4.0981e-01],\n","        [-1.5797e-01, -6.9244e-02,  1.0181e-01, -3.5819e-01,  5.9233e-01,\n","         -5.4010e-01,  3.7349e-01, -1.0624e+00,  7.3132e-01],\n","        [-1.8175e-01,  4.8880e-01, -2.3357e-01, -2.1210e-01, -6.0568e-01,\n","          2.4372e-01,  5.0372e-01,  1.3840e-01, -1.7155e-01],\n","        [ 8.5182e-02,  5.6441e-02,  2.3633e-02,  1.6142e-01, -2.2436e-01,\n","          3.9845e-01, -9.8465e-02,  1.4589e-01, -2.9834e-01],\n","        [-9.2718e-02, -8.0230e-02, -3.5207e-02, -2.4669e-01, -1.8847e-01,\n","         -1.6863e-01, -1.2940e-01, -1.1543e-01, -1.8892e-01],\n","        [-2.5858e-01,  1.4291e-01, -8.6154e-02,  4.8972e-01,  4.1787e-01,\n","         -1.2269e-01, -5.9792e-01, -1.2480e-01,  2.6662e-01],\n","        [-1.8702e-01,  2.4539e-01, -3.4129e-01, -3.8769e-01,  3.8133e-01,\n","          3.4775e-01,  8.8975e-01, -1.0798e+00,  1.3132e-01],\n","        [-3.1642e-01, -1.1541e-03,  7.2263e-01,  6.6573e-01, -2.1315e-01,\n","         -9.4030e-01, -2.5628e-01, -4.0051e-03,  4.1740e-01],\n","        [-4.8749e-01, -8.4994e-02, -2.3265e-01,  1.3201e+00, -2.8054e-01,\n","          4.4545e-01, -3.1898e-01, -1.5590e-01, -1.6368e-02]])\n","first_linear.bias tensor([ 0.9811,  0.4761, -0.8592, -0.5014,  0.0228,  0.9170,  0.3518,  0.7830,\n","         0.7250, -0.8817])\n","linear_hidden.0.weight tensor([[ 0.5523,  0.4730, -0.2773,  0.5346,  0.4458,  0.2891, -0.1716, -0.6355,\n","         -0.6541,  0.9154],\n","        [-0.7502,  0.2580, -0.5444, -0.2527,  0.0543, -0.7358,  0.2016,  0.4639,\n","          0.4926, -0.6069],\n","        [ 0.4834, -0.6539,  0.7271, -0.4745, -0.3087,  0.3714,  0.3796, -0.7680,\n","          0.1178,  0.7144],\n","        [-0.7695,  0.3219, -0.5154, -0.2165,  0.0808, -0.6904,  0.2499,  0.5141,\n","          0.5796, -0.6197],\n","        [ 0.8468, -0.4155,  0.3800,  0.2902, -0.2287,  0.6255, -0.2268, -0.5242,\n","         -0.6955,  0.6850],\n","        [ 0.8608, -0.3213,  0.3836,  0.1918, -0.2074,  0.6941, -0.2849, -0.5766,\n","         -0.6609,  0.6457],\n","        [-0.5960,  0.2456, -0.4118, -0.2177, -0.0269, -0.4139,  0.1940,  0.3642,\n","          0.3231, -0.3841],\n","        [ 0.6568, -0.2789,  0.4587,  0.2765,  0.0416,  0.6964, -0.1384, -0.3795,\n","         -0.3468,  0.4296],\n","        [-0.7339,  0.3128, -0.5569, -0.2466, -0.0124, -0.7257,  0.2453,  0.5262,\n","          0.4808, -0.5327],\n","        [ 0.6529, -0.3293,  0.4138,  0.2918, -0.0652,  0.4769, -0.2968, -0.4342,\n","         -0.3852,  0.0713]])\n","linear_hidden.0.bias tensor([-0.0420, -0.4310,  0.0809, -0.5255,  0.5965,  0.4761, -0.4270,  0.2807,\n","        -0.4627,  0.1617])\n","linear_output.weight tensor([[-0.7418,  1.1656, -0.9867,  1.2108, -1.1699, -1.0824,  0.5660, -0.7943,\n","          1.0652, -0.6414]])\n","linear_output.bias tensor([-0.4586])\n","Testing out: \n","batch_size:  1463\n","train_size:  2757\n","n_epochs:  110\n","lr:  0.031164504439018484\n","weight_decay:  0.002358658403672599\n","betas0:  0.9999841235141775\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[ 0.1873, -0.1313,  0.2719],\n","          [ 0.2775,  0.0295,  0.1248],\n","          [ 0.0658, -0.2576,  0.2458]]]])\n","conv1.bias tensor([-0.1092])\n","first_linear.weight tensor([[-0.1075, -0.0862, -0.1845,  0.0542,  0.3159, -0.2695,  0.1561, -0.0400,\n","          0.3057],\n","        [-0.1261,  0.0323, -0.0865,  0.2021, -0.3056,  0.2274,  0.0209,  0.0809,\n","          0.0738],\n","        [-0.2932, -0.1702, -0.1219, -0.1508,  0.1837, -0.2025, -0.0858, -0.1961,\n","          0.2044],\n","        [-0.0576, -0.2564,  0.2769, -0.2405,  0.2314, -0.0933, -0.0009, -0.0170,\n","          0.2880],\n","        [ 0.2906, -0.1356,  0.2550,  0.1631,  0.3008, -0.2270,  0.2972, -0.1099,\n","         -0.0841],\n","        [ 0.3211, -0.1492, -0.2203, -0.0636, -0.0545, -0.0041, -0.3235, -0.0028,\n","          0.0794],\n","        [-0.3207,  0.0206, -0.2503, -0.3068, -0.0496, -0.1847, -0.2319,  0.0256,\n","          0.0336],\n","        [ 0.0932, -0.2427,  0.2479, -0.1976, -0.0925, -0.1314, -0.1130,  0.0768,\n","          0.2577],\n","        [-0.2669,  0.2130,  0.1321, -0.0849, -0.2446, -0.2120, -0.0011, -0.2718,\n","          0.2091],\n","        [-0.1881, -0.2330,  0.1992,  0.3175,  0.1539,  0.3260,  0.1761,  0.0475,\n","         -0.1719]])\n","first_linear.bias tensor([ 0.3002, -0.3300, -0.1181,  0.2724, -0.0888,  0.1047, -0.1368, -0.0557,\n","        -0.3087, -0.0956])\n","linear_hidden.0.weight tensor([[ 0.0082,  0.2914, -0.2131, -0.0683, -0.1694,  0.0840,  0.0628, -0.1525,\n","          0.3121, -0.1754],\n","        [-0.2449,  0.0137,  0.2930,  0.1590, -0.1841, -0.0742,  0.2974,  0.1037,\n","         -0.1805,  0.2354],\n","        [-0.2390,  0.1244,  0.3037, -0.0886, -0.0227, -0.2380, -0.3162,  0.2078,\n","          0.1082, -0.1024],\n","        [ 0.1621, -0.2643,  0.1972,  0.1770, -0.2269, -0.1012,  0.0540,  0.2392,\n","         -0.1249, -0.0890],\n","        [-0.3108, -0.1418,  0.2045, -0.2118,  0.3054,  0.2908,  0.0822,  0.2501,\n","          0.1790,  0.0344],\n","        [ 0.1851, -0.2941, -0.2259,  0.0896, -0.2868, -0.0877,  0.1810,  0.2109,\n","         -0.0595, -0.1390],\n","        [-0.2722,  0.0074, -0.2266,  0.1409, -0.1156, -0.0909, -0.2441,  0.2355,\n","         -0.2997,  0.0075],\n","        [ 0.1377,  0.2247, -0.1006, -0.1500, -0.1894,  0.2731, -0.2465, -0.2767,\n","          0.1922, -0.1828],\n","        [ 0.2555, -0.1807, -0.2742, -0.0463,  0.1361,  0.0782, -0.2976, -0.1860,\n","          0.1155,  0.2120],\n","        [ 0.2776, -0.1402,  0.0079, -0.0926,  0.0093, -0.1121,  0.1730,  0.1225,\n","          0.0100,  0.2754]])\n","linear_hidden.0.bias tensor([ 0.0237,  0.1462, -0.0887, -0.1835, -0.2846,  0.1464, -0.0113, -0.1668,\n","         0.2820, -0.1872])\n","linear_output.weight tensor([[-0.1908, -0.2603, -0.2674,  0.3034, -0.2817, -0.2478, -0.2844,  0.1157,\n","          0.0160, -0.0240]])\n","linear_output.bias tensor([0.0661])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1463])) that is different to the input size (torch.Size([1463, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1294])) that is different to the input size (torch.Size([1294, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.6939660906791687 \t Validate_Accuracy: 0.4745\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6867953836917877 \t Validate_Accuracy: 0.545\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.6760939657688141 \t Validate_Accuracy: 0.591\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6601088047027588 \t Validate_Accuracy: 0.746\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6356361508369446 \t Validate_Accuracy: 0.8005\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6026951968669891 \t Validate_Accuracy: 0.8095\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.5658406615257263 \t Validate_Accuracy: 0.8165\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.5281037390232086 \t Validate_Accuracy: 0.8195\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.4904510825872421 \t Validate_Accuracy: 0.821\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.45811907947063446 \t Validate_Accuracy: 0.812\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.4330664128065109 \t Validate_Accuracy: 0.8255\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.4152846485376358 \t Validate_Accuracy: 0.8205\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.40175390243530273 \t Validate_Accuracy: 0.829\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.3892313092947006 \t Validate_Accuracy: 0.8305\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.37824925780296326 \t Validate_Accuracy: 0.829\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.3664791136980057 \t Validate_Accuracy: 0.83\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.36208027601242065 \t Validate_Accuracy: 0.829\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.35553935170173645 \t Validate_Accuracy: 0.835\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.3462408035993576 \t Validate_Accuracy: 0.83\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.34075193107128143 \t Validate_Accuracy: 0.836\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3356846421957016 \t Validate_Accuracy: 0.8385\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3295392096042633 \t Validate_Accuracy: 0.8325\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.327433317899704 \t Validate_Accuracy: 0.845\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3222869038581848 \t Validate_Accuracy: 0.8525\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.3169276863336563 \t Validate_Accuracy: 0.8545\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.30868057906627655 \t Validate_Accuracy: 0.851\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.3062758594751358 \t Validate_Accuracy: 0.8495\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.30695199966430664 \t Validate_Accuracy: 0.8555\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.2981450706720352 \t Validate_Accuracy: 0.858\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.2972165197134018 \t Validate_Accuracy: 0.862\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.29090793430805206 \t Validate_Accuracy: 0.8585\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.2886974662542343 \t Validate_Accuracy: 0.863\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.2881455421447754 \t Validate_Accuracy: 0.8595\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.2841460108757019 \t Validate_Accuracy: 0.8585\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.28069981932640076 \t Validate_Accuracy: 0.8555\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2820770740509033 \t Validate_Accuracy: 0.8655\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.2820684313774109 \t Validate_Accuracy: 0.859\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.27807411551475525 \t Validate_Accuracy: 0.858\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.27387991547584534 \t Validate_Accuracy: 0.87\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.27571193873882294 \t Validate_Accuracy: 0.863\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2720162719488144 \t Validate_Accuracy: 0.8655\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.2689400464296341 \t Validate_Accuracy: 0.871\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2699871063232422 \t Validate_Accuracy: 0.867\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2697213888168335 \t Validate_Accuracy: 0.867\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2662642151117325 \t Validate_Accuracy: 0.8685\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.2653852552175522 \t Validate_Accuracy: 0.869\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.26325467228889465 \t Validate_Accuracy: 0.872\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.26061998307704926 \t Validate_Accuracy: 0.8715\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.2577725648880005 \t Validate_Accuracy: 0.871\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.25813719630241394 \t Validate_Accuracy: 0.872\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.25865666568279266 \t Validate_Accuracy: 0.8705\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.25547899305820465 \t Validate_Accuracy: 0.875\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.2555164247751236 \t Validate_Accuracy: 0.8765\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2515825033187866 \t Validate_Accuracy: 0.865\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2541930675506592 \t Validate_Accuracy: 0.873\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.25192467123270035 \t Validate_Accuracy: 0.878\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2471483126282692 \t Validate_Accuracy: 0.877\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.24643821269273758 \t Validate_Accuracy: 0.8735\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.2477814182639122 \t Validate_Accuracy: 0.8765\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2436552494764328 \t Validate_Accuracy: 0.877\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2448907047510147 \t Validate_Accuracy: 0.8755\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.24568841606378555 \t Validate_Accuracy: 0.878\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.24275106191635132 \t Validate_Accuracy: 0.874\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.24715358763933182 \t Validate_Accuracy: 0.877\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.2466890811920166 \t Validate_Accuracy: 0.8785\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.24585697054862976 \t Validate_Accuracy: 0.878\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.24803117662668228 \t Validate_Accuracy: 0.8745\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.24893491715192795 \t Validate_Accuracy: 0.871\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.24381009489297867 \t Validate_Accuracy: 0.8765\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.24666519463062286 \t Validate_Accuracy: 0.8765\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.24546808749437332 \t Validate_Accuracy: 0.8755\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.24259870499372482 \t Validate_Accuracy: 0.8705\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.24453487992286682 \t Validate_Accuracy: 0.877\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.24286171048879623 \t Validate_Accuracy: 0.878\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.24252469092607498 \t Validate_Accuracy: 0.876\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.243881456553936 \t Validate_Accuracy: 0.8765\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.24318478256464005 \t Validate_Accuracy: 0.8735\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.24274639040231705 \t Validate_Accuracy: 0.879\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2415158450603485 \t Validate_Accuracy: 0.877\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.2434803992509842 \t Validate_Accuracy: 0.8735\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.24115728586912155 \t Validate_Accuracy: 0.8815\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.24191439151763916 \t Validate_Accuracy: 0.881\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.24110838770866394 \t Validate_Accuracy: 0.876\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.24043752998113632 \t Validate_Accuracy: 0.8745\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.24003581702709198 \t Validate_Accuracy: 0.8805\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.24212349206209183 \t Validate_Accuracy: 0.876\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2398538962006569 \t Validate_Accuracy: 0.873\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.24036452919244766 \t Validate_Accuracy: 0.8795\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.24222100526094437 \t Validate_Accuracy: 0.88\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.24057388305664062 \t Validate_Accuracy: 0.877\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.24094924330711365 \t Validate_Accuracy: 0.8795\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.2381327897310257 \t Validate_Accuracy: 0.881\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.23741070181131363 \t Validate_Accuracy: 0.879\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.23701101541519165 \t Validate_Accuracy: 0.8795\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.23861853778362274 \t Validate_Accuracy: 0.881\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.23641258478164673 \t Validate_Accuracy: 0.881\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.23756350576877594 \t Validate_Accuracy: 0.8855\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.23581822961568832 \t Validate_Accuracy: 0.8855\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.23429934680461884 \t Validate_Accuracy: 0.886\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.23325928300619125 \t Validate_Accuracy: 0.8805\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.23298361152410507 \t Validate_Accuracy: 0.8845\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.23187971860170364 \t Validate_Accuracy: 0.8855\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.23470517247915268 \t Validate_Accuracy: 0.8885\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.23802398890256882 \t Validate_Accuracy: 0.879\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.24140813201665878 \t Validate_Accuracy: 0.8825\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2433447688817978 \t Validate_Accuracy: 0.8865\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.24009303748607635 \t Validate_Accuracy: 0.8785\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2377673164010048 \t Validate_Accuracy: 0.8815\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.23795946687459946 \t Validate_Accuracy: 0.877\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.23240910470485687 \t Validate_Accuracy: 0.8815\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.1962,  0.1034,  0.2484],\n","          [ 0.1002, -0.0298,  0.0784],\n","          [ 0.1864,  0.0653,  0.2630]]]])\n","conv1.bias tensor([-0.0619])\n","first_linear.weight tensor([[-0.0558, -0.2309,  0.1019, -0.4412,  0.8316, -0.5124,  0.5143, -1.1422,\n","          0.6507],\n","        [ 0.0300,  0.1899,  0.2884, -0.1187,  0.2259,  0.3328,  0.1200,  0.0812,\n","          0.2785],\n","        [ 0.0414,  0.1051,  0.1427, -0.0127,  0.0831,  0.1079, -0.0981, -0.2053,\n","          0.0097],\n","        [-0.0253,  0.5180,  0.0845, -0.4862, -0.1475, -0.4642, -0.1325,  0.5080,\n","          0.1153],\n","        [ 0.3287,  0.2749,  0.2772,  0.2952,  0.2186, -0.0250, -0.0293,  0.0847,\n","          0.0129],\n","        [ 0.7571, -0.7753, -0.0258, -0.6532,  0.2093,  0.5093,  0.0367,  0.3283,\n","         -0.4560],\n","        [-0.1025, -0.0465,  0.0027, -0.2067, -0.3019, -0.2589, -0.2344, -0.2563,\n","         -0.2600],\n","        [-0.0452, -0.2175, -0.2127, -0.1504, -0.1501, -0.2622,  0.1947,  0.2339,\n","         -0.1345],\n","        [ 0.4548, -0.7123,  0.7344, -0.3319,  0.4747, -0.9348, -0.2630,  0.2695,\n","          0.2065],\n","        [ 0.2461,  0.3343, -0.2560, -1.3495,  0.2777,  0.1248,  0.7801, -0.3461,\n","         -0.1142]])\n","first_linear.bias tensor([ 1.0682, -0.7168,  0.5673,  0.5638, -0.7284,  0.9286, -0.8747, -0.4271,\n","        -1.0299, -0.9953])\n","linear_hidden.0.weight tensor([[-0.5202, -0.4177,  0.2901, -0.2801, -0.4915, -0.4700, -0.5523, -0.3180,\n","          0.5281,  0.5181],\n","        [-0.2796, -0.2197,  0.1524, -0.2454, -0.1985, -0.3224, -0.1927, -0.1213,\n","          0.2933,  0.3180],\n","        [-0.5395, -0.4294,  0.3055, -0.2759, -0.5011, -0.4746, -0.6223, -0.2952,\n","          0.5258,  0.5161],\n","        [ 0.3169,  0.2327, -0.1159,  0.2616,  0.2206,  0.3642,  0.2666,  0.1760,\n","         -0.3566, -0.3816],\n","        [-0.4817, -0.1662,  0.1404, -0.2677, -0.1728,  0.2083, -0.3473, -0.0473,\n","          0.4631,  0.5326],\n","        [-0.2828, -0.2222,  0.1538, -0.2468, -0.2023, -0.3243, -0.1931, -0.1230,\n","          0.2951,  0.3190],\n","        [-0.5177, -0.4347,  0.2791, -0.2685, -0.4529, -0.4474, -0.5696, -0.2980,\n","          0.4762,  0.5039],\n","        [ 0.2688,  0.2556, -0.1788,  0.2407,  0.2395,  0.3279,  0.2680,  0.1583,\n","         -0.3098, -0.3337],\n","        [-0.4297, -0.3348,  0.2256, -0.2958, -0.3425, -0.4331, -0.3871, -0.2366,\n","          0.4504,  0.4629],\n","        [ 0.4356,  0.3964, -0.2615,  0.2652,  0.3966,  0.4168,  0.4771,  0.2717,\n","         -0.4511, -0.4566]])\n","linear_hidden.0.bias tensor([ 0.2805,  0.4039,  0.2380, -0.4667,  0.1699,  0.4033,  0.2378, -0.2947,\n","         0.4142, -0.2497])\n","linear_output.weight tensor([[-1.4210, -0.7878, -1.4272,  0.9553, -0.8756, -0.7943, -1.3526,  0.8267,\n","         -1.2217,  1.2231]])\n","linear_output.bias tensor([0.0199])\n","Testing out: \n","batch_size:  2106\n","train_size:  3990\n","n_epochs:  176\n","lr:  0.003333050187451363\n","weight_decay:  0.00011420573148637796\n","betas0:  0.9972077400428084\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.1453,  0.0397, -0.0782],\n","          [-0.2956,  0.2267,  0.0414],\n","          [-0.0744, -0.2021,  0.0844]]]])\n","conv1.bias tensor([0.1535])\n","first_linear.weight tensor([[ 0.1147,  0.1528,  0.1833, -0.1627,  0.2714,  0.2438,  0.1734, -0.3131,\n","         -0.2318],\n","        [ 0.2106,  0.2265,  0.0607, -0.1519,  0.0190, -0.2906, -0.1920,  0.0836,\n","         -0.2622],\n","        [-0.1395, -0.0508, -0.0660,  0.1388, -0.1423,  0.1271,  0.0431,  0.1840,\n","          0.1542],\n","        [-0.1122,  0.0108, -0.0775, -0.1502,  0.0142,  0.2653,  0.1955, -0.1806,\n","         -0.2849],\n","        [ 0.2289, -0.3191, -0.2277, -0.2059, -0.0283, -0.2074, -0.2252,  0.1796,\n","          0.1837],\n","        [-0.2439, -0.2340, -0.1185,  0.3316, -0.1495, -0.0057, -0.2708,  0.0407,\n","          0.0153],\n","        [-0.0090, -0.2442,  0.1378,  0.2889,  0.0843,  0.2496,  0.2115, -0.3266,\n","          0.2868],\n","        [-0.2554, -0.0584, -0.0649,  0.1400, -0.0504, -0.3219,  0.0003, -0.3315,\n","         -0.2847],\n","        [-0.1046, -0.2469, -0.1188,  0.1797,  0.0844, -0.2208,  0.2745,  0.2343,\n","          0.2260],\n","        [-0.3007, -0.2748,  0.1410,  0.3227,  0.1235,  0.0165, -0.0202,  0.1479,\n","          0.0499]])\n","first_linear.bias tensor([-0.2297, -0.2467,  0.0518, -0.0277, -0.1770, -0.0317, -0.2168, -0.0443,\n","         0.0769,  0.3268])\n","linear_hidden.0.weight tensor([[-0.0229,  0.0496,  0.1333,  0.2803,  0.0962,  0.0180, -0.1065, -0.0824,\n","          0.1195, -0.2166],\n","        [-0.2393,  0.1466,  0.0323,  0.0520, -0.2302, -0.1232, -0.2394,  0.1524,\n","          0.0048, -0.1168],\n","        [-0.0801, -0.1603,  0.0853, -0.0784,  0.0770, -0.2783,  0.3133, -0.2146,\n","         -0.1040,  0.1280],\n","        [ 0.2420, -0.2973, -0.0294,  0.0117, -0.0879,  0.1111,  0.1987,  0.0852,\n","          0.2765, -0.2961],\n","        [-0.1699,  0.2797, -0.2525,  0.1278, -0.0638, -0.0317,  0.1143, -0.0635,\n","          0.2440,  0.1126],\n","        [ 0.1696, -0.0176,  0.1807, -0.1502, -0.1715, -0.2920, -0.0149,  0.0583,\n","         -0.1576,  0.1814],\n","        [ 0.1564, -0.2851, -0.0940, -0.0672,  0.0091,  0.2187, -0.1735, -0.0583,\n","          0.0134,  0.1513],\n","        [ 0.2215,  0.0016, -0.1542,  0.0912,  0.2295,  0.0252, -0.1524,  0.0154,\n","          0.1350,  0.2566],\n","        [ 0.1393, -0.1798, -0.2619, -0.1201,  0.0701, -0.3098, -0.0927,  0.1253,\n","          0.2482, -0.2437],\n","        [ 0.2787, -0.0455,  0.2979,  0.0531,  0.2112, -0.0871, -0.0766, -0.0701,\n","          0.2203, -0.1065]])\n","linear_hidden.0.bias tensor([-0.2241, -0.0847,  0.0459, -0.0394, -0.2282,  0.2260, -0.2565, -0.1060,\n","         0.0433, -0.1433])\n","linear_output.weight tensor([[-0.0102, -0.1450, -0.2275,  0.1386, -0.2091, -0.2992, -0.0325,  0.2710,\n","         -0.1660,  0.2328]])\n","linear_output.bias tensor([-0.2779])\n","epoch 1\n","Epoch: 1 \t Train Loss: 0.7143009305000305 \t Validate_Accuracy: 0.503\n","epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([2106])) that is different to the input size (torch.Size([2106, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1884])) that is different to the input size (torch.Size([1884, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.7087441682815552 \t Validate_Accuracy: 0.503\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.7046214938163757 \t Validate_Accuracy: 0.503\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.7011540532112122 \t Validate_Accuracy: 0.503\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.6985942721366882 \t Validate_Accuracy: 0.503\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.6967005729675293 \t Validate_Accuracy: 0.497\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.6951795220375061 \t Validate_Accuracy: 0.5105\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.6941710412502289 \t Validate_Accuracy: 0.5245\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.6935839653015137 \t Validate_Accuracy: 0.517\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.6931160688400269 \t Validate_Accuracy: 0.4895\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.6929435133934021 \t Validate_Accuracy: 0.482\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.6928936541080475 \t Validate_Accuracy: 0.4985\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.692970871925354 \t Validate_Accuracy: 0.5135\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.6928998827934265 \t Validate_Accuracy: 0.515\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.6928701996803284 \t Validate_Accuracy: 0.5155\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.6926772594451904 \t Validate_Accuracy: 0.523\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.6924467980861664 \t Validate_Accuracy: 0.521\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.6922074556350708 \t Validate_Accuracy: 0.5195\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.6920643746852875 \t Validate_Accuracy: 0.504\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.6918044090270996 \t Validate_Accuracy: 0.5035\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.6915742754936218 \t Validate_Accuracy: 0.5075\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.6912845373153687 \t Validate_Accuracy: 0.517\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.6910055875778198 \t Validate_Accuracy: 0.519\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.6906780302524567 \t Validate_Accuracy: 0.5145\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.6902647018432617 \t Validate_Accuracy: 0.52\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.689744770526886 \t Validate_Accuracy: 0.5265\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.6891682147979736 \t Validate_Accuracy: 0.535\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.6886511147022247 \t Validate_Accuracy: 0.553\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.6878528296947479 \t Validate_Accuracy: 0.5755\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.6870472133159637 \t Validate_Accuracy: 0.588\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.6862713098526001 \t Validate_Accuracy: 0.6015\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.6849965453147888 \t Validate_Accuracy: 0.6075\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.6837225556373596 \t Validate_Accuracy: 0.614\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.6824549436569214 \t Validate_Accuracy: 0.6155\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.6807496249675751 \t Validate_Accuracy: 0.621\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.6787588894367218 \t Validate_Accuracy: 0.6205\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.6763787269592285 \t Validate_Accuracy: 0.6295\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.6737185120582581 \t Validate_Accuracy: 0.6335\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.6705511808395386 \t Validate_Accuracy: 0.64\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.6671286523342133 \t Validate_Accuracy: 0.6475\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.6629502177238464 \t Validate_Accuracy: 0.652\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.6580718457698822 \t Validate_Accuracy: 0.6695\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.652851402759552 \t Validate_Accuracy: 0.679\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.646820604801178 \t Validate_Accuracy: 0.695\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.6399568915367126 \t Validate_Accuracy: 0.7075\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.6323497891426086 \t Validate_Accuracy: 0.723\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.6239152252674103 \t Validate_Accuracy: 0.734\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.6147581040859222 \t Validate_Accuracy: 0.747\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.6045530140399933 \t Validate_Accuracy: 0.764\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.5942256450653076 \t Validate_Accuracy: 0.7705\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.5825236737728119 \t Validate_Accuracy: 0.779\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.5706391334533691 \t Validate_Accuracy: 0.79\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.5575585961341858 \t Validate_Accuracy: 0.794\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.5453034937381744 \t Validate_Accuracy: 0.8005\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.5312969386577606 \t Validate_Accuracy: 0.809\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.5179851651191711 \t Validate_Accuracy: 0.814\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.5043525695800781 \t Validate_Accuracy: 0.8135\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.49062903225421906 \t Validate_Accuracy: 0.817\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.4769012928009033 \t Validate_Accuracy: 0.8195\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.4642030745744705 \t Validate_Accuracy: 0.824\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.45106208324432373 \t Validate_Accuracy: 0.8265\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.4392698109149933 \t Validate_Accuracy: 0.828\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.4283449500799179 \t Validate_Accuracy: 0.831\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.41798630356788635 \t Validate_Accuracy: 0.831\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.4083728641271591 \t Validate_Accuracy: 0.833\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.39983563125133514 \t Validate_Accuracy: 0.837\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.3920246809720993 \t Validate_Accuracy: 0.8395\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.3851791322231293 \t Validate_Accuracy: 0.8415\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.3777396082878113 \t Validate_Accuracy: 0.842\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.3725758343935013 \t Validate_Accuracy: 0.8405\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.36598698794841766 \t Validate_Accuracy: 0.843\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.36051611602306366 \t Validate_Accuracy: 0.846\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.35582055151462555 \t Validate_Accuracy: 0.8455\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.3509193956851959 \t Validate_Accuracy: 0.846\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.3459988534450531 \t Validate_Accuracy: 0.8475\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.3410540074110031 \t Validate_Accuracy: 0.8495\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.336932048201561 \t Validate_Accuracy: 0.853\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.3328206241130829 \t Validate_Accuracy: 0.8575\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.3278794437646866 \t Validate_Accuracy: 0.86\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.3246825486421585 \t Validate_Accuracy: 0.861\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.3199782818555832 \t Validate_Accuracy: 0.859\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.31708453595638275 \t Validate_Accuracy: 0.861\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.31410349905490875 \t Validate_Accuracy: 0.862\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.3103519231081009 \t Validate_Accuracy: 0.865\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.3054565042257309 \t Validate_Accuracy: 0.8615\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.30242908000946045 \t Validate_Accuracy: 0.8625\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.29981911182403564 \t Validate_Accuracy: 0.864\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2967197299003601 \t Validate_Accuracy: 0.8645\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.29359428584575653 \t Validate_Accuracy: 0.865\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.29087795317173004 \t Validate_Accuracy: 0.8655\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.2873876988887787 \t Validate_Accuracy: 0.866\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.28573453426361084 \t Validate_Accuracy: 0.8665\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.28337496519088745 \t Validate_Accuracy: 0.8685\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.2801337093114853 \t Validate_Accuracy: 0.867\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.27845342457294464 \t Validate_Accuracy: 0.8695\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.2762622833251953 \t Validate_Accuracy: 0.8685\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.27373461425304413 \t Validate_Accuracy: 0.869\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.2715989798307419 \t Validate_Accuracy: 0.8675\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.2709943652153015 \t Validate_Accuracy: 0.866\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.2674602270126343 \t Validate_Accuracy: 0.8675\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2662980258464813 \t Validate_Accuracy: 0.87\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.2640765905380249 \t Validate_Accuracy: 0.87\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.2628657817840576 \t Validate_Accuracy: 0.87\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.261038675904274 \t Validate_Accuracy: 0.8715\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.25938497483730316 \t Validate_Accuracy: 0.8715\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.25811271369457245 \t Validate_Accuracy: 0.872\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2561100646853447 \t Validate_Accuracy: 0.872\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.2552749365568161 \t Validate_Accuracy: 0.873\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.25389283895492554 \t Validate_Accuracy: 0.8735\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.2521327808499336 \t Validate_Accuracy: 0.874\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.25175791233778 \t Validate_Accuracy: 0.876\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.25032736361026764 \t Validate_Accuracy: 0.8795\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2490585297346115 \t Validate_Accuracy: 0.876\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2480311468243599 \t Validate_Accuracy: 0.8775\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.24763565510511398 \t Validate_Accuracy: 0.8785\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.24616654217243195 \t Validate_Accuracy: 0.8795\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.2445303052663803 \t Validate_Accuracy: 0.879\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.24404259026050568 \t Validate_Accuracy: 0.8795\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.24304328858852386 \t Validate_Accuracy: 0.88\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.2418115884065628 \t Validate_Accuracy: 0.8805\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.24124887585639954 \t Validate_Accuracy: 0.8825\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.24035515636205673 \t Validate_Accuracy: 0.881\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2396513596177101 \t Validate_Accuracy: 0.8835\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.23829302936792374 \t Validate_Accuracy: 0.8815\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.23759941011667252 \t Validate_Accuracy: 0.88\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.23748430609703064 \t Validate_Accuracy: 0.881\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.23640427738428116 \t Validate_Accuracy: 0.881\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.2369750216603279 \t Validate_Accuracy: 0.883\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.23568066209554672 \t Validate_Accuracy: 0.883\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.23486080765724182 \t Validate_Accuracy: 0.884\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.23353811353445053 \t Validate_Accuracy: 0.884\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.2336619347333908 \t Validate_Accuracy: 0.885\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.23305729031562805 \t Validate_Accuracy: 0.8845\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.23266670852899551 \t Validate_Accuracy: 0.885\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.23217376321554184 \t Validate_Accuracy: 0.8865\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.23037734627723694 \t Validate_Accuracy: 0.8875\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.23102135956287384 \t Validate_Accuracy: 0.888\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.23019114136695862 \t Validate_Accuracy: 0.8865\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.2298097088932991 \t Validate_Accuracy: 0.8875\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.22872024774551392 \t Validate_Accuracy: 0.8875\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.22972608357667923 \t Validate_Accuracy: 0.888\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.22852560132741928 \t Validate_Accuracy: 0.8875\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.22711073607206345 \t Validate_Accuracy: 0.888\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.2271452099084854 \t Validate_Accuracy: 0.889\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2263418212532997 \t Validate_Accuracy: 0.8885\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.22656012326478958 \t Validate_Accuracy: 0.8895\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.22531022876501083 \t Validate_Accuracy: 0.889\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.22567786276340485 \t Validate_Accuracy: 0.8895\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.22536727041006088 \t Validate_Accuracy: 0.889\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.22583907842636108 \t Validate_Accuracy: 0.8885\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.22355139255523682 \t Validate_Accuracy: 0.889\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.22429442405700684 \t Validate_Accuracy: 0.889\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.22353707998991013 \t Validate_Accuracy: 0.888\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.22304998338222504 \t Validate_Accuracy: 0.8895\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.2224099412560463 \t Validate_Accuracy: 0.8895\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.2229372337460518 \t Validate_Accuracy: 0.8895\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.22208276391029358 \t Validate_Accuracy: 0.889\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.22216330468654633 \t Validate_Accuracy: 0.89\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.22145671397447586 \t Validate_Accuracy: 0.8905\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.2211095690727234 \t Validate_Accuracy: 0.889\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.2215586081147194 \t Validate_Accuracy: 0.889\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.2203536480665207 \t Validate_Accuracy: 0.8905\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.22042115777730942 \t Validate_Accuracy: 0.889\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.21988972276449203 \t Validate_Accuracy: 0.8915\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.21946264058351517 \t Validate_Accuracy: 0.8915\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.21896831691265106 \t Validate_Accuracy: 0.8905\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.21938317269086838 \t Validate_Accuracy: 0.8905\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.2186829373240471 \t Validate_Accuracy: 0.892\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.21848726272583008 \t Validate_Accuracy: 0.891\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.21930110454559326 \t Validate_Accuracy: 0.892\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.21854497492313385 \t Validate_Accuracy: 0.891\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.218107670545578 \t Validate_Accuracy: 0.892\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.21858706325292587 \t Validate_Accuracy: 0.8905\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.21745570749044418 \t Validate_Accuracy: 0.892\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.2177470475435257 \t Validate_Accuracy: 0.891\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.21758201718330383 \t Validate_Accuracy: 0.891\n","model parameters! \n","\n","conv1.weight tensor([[[[-0.1644, -0.0608, -0.1923],\n","          [-0.0713,  0.0074, -0.0503],\n","          [-0.1552, -0.0552, -0.1703]]]])\n","conv1.bias tensor([0.0954])\n","first_linear.weight tensor([[ 0.1751,  0.1759,  0.2931, -0.0219,  0.2503,  0.1898, -0.0253, -0.0282,\n","          0.0945],\n","        [ 0.4623, -0.6726,  0.7066, -0.4011,  0.2281, -0.6234, -0.5358,  0.8788,\n","         -0.3481],\n","        [-0.6628,  0.9029, -0.2572,  0.9160, -0.8350,  0.1030, -0.3323,  0.5649,\n","         -0.0778],\n","        [ 0.4354, -0.4134, -0.2846, -0.6379,  0.1321,  1.0863,  0.7730, -0.3929,\n","         -0.4095],\n","        [-0.3986, -0.1653, -0.0089, -0.1573, -0.2758, -0.1263, -0.2729, -0.2008,\n","         -0.0373],\n","        [-0.3863,  0.0272,  0.0053,  0.7300, -0.8884,  0.1734, -0.5342,  0.2516,\n","          0.0409],\n","        [-0.5747, -0.0430,  0.1524,  0.4907,  0.3179, -0.3150,  0.2284, -1.2789,\n","          0.6252],\n","        [-0.1309, -0.1107, -0.1479, -0.0190, -0.2974, -0.3209, -0.2554, -0.1565,\n","         -0.2011],\n","        [-0.0586, -0.1616,  0.4498, -0.5365,  0.6448, -1.1824,  0.2677, -0.4287,\n","          0.7725],\n","        [ 0.1192, -1.0918,  0.5918,  1.2398,  0.0933, -0.3036, -0.7029,  0.3109,\n","          0.0545]])\n","first_linear.bias tensor([-0.8520, -0.8799,  0.7826,  0.6779, -0.7935, -0.2554, -0.7829, -0.8293,\n","         0.7446,  1.0076])\n","linear_hidden.0.weight tensor([[-0.7789,  0.7036, -0.6635, -0.1147, -0.6414,  0.5771,  0.6096, -0.9610,\n","         -0.7169, -0.7769],\n","        [-0.8623,  0.5608, -0.5083, -0.4495, -0.8208,  0.4597,  0.4234, -0.5752,\n","         -0.6435, -0.6685],\n","        [-0.7433,  0.4023, -0.5745, -0.5225, -0.4793,  0.3138,  0.6305, -0.8935,\n","         -0.7436, -0.4889],\n","        [ 0.9375, -0.5482,  0.6231,  0.5165,  0.5228, -0.2623, -0.5411,  0.7912,\n","          0.8531,  0.4013],\n","        [-0.6525,  0.7882, -0.8264, -0.6963, -0.5178,  0.0769,  0.8108, -0.6577,\n","          0.5715, -0.3424],\n","        [-0.1827, -0.1745,  0.4925, -0.0668, -0.6157, -0.6292, -0.1588, -0.5463,\n","          0.0224,  0.3480],\n","        [-0.5850,  0.2384, -0.0121, -0.7460, -0.6692,  0.8596,  0.5165, -0.8208,\n","         -0.7384, -0.7738],\n","        [ 0.8291, -0.5201,  0.4081,  0.5371,  0.7632, -0.4307, -0.6382,  0.6597,\n","          0.6760,  0.7253],\n","        [-0.3618,  0.5071, -0.9675, -0.0449, -0.2945, -0.7806,  0.5480, -0.4740,\n","          0.0210, -0.9879],\n","        [ 0.9137, -0.4788,  0.7179,  0.4789,  0.7546, -0.4046, -0.5981,  0.5742,\n","          0.7663,  0.4909]])\n","linear_hidden.0.bias tensor([ 0.4905,  0.4084,  0.5144, -0.4867,  0.0222,  0.5138,  0.2218, -0.5224,\n","         0.1473, -0.5560])\n","linear_output.weight tensor([[-0.7896, -1.0012, -1.0760,  0.9201, -0.8761, -0.7454, -0.7833,  1.0941,\n","         -0.8932,  0.9832]])\n","linear_output.bias tensor([-0.4745])\n","Testing out: \n","batch_size:  1419\n","train_size:  3854\n","n_epochs:  250\n","lr:  0.19252370840499847\n","weight_decay:  0.00035201085740259276\n","betas0:  0.7999999999999999\n","betas1:  0.99\n","hidden_size:  10\n","conv1.weight tensor([[[[-0.3281, -0.1243,  0.0736],\n","          [-0.1473, -0.1053,  0.1931],\n","          [-0.2591, -0.0032, -0.1980]]]])\n","conv1.bias tensor([-0.1306])\n","first_linear.weight tensor([[ 0.0393, -0.2527,  0.0796,  0.1277, -0.2036,  0.1787, -0.0948,  0.2653,\n","          0.0247],\n","        [ 0.3174,  0.0593,  0.1652, -0.1198,  0.1343, -0.2701,  0.2963, -0.1637,\n","         -0.3269],\n","        [ 0.0571, -0.1493,  0.1445,  0.2496,  0.2304,  0.1280,  0.2343,  0.2142,\n","         -0.0847],\n","        [-0.2232,  0.3088, -0.3186, -0.1891, -0.1151,  0.3163, -0.1632, -0.2292,\n","         -0.1649],\n","        [-0.2737,  0.0765, -0.0583, -0.0097,  0.1863, -0.2106, -0.2325, -0.3246,\n","         -0.2709],\n","        [-0.1072,  0.2326, -0.3016, -0.1917,  0.0186,  0.1531, -0.0821, -0.1090,\n","          0.2700],\n","        [ 0.0019, -0.0045, -0.0062, -0.2522,  0.1469, -0.2723,  0.0940,  0.2003,\n","         -0.1635],\n","        [ 0.1994,  0.2191,  0.1314,  0.1474, -0.0140, -0.2230,  0.2389,  0.0251,\n","         -0.0505],\n","        [ 0.1061,  0.0725, -0.1745, -0.1989, -0.2828,  0.0545, -0.0259,  0.0243,\n","         -0.1877],\n","        [-0.0880, -0.1375,  0.2530, -0.2390, -0.0909,  0.2588, -0.1617,  0.0593,\n","         -0.1448]])\n","first_linear.bias tensor([-0.0709,  0.2342, -0.0144,  0.2820, -0.2428,  0.2697,  0.1452, -0.0652,\n","        -0.2047, -0.0919])\n","linear_hidden.0.weight tensor([[-0.2952, -0.3038,  0.0253,  0.2344, -0.2026,  0.0860, -0.1289, -0.1055,\n","          0.2067,  0.0600],\n","        [-0.1466, -0.2167, -0.1891, -0.1490, -0.1096,  0.1654, -0.2088, -0.1226,\n","         -0.1904,  0.1940],\n","        [-0.0021,  0.1795,  0.1525,  0.2093,  0.2831,  0.0140,  0.1916, -0.1888,\n","          0.1216, -0.2485],\n","        [ 0.1578,  0.0312,  0.1725, -0.1441, -0.0626, -0.2559,  0.1114,  0.2978,\n","         -0.1267,  0.0428],\n","        [ 0.2902, -0.2213,  0.1553, -0.0559, -0.1614, -0.2954,  0.1758,  0.1698,\n","         -0.2445,  0.2817],\n","        [ 0.2104, -0.1497, -0.1271, -0.2173,  0.2478,  0.2074,  0.0721, -0.3055,\n","          0.1899, -0.2204],\n","        [ 0.1622, -0.1401,  0.1510,  0.2188,  0.1689, -0.1418,  0.0750,  0.0104,\n","         -0.1625,  0.1544],\n","        [ 0.0698, -0.0450, -0.0480, -0.2445, -0.0390, -0.1389, -0.0979, -0.1005,\n","         -0.0682,  0.2780],\n","        [-0.1089, -0.0856,  0.0082,  0.0673, -0.0983, -0.1751, -0.0302,  0.1555,\n","          0.0924, -0.2344],\n","        [ 0.1003,  0.2787,  0.2256, -0.3081,  0.1433,  0.2279,  0.1294,  0.0598,\n","          0.0261,  0.2843]])\n","linear_hidden.0.bias tensor([ 0.0515,  0.0291,  0.2295, -0.0177,  0.2277,  0.0541,  0.1644,  0.0977,\n","         0.1822,  0.1079])\n","linear_output.weight tensor([[ 0.1842, -0.0195, -0.1088,  0.3124, -0.1714, -0.0380,  0.1845,  0.2196,\n","          0.0447, -0.3143]])\n","linear_output.bias tensor([-0.3106])\n","epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1419])) that is different to the input size (torch.Size([1419, 1])) is deprecated. Please ensure they have the same size.\n","\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning:\n","\n","Using a target size (torch.Size([1016])) that is different to the input size (torch.Size([1016, 1])) is deprecated. Please ensure they have the same size.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.7243368029594421 \t Validate_Accuracy: 0.516\n","epoch 2\n","Epoch: 2 \t Train Loss: 0.6949010888735453 \t Validate_Accuracy: 0.4415\n","epoch 3\n","Epoch: 3 \t Train Loss: 0.663761556148529 \t Validate_Accuracy: 0.596\n","epoch 4\n","Epoch: 4 \t Train Loss: 0.6453800996144613 \t Validate_Accuracy: 0.684\n","epoch 5\n","Epoch: 5 \t Train Loss: 0.5827124118804932 \t Validate_Accuracy: 0.763\n","epoch 6\n","Epoch: 6 \t Train Loss: 0.5273489554723104 \t Validate_Accuracy: 0.7895\n","epoch 7\n","Epoch: 7 \t Train Loss: 0.4799172580242157 \t Validate_Accuracy: 0.797\n","epoch 8\n","Epoch: 8 \t Train Loss: 0.44087935487429303 \t Validate_Accuracy: 0.802\n","epoch 9\n","Epoch: 9 \t Train Loss: 0.41204263766606647 \t Validate_Accuracy: 0.8065\n","epoch 10\n","Epoch: 10 \t Train Loss: 0.3975741962591807 \t Validate_Accuracy: 0.8055\n","epoch 11\n","Epoch: 11 \t Train Loss: 0.38075260321299237 \t Validate_Accuracy: 0.816\n","epoch 12\n","Epoch: 12 \t Train Loss: 0.37641750772794086 \t Validate_Accuracy: 0.8205\n","epoch 13\n","Epoch: 13 \t Train Loss: 0.36411140362421673 \t Validate_Accuracy: 0.829\n","epoch 14\n","Epoch: 14 \t Train Loss: 0.36676011482874554 \t Validate_Accuracy: 0.833\n","epoch 15\n","Epoch: 15 \t Train Loss: 0.35425976912180585 \t Validate_Accuracy: 0.8295\n","epoch 16\n","Epoch: 16 \t Train Loss: 0.351631502310435 \t Validate_Accuracy: 0.8325\n","epoch 17\n","Epoch: 17 \t Train Loss: 0.3466582993666331 \t Validate_Accuracy: 0.8295\n","epoch 18\n","Epoch: 18 \t Train Loss: 0.3440815508365631 \t Validate_Accuracy: 0.84\n","epoch 19\n","Epoch: 19 \t Train Loss: 0.33844831585884094 \t Validate_Accuracy: 0.848\n","epoch 20\n","Epoch: 20 \t Train Loss: 0.33068259557088214 \t Validate_Accuracy: 0.846\n","epoch 21\n","Epoch: 21 \t Train Loss: 0.3204732338587443 \t Validate_Accuracy: 0.845\n","epoch 22\n","Epoch: 22 \t Train Loss: 0.3139379024505615 \t Validate_Accuracy: 0.838\n","epoch 23\n","Epoch: 23 \t Train Loss: 0.30710574984550476 \t Validate_Accuracy: 0.8505\n","epoch 24\n","Epoch: 24 \t Train Loss: 0.3040110965569814 \t Validate_Accuracy: 0.862\n","epoch 25\n","Epoch: 25 \t Train Loss: 0.2966749966144562 \t Validate_Accuracy: 0.8575\n","epoch 26\n","Epoch: 26 \t Train Loss: 0.30245155096054077 \t Validate_Accuracy: 0.859\n","epoch 27\n","Epoch: 27 \t Train Loss: 0.28193612893422443 \t Validate_Accuracy: 0.8575\n","epoch 28\n","Epoch: 28 \t Train Loss: 0.2880275746186574 \t Validate_Accuracy: 0.8615\n","epoch 29\n","Epoch: 29 \t Train Loss: 0.28613269329071045 \t Validate_Accuracy: 0.8685\n","epoch 30\n","Epoch: 30 \t Train Loss: 0.27997203667958576 \t Validate_Accuracy: 0.871\n","epoch 31\n","Epoch: 31 \t Train Loss: 0.27489693959554035 \t Validate_Accuracy: 0.8625\n","epoch 32\n","Epoch: 32 \t Train Loss: 0.2781555155913035 \t Validate_Accuracy: 0.8585\n","epoch 33\n","Epoch: 33 \t Train Loss: 0.2846236030260722 \t Validate_Accuracy: 0.857\n","epoch 34\n","Epoch: 34 \t Train Loss: 0.2852381964524587 \t Validate_Accuracy: 0.8705\n","epoch 35\n","Epoch: 35 \t Train Loss: 0.2739875813325246 \t Validate_Accuracy: 0.8685\n","epoch 36\n","Epoch: 36 \t Train Loss: 0.2820712129275004 \t Validate_Accuracy: 0.8675\n","epoch 37\n","Epoch: 37 \t Train Loss: 0.2706165413061778 \t Validate_Accuracy: 0.8625\n","epoch 38\n","Epoch: 38 \t Train Loss: 0.27006909251213074 \t Validate_Accuracy: 0.8665\n","epoch 39\n","Epoch: 39 \t Train Loss: 0.2684723337491353 \t Validate_Accuracy: 0.87\n","epoch 40\n","Epoch: 40 \t Train Loss: 0.2684546609719594 \t Validate_Accuracy: 0.854\n","epoch 41\n","Epoch: 41 \t Train Loss: 0.2795053819815318 \t Validate_Accuracy: 0.8695\n","epoch 42\n","Epoch: 42 \t Train Loss: 0.27299846212069195 \t Validate_Accuracy: 0.8645\n","epoch 43\n","Epoch: 43 \t Train Loss: 0.2789504925409953 \t Validate_Accuracy: 0.8705\n","epoch 44\n","Epoch: 44 \t Train Loss: 0.2684646149476369 \t Validate_Accuracy: 0.865\n","epoch 45\n","Epoch: 45 \t Train Loss: 0.2730282147725423 \t Validate_Accuracy: 0.86\n","epoch 46\n","Epoch: 46 \t Train Loss: 0.26863132913907367 \t Validate_Accuracy: 0.8545\n","epoch 47\n","Epoch: 47 \t Train Loss: 0.27103229363759357 \t Validate_Accuracy: 0.84\n","epoch 48\n","Epoch: 48 \t Train Loss: 0.27976061900456745 \t Validate_Accuracy: 0.8685\n","epoch 49\n","Epoch: 49 \t Train Loss: 0.26191645860671997 \t Validate_Accuracy: 0.862\n","epoch 50\n","Epoch: 50 \t Train Loss: 0.26912928620974225 \t Validate_Accuracy: 0.8715\n","epoch 51\n","Epoch: 51 \t Train Loss: 0.2644496311744054 \t Validate_Accuracy: 0.867\n","epoch 52\n","Epoch: 52 \t Train Loss: 0.26023619373639423 \t Validate_Accuracy: 0.864\n","epoch 53\n","Epoch: 53 \t Train Loss: 0.27259378135204315 \t Validate_Accuracy: 0.8525\n","epoch 54\n","Epoch: 54 \t Train Loss: 0.2694100538889567 \t Validate_Accuracy: 0.871\n","epoch 55\n","Epoch: 55 \t Train Loss: 0.2623488207658132 \t Validate_Accuracy: 0.8645\n","epoch 56\n","Epoch: 56 \t Train Loss: 0.26724950472513836 \t Validate_Accuracy: 0.869\n","epoch 57\n","Epoch: 57 \t Train Loss: 0.2540170947710673 \t Validate_Accuracy: 0.8725\n","epoch 58\n","Epoch: 58 \t Train Loss: 0.2493156542380651 \t Validate_Accuracy: 0.8555\n","epoch 59\n","Epoch: 59 \t Train Loss: 0.26137853662172955 \t Validate_Accuracy: 0.865\n","epoch 60\n","Epoch: 60 \t Train Loss: 0.2649261603752772 \t Validate_Accuracy: 0.8745\n","epoch 61\n","Epoch: 61 \t Train Loss: 0.2629774312178294 \t Validate_Accuracy: 0.874\n","epoch 62\n","Epoch: 62 \t Train Loss: 0.24774978558222452 \t Validate_Accuracy: 0.8745\n","epoch 63\n","Epoch: 63 \t Train Loss: 0.25565778215726215 \t Validate_Accuracy: 0.8805\n","epoch 64\n","Epoch: 64 \t Train Loss: 0.26287007828553516 \t Validate_Accuracy: 0.8785\n","epoch 65\n","Epoch: 65 \t Train Loss: 0.25489874680836994 \t Validate_Accuracy: 0.8735\n","epoch 66\n","Epoch: 66 \t Train Loss: 0.2518163075049718 \t Validate_Accuracy: 0.872\n","epoch 67\n","Epoch: 67 \t Train Loss: 0.24503771464029947 \t Validate_Accuracy: 0.8755\n","epoch 68\n","Epoch: 68 \t Train Loss: 0.2534538209438324 \t Validate_Accuracy: 0.8765\n","epoch 69\n","Epoch: 69 \t Train Loss: 0.24227633575598398 \t Validate_Accuracy: 0.883\n","epoch 70\n","Epoch: 70 \t Train Loss: 0.23752019306023917 \t Validate_Accuracy: 0.877\n","epoch 71\n","Epoch: 71 \t Train Loss: 0.23371249437332153 \t Validate_Accuracy: 0.868\n","epoch 72\n","Epoch: 72 \t Train Loss: 0.24645860493183136 \t Validate_Accuracy: 0.876\n","epoch 73\n","Epoch: 73 \t Train Loss: 0.24595792094866434 \t Validate_Accuracy: 0.849\n","epoch 74\n","Epoch: 74 \t Train Loss: 0.2777407417694728 \t Validate_Accuracy: 0.8775\n","epoch 75\n","Epoch: 75 \t Train Loss: 0.26083055635293323 \t Validate_Accuracy: 0.864\n","epoch 76\n","Epoch: 76 \t Train Loss: 0.2711031138896942 \t Validate_Accuracy: 0.8515\n","epoch 77\n","Epoch: 77 \t Train Loss: 0.29978151122728985 \t Validate_Accuracy: 0.8585\n","epoch 78\n","Epoch: 78 \t Train Loss: 0.28296753764152527 \t Validate_Accuracy: 0.865\n","epoch 79\n","Epoch: 79 \t Train Loss: 0.2754485507806142 \t Validate_Accuracy: 0.8735\n","epoch 80\n","Epoch: 80 \t Train Loss: 0.25415537257989246 \t Validate_Accuracy: 0.8725\n","epoch 81\n","Epoch: 81 \t Train Loss: 0.24564412732919058 \t Validate_Accuracy: 0.875\n","epoch 82\n","Epoch: 82 \t Train Loss: 0.23787836730480194 \t Validate_Accuracy: 0.877\n","epoch 83\n","Epoch: 83 \t Train Loss: 0.23839592933654785 \t Validate_Accuracy: 0.8715\n","epoch 84\n","Epoch: 84 \t Train Loss: 0.24158617357412973 \t Validate_Accuracy: 0.8815\n","epoch 85\n","Epoch: 85 \t Train Loss: 0.23929913838704428 \t Validate_Accuracy: 0.8655\n","epoch 86\n","Epoch: 86 \t Train Loss: 0.23944008350372314 \t Validate_Accuracy: 0.876\n","epoch 87\n","Epoch: 87 \t Train Loss: 0.2423596183458964 \t Validate_Accuracy: 0.864\n","epoch 88\n","Epoch: 88 \t Train Loss: 0.2508903493483861 \t Validate_Accuracy: 0.8795\n","epoch 89\n","Epoch: 89 \t Train Loss: 0.24024029076099396 \t Validate_Accuracy: 0.858\n","epoch 90\n","Epoch: 90 \t Train Loss: 0.2589076558748881 \t Validate_Accuracy: 0.8755\n","epoch 91\n","Epoch: 91 \t Train Loss: 0.25403255720933277 \t Validate_Accuracy: 0.875\n","epoch 92\n","Epoch: 92 \t Train Loss: 0.23875262339909872 \t Validate_Accuracy: 0.872\n","epoch 93\n","Epoch: 93 \t Train Loss: 0.2357008953889211 \t Validate_Accuracy: 0.875\n","epoch 94\n","Epoch: 94 \t Train Loss: 0.24596001704533896 \t Validate_Accuracy: 0.8735\n","epoch 95\n","Epoch: 95 \t Train Loss: 0.23707676927248636 \t Validate_Accuracy: 0.8805\n","epoch 96\n","Epoch: 96 \t Train Loss: 0.2419173369805018 \t Validate_Accuracy: 0.8705\n","epoch 97\n","Epoch: 97 \t Train Loss: 0.2392200529575348 \t Validate_Accuracy: 0.871\n","epoch 98\n","Epoch: 98 \t Train Loss: 0.25290918350219727 \t Validate_Accuracy: 0.873\n","epoch 99\n","Epoch: 99 \t Train Loss: 0.24159717559814453 \t Validate_Accuracy: 0.8785\n","epoch 100\n","Epoch: 100 \t Train Loss: 0.23309949040412903 \t Validate_Accuracy: 0.877\n","epoch 101\n","Epoch: 101 \t Train Loss: 0.2268238216638565 \t Validate_Accuracy: 0.8795\n","epoch 102\n","Epoch: 102 \t Train Loss: 0.22686771055062613 \t Validate_Accuracy: 0.8725\n","epoch 103\n","Epoch: 103 \t Train Loss: 0.25067639350891113 \t Validate_Accuracy: 0.8695\n","epoch 104\n","Epoch: 104 \t Train Loss: 0.24289900064468384 \t Validate_Accuracy: 0.8745\n","epoch 105\n","Epoch: 105 \t Train Loss: 0.23532801866531372 \t Validate_Accuracy: 0.8745\n","epoch 106\n","Epoch: 106 \t Train Loss: 0.2464688221613566 \t Validate_Accuracy: 0.8785\n","epoch 107\n","Epoch: 107 \t Train Loss: 0.2432640790939331 \t Validate_Accuracy: 0.8775\n","epoch 108\n","Epoch: 108 \t Train Loss: 0.235465407371521 \t Validate_Accuracy: 0.8735\n","epoch 109\n","Epoch: 109 \t Train Loss: 0.254750316341718 \t Validate_Accuracy: 0.8785\n","epoch 110\n","Epoch: 110 \t Train Loss: 0.24391440550486246 \t Validate_Accuracy: 0.8765\n","epoch 111\n","Epoch: 111 \t Train Loss: 0.2327261120080948 \t Validate_Accuracy: 0.875\n","epoch 112\n","Epoch: 112 \t Train Loss: 0.23229322334130606 \t Validate_Accuracy: 0.87\n","epoch 113\n","Epoch: 113 \t Train Loss: 0.2427148073911667 \t Validate_Accuracy: 0.883\n","epoch 114\n","Epoch: 114 \t Train Loss: 0.2272063046693802 \t Validate_Accuracy: 0.8805\n","epoch 115\n","Epoch: 115 \t Train Loss: 0.23445776104927063 \t Validate_Accuracy: 0.878\n","epoch 116\n","Epoch: 116 \t Train Loss: 0.25307505826155346 \t Validate_Accuracy: 0.8825\n","epoch 117\n","Epoch: 117 \t Train Loss: 0.23455679416656494 \t Validate_Accuracy: 0.8685\n","epoch 118\n","Epoch: 118 \t Train Loss: 0.24446677168210348 \t Validate_Accuracy: 0.8735\n","epoch 119\n","Epoch: 119 \t Train Loss: 0.23522492249806723 \t Validate_Accuracy: 0.8715\n","epoch 120\n","Epoch: 120 \t Train Loss: 0.22891619304815927 \t Validate_Accuracy: 0.881\n","epoch 121\n","Epoch: 121 \t Train Loss: 0.22431492805480957 \t Validate_Accuracy: 0.8745\n","epoch 122\n","Epoch: 122 \t Train Loss: 0.23051407933235168 \t Validate_Accuracy: 0.8835\n","epoch 123\n","Epoch: 123 \t Train Loss: 0.2262133906284968 \t Validate_Accuracy: 0.884\n","epoch 124\n","Epoch: 124 \t Train Loss: 0.22873370349407196 \t Validate_Accuracy: 0.8835\n","epoch 125\n","Epoch: 125 \t Train Loss: 0.26362237334251404 \t Validate_Accuracy: 0.879\n","epoch 126\n","Epoch: 126 \t Train Loss: 0.27578450242678326 \t Validate_Accuracy: 0.8585\n","epoch 127\n","Epoch: 127 \t Train Loss: 0.2609809339046478 \t Validate_Accuracy: 0.874\n","epoch 128\n","Epoch: 128 \t Train Loss: 0.2536347458759944 \t Validate_Accuracy: 0.8705\n","epoch 129\n","Epoch: 129 \t Train Loss: 0.24232080082098642 \t Validate_Accuracy: 0.8735\n","epoch 130\n","Epoch: 130 \t Train Loss: 0.2404524932305018 \t Validate_Accuracy: 0.877\n","epoch 131\n","Epoch: 131 \t Train Loss: 0.23468758165836334 \t Validate_Accuracy: 0.881\n","epoch 132\n","Epoch: 132 \t Train Loss: 0.23060009876887003 \t Validate_Accuracy: 0.8825\n","epoch 133\n","Epoch: 133 \t Train Loss: 0.22226329644521078 \t Validate_Accuracy: 0.879\n","epoch 134\n","Epoch: 134 \t Train Loss: 0.260780930519104 \t Validate_Accuracy: 0.8595\n","epoch 135\n","Epoch: 135 \t Train Loss: 0.26608150204022724 \t Validate_Accuracy: 0.8795\n","epoch 136\n","Epoch: 136 \t Train Loss: 0.2416229099035263 \t Validate_Accuracy: 0.873\n","epoch 137\n","Epoch: 137 \t Train Loss: 0.23700628678003946 \t Validate_Accuracy: 0.8765\n","epoch 138\n","Epoch: 138 \t Train Loss: 0.2350557545820872 \t Validate_Accuracy: 0.8835\n","epoch 139\n","Epoch: 139 \t Train Loss: 0.22506654262542725 \t Validate_Accuracy: 0.8845\n","epoch 140\n","Epoch: 140 \t Train Loss: 0.22386400401592255 \t Validate_Accuracy: 0.879\n","epoch 141\n","Epoch: 141 \t Train Loss: 0.23386568824450174 \t Validate_Accuracy: 0.873\n","epoch 142\n","Epoch: 142 \t Train Loss: 0.23301824430624643 \t Validate_Accuracy: 0.872\n","epoch 143\n","Epoch: 143 \t Train Loss: 0.23188317318757376 \t Validate_Accuracy: 0.883\n","epoch 144\n","Epoch: 144 \t Train Loss: 0.24379867315292358 \t Validate_Accuracy: 0.882\n","epoch 145\n","Epoch: 145 \t Train Loss: 0.2352947841087977 \t Validate_Accuracy: 0.886\n","epoch 146\n","Epoch: 146 \t Train Loss: 0.22286663949489594 \t Validate_Accuracy: 0.879\n","epoch 147\n","Epoch: 147 \t Train Loss: 0.22030219932397208 \t Validate_Accuracy: 0.8835\n","epoch 148\n","Epoch: 148 \t Train Loss: 0.2203183720509211 \t Validate_Accuracy: 0.879\n","epoch 149\n","Epoch: 149 \t Train Loss: 0.2295787831147512 \t Validate_Accuracy: 0.882\n","epoch 150\n","Epoch: 150 \t Train Loss: 0.23053401708602905 \t Validate_Accuracy: 0.885\n","epoch 151\n","Epoch: 151 \t Train Loss: 0.22506564855575562 \t Validate_Accuracy: 0.8765\n","epoch 152\n","Epoch: 152 \t Train Loss: 0.24052690466245016 \t Validate_Accuracy: 0.876\n","epoch 153\n","Epoch: 153 \t Train Loss: 0.24181934694449106 \t Validate_Accuracy: 0.878\n","epoch 154\n","Epoch: 154 \t Train Loss: 0.24112394452095032 \t Validate_Accuracy: 0.886\n","epoch 155\n","Epoch: 155 \t Train Loss: 0.22520461678504944 \t Validate_Accuracy: 0.876\n","epoch 156\n","Epoch: 156 \t Train Loss: 0.22835651536782584 \t Validate_Accuracy: 0.8795\n","epoch 157\n","Epoch: 157 \t Train Loss: 0.22109606862068176 \t Validate_Accuracy: 0.8775\n","epoch 158\n","Epoch: 158 \t Train Loss: 0.2182284196217855 \t Validate_Accuracy: 0.882\n","epoch 159\n","Epoch: 159 \t Train Loss: 0.21271697680155435 \t Validate_Accuracy: 0.8835\n","epoch 160\n","Epoch: 160 \t Train Loss: 0.2129577249288559 \t Validate_Accuracy: 0.8795\n","epoch 161\n","Epoch: 161 \t Train Loss: 0.21520153681437174 \t Validate_Accuracy: 0.8815\n","epoch 162\n","Epoch: 162 \t Train Loss: 0.2099668929974238 \t Validate_Accuracy: 0.8895\n","epoch 163\n","Epoch: 163 \t Train Loss: 0.20529749492804208 \t Validate_Accuracy: 0.885\n","epoch 164\n","Epoch: 164 \t Train Loss: 0.23304840425650278 \t Validate_Accuracy: 0.863\n","epoch 165\n","Epoch: 165 \t Train Loss: 0.27663617332776386 \t Validate_Accuracy: 0.8655\n","epoch 166\n","Epoch: 166 \t Train Loss: 0.2762569735447566 \t Validate_Accuracy: 0.878\n","epoch 167\n","Epoch: 167 \t Train Loss: 0.25922271609306335 \t Validate_Accuracy: 0.876\n","epoch 168\n","Epoch: 168 \t Train Loss: 0.23903051018714905 \t Validate_Accuracy: 0.8625\n","epoch 169\n","Epoch: 169 \t Train Loss: 0.2421101580063502 \t Validate_Accuracy: 0.8835\n","epoch 170\n","Epoch: 170 \t Train Loss: 0.23628953099250793 \t Validate_Accuracy: 0.872\n","epoch 171\n","Epoch: 171 \t Train Loss: 0.24073739349842072 \t Validate_Accuracy: 0.871\n","epoch 172\n","Epoch: 172 \t Train Loss: 0.23269627491633096 \t Validate_Accuracy: 0.883\n","epoch 173\n","Epoch: 173 \t Train Loss: 0.21774394313494363 \t Validate_Accuracy: 0.883\n","epoch 174\n","Epoch: 174 \t Train Loss: 0.24121693770090738 \t Validate_Accuracy: 0.872\n","epoch 175\n","Epoch: 175 \t Train Loss: 0.2527570277452469 \t Validate_Accuracy: 0.8825\n","epoch 176\n","Epoch: 176 \t Train Loss: 0.2353136291106542 \t Validate_Accuracy: 0.876\n","epoch 177\n","Epoch: 177 \t Train Loss: 0.22860694924990335 \t Validate_Accuracy: 0.8735\n","epoch 178\n","Epoch: 178 \t Train Loss: 0.23320403198401132 \t Validate_Accuracy: 0.878\n","epoch 179\n","Epoch: 179 \t Train Loss: 0.2355297009150187 \t Validate_Accuracy: 0.87\n","epoch 180\n","Epoch: 180 \t Train Loss: 0.2320999652147293 \t Validate_Accuracy: 0.8785\n","epoch 181\n","Epoch: 181 \t Train Loss: 0.2293215443690618 \t Validate_Accuracy: 0.8835\n","epoch 182\n","Epoch: 182 \t Train Loss: 0.22709268828233084 \t Validate_Accuracy: 0.8825\n","epoch 183\n","Epoch: 183 \t Train Loss: 0.21231834093729654 \t Validate_Accuracy: 0.8795\n","epoch 184\n","Epoch: 184 \t Train Loss: 0.21180924276510874 \t Validate_Accuracy: 0.8775\n","epoch 185\n","Epoch: 185 \t Train Loss: 0.2181563526391983 \t Validate_Accuracy: 0.881\n","epoch 186\n","Epoch: 186 \t Train Loss: 0.20812674363454184 \t Validate_Accuracy: 0.881\n","epoch 187\n","Epoch: 187 \t Train Loss: 0.20781897008419037 \t Validate_Accuracy: 0.8845\n","epoch 188\n","Epoch: 188 \t Train Loss: 0.20995343228181204 \t Validate_Accuracy: 0.8765\n","epoch 189\n","Epoch: 189 \t Train Loss: 0.2412407100200653 \t Validate_Accuracy: 0.877\n","epoch 190\n","Epoch: 190 \t Train Loss: 0.23778225481510162 \t Validate_Accuracy: 0.8805\n","epoch 191\n","Epoch: 191 \t Train Loss: 0.22953933974107107 \t Validate_Accuracy: 0.872\n","epoch 192\n","Epoch: 192 \t Train Loss: 0.23840845624605814 \t Validate_Accuracy: 0.8815\n","epoch 193\n","Epoch: 193 \t Train Loss: 0.22611796855926514 \t Validate_Accuracy: 0.878\n","epoch 194\n","Epoch: 194 \t Train Loss: 0.226481094956398 \t Validate_Accuracy: 0.879\n","epoch 195\n","Epoch: 195 \t Train Loss: 0.2175671805938085 \t Validate_Accuracy: 0.8835\n","epoch 196\n","Epoch: 196 \t Train Loss: 0.21680143972237906 \t Validate_Accuracy: 0.8845\n","epoch 197\n","Epoch: 197 \t Train Loss: 0.21777063608169556 \t Validate_Accuracy: 0.8845\n","epoch 198\n","Epoch: 198 \t Train Loss: 0.21869434416294098 \t Validate_Accuracy: 0.8885\n","epoch 199\n","Epoch: 199 \t Train Loss: 0.21691964070002237 \t Validate_Accuracy: 0.883\n","epoch 200\n","Epoch: 200 \t Train Loss: 0.22259783744812012 \t Validate_Accuracy: 0.878\n","epoch 201\n","Epoch: 201 \t Train Loss: 0.22239944835503897 \t Validate_Accuracy: 0.883\n","epoch 202\n","Epoch: 202 \t Train Loss: 0.21912819147109985 \t Validate_Accuracy: 0.887\n","epoch 203\n","Epoch: 203 \t Train Loss: 0.2125715066989263 \t Validate_Accuracy: 0.873\n","epoch 204\n","Epoch: 204 \t Train Loss: 0.2416990945736567 \t Validate_Accuracy: 0.8565\n","epoch 205\n","Epoch: 205 \t Train Loss: 0.25710511207580566 \t Validate_Accuracy: 0.881\n","epoch 206\n","Epoch: 206 \t Train Loss: 0.2273224393526713 \t Validate_Accuracy: 0.873\n","epoch 207\n","Epoch: 207 \t Train Loss: 0.22826105852921805 \t Validate_Accuracy: 0.8845\n","epoch 208\n","Epoch: 208 \t Train Loss: 0.21645282208919525 \t Validate_Accuracy: 0.87\n","epoch 209\n","Epoch: 209 \t Train Loss: 0.22655499974886575 \t Validate_Accuracy: 0.891\n","epoch 210\n","Epoch: 210 \t Train Loss: 0.22479045391082764 \t Validate_Accuracy: 0.8805\n","epoch 211\n","Epoch: 211 \t Train Loss: 0.22557914753754935 \t Validate_Accuracy: 0.8715\n","epoch 212\n","Epoch: 212 \t Train Loss: 0.25158292055130005 \t Validate_Accuracy: 0.8725\n","epoch 213\n","Epoch: 213 \t Train Loss: 0.2503074953953425 \t Validate_Accuracy: 0.88\n","epoch 214\n","Epoch: 214 \t Train Loss: 0.23661024371782938 \t Validate_Accuracy: 0.8655\n","epoch 215\n","Epoch: 215 \t Train Loss: 0.2476411759853363 \t Validate_Accuracy: 0.8835\n","epoch 216\n","Epoch: 216 \t Train Loss: 0.23373647530873617 \t Validate_Accuracy: 0.874\n","epoch 217\n","Epoch: 217 \t Train Loss: 0.242424875497818 \t Validate_Accuracy: 0.894\n","epoch 218\n","Epoch: 218 \t Train Loss: 0.2254370798667272 \t Validate_Accuracy: 0.885\n","epoch 219\n","Epoch: 219 \t Train Loss: 0.2265364279349645 \t Validate_Accuracy: 0.887\n","epoch 220\n","Epoch: 220 \t Train Loss: 0.218841552734375 \t Validate_Accuracy: 0.888\n","epoch 221\n","Epoch: 221 \t Train Loss: 0.2123065491517385 \t Validate_Accuracy: 0.8925\n","epoch 222\n","Epoch: 222 \t Train Loss: 0.2164501150449117 \t Validate_Accuracy: 0.8885\n","epoch 223\n","Epoch: 223 \t Train Loss: 0.22874779999256134 \t Validate_Accuracy: 0.8605\n","epoch 224\n","Epoch: 224 \t Train Loss: 0.2575195829073588 \t Validate_Accuracy: 0.878\n","epoch 225\n","Epoch: 225 \t Train Loss: 0.23719050486882529 \t Validate_Accuracy: 0.876\n","epoch 226\n","Epoch: 226 \t Train Loss: 0.23512501021226248 \t Validate_Accuracy: 0.865\n","epoch 227\n","Epoch: 227 \t Train Loss: 0.242856835325559 \t Validate_Accuracy: 0.884\n","epoch 228\n","Epoch: 228 \t Train Loss: 0.22753130892912546 \t Validate_Accuracy: 0.8865\n","epoch 229\n","Epoch: 229 \t Train Loss: 0.2229318618774414 \t Validate_Accuracy: 0.8925\n","epoch 230\n","Epoch: 230 \t Train Loss: 0.2183503359556198 \t Validate_Accuracy: 0.892\n","epoch 231\n","Epoch: 231 \t Train Loss: 0.22357423106829324 \t Validate_Accuracy: 0.8935\n","epoch 232\n","Epoch: 232 \t Train Loss: 0.21473340690135956 \t Validate_Accuracy: 0.889\n","epoch 233\n","Epoch: 233 \t Train Loss: 0.21008501450220743 \t Validate_Accuracy: 0.8835\n","epoch 234\n","Epoch: 234 \t Train Loss: 0.2114338477452596 \t Validate_Accuracy: 0.8895\n","epoch 235\n","Epoch: 235 \t Train Loss: 0.21807875235875449 \t Validate_Accuracy: 0.8865\n","epoch 236\n","Epoch: 236 \t Train Loss: 0.21691334744294485 \t Validate_Accuracy: 0.889\n","epoch 237\n","Epoch: 237 \t Train Loss: 0.21020883321762085 \t Validate_Accuracy: 0.889\n","epoch 238\n","Epoch: 238 \t Train Loss: 0.21248888969421387 \t Validate_Accuracy: 0.8825\n","epoch 239\n","Epoch: 239 \t Train Loss: 0.2263166308403015 \t Validate_Accuracy: 0.882\n","epoch 240\n","Epoch: 240 \t Train Loss: 0.2307407110929489 \t Validate_Accuracy: 0.8775\n","epoch 241\n","Epoch: 241 \t Train Loss: 0.2208019495010376 \t Validate_Accuracy: 0.8875\n","epoch 242\n","Epoch: 242 \t Train Loss: 0.21395846207936606 \t Validate_Accuracy: 0.8865\n","epoch 243\n","Epoch: 243 \t Train Loss: 0.22140778104464212 \t Validate_Accuracy: 0.888\n","epoch 244\n","Epoch: 244 \t Train Loss: 0.2230175236860911 \t Validate_Accuracy: 0.889\n","epoch 245\n","Epoch: 245 \t Train Loss: 0.2121517906586329 \t Validate_Accuracy: 0.8935\n","epoch 246\n","Epoch: 246 \t Train Loss: 0.2066926807165146 \t Validate_Accuracy: 0.877\n","epoch 247\n","Epoch: 247 \t Train Loss: 0.2270511339108149 \t Validate_Accuracy: 0.862\n","epoch 248\n","Epoch: 248 \t Train Loss: 0.24002555509408316 \t Validate_Accuracy: 0.8905\n","epoch 249\n","Epoch: 249 \t Train Loss: 0.21918698648611704 \t Validate_Accuracy: 0.879\n","epoch 250\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 08-29 04:59:55] ax.service.managed_loop: Running optimization trial 9...\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 250 \t Train Loss: 0.23374617099761963 \t Validate_Accuracy: 0.8765\n","model parameters! \n","\n","conv1.weight tensor([[[[ 0.2129,  0.0241,  0.2804],\n","          [ 0.0543, -0.0136,  0.0703],\n","          [ 0.2549,  0.0870,  0.2505]]]])\n","conv1.bias tensor([-0.0720])\n","first_linear.weight tensor([[ 0.8521, -0.4787,  0.2435, -0.1044, -0.6683,  0.5277, -0.5835,  1.5780,\n","         -0.8038],\n","        [-0.4190,  0.1715, -0.2043,  0.3390,  1.6610, -0.1174,  0.3284,  0.7539,\n","          0.0430],\n","        [ 0.5420, -0.6766,  0.1462, -0.9829,  1.0317,  0.1476,  1.1427, -1.6068,\n","          0.7493],\n","        [-0.8120,  1.7491, -0.9967, -0.0960, -0.9365,  1.0114,  0.5385, -0.0662,\n","         -0.2262],\n","        [ 0.4533,  0.4099,  0.5010,  0.2191,  0.9822,  0.8834, -0.0177, -0.0304,\n","         -0.1492],\n","        [ 0.5229,  0.3699,  1.3970, -0.1383, -0.8942,  0.1337,  0.7386, -0.3382,\n","          1.5705],\n","        [ 0.8914,  0.2953, -0.4049, -2.5576,  0.4692,  0.1996,  0.9908, -0.0998,\n","         -0.3886],\n","        [-0.6769,  0.4191,  0.7005, -0.5344,  0.7627, -0.8775, -1.0684,  0.0045,\n","         -0.1681],\n","        [ 0.2070,  0.7978,  0.6585,  0.9005,  0.1458,  0.7844,  0.1945,  0.4621,\n","          0.1834],\n","        [ 0.1928, -0.1724,  0.7074, -0.9371,  0.0649, -1.9590, -0.1888,  0.1876,\n","          0.9365]])\n","first_linear.bias tensor([ 1.2774,  0.5723,  1.4012,  1.6407, -1.2336, -0.0324, -1.5803,  0.6345,\n","         2.2670,  1.5169])\n","linear_hidden.0.weight tensor([[-6.3527e-01,  1.1634e-01, -1.4934e-01,  2.3495e-01, -8.8153e-01,\n","          7.0546e-01, -1.3765e+00,  1.0183e+00,  7.6956e-01,  1.5183e+00],\n","        [-3.6220e-01, -6.4015e-02, -3.3133e-01, -3.0670e-01,  3.0904e-01,\n","          1.4918e-02,  1.9492e-01, -6.2816e-02, -5.7138e-01, -2.7061e-01],\n","        [ 1.1103e+00, -1.0481e+00,  1.0620e+00,  1.3312e+00, -2.1933e-01,\n","         -3.3053e-02, -1.1656e+00, -1.1498e+00, -2.2464e-01,  3.4699e-01],\n","        [ 2.5164e-01,  1.7632e-01,  1.0232e-01, -9.2594e-02, -1.9235e-01,\n","          3.2908e-01,  2.4443e-01, -1.9961e-01,  1.1055e+00,  6.5039e-02],\n","        [-6.0506e-01, -4.6891e-01, -3.6222e-01, -5.1075e-01, -3.3064e-01,\n","          6.7222e-01,  2.5315e-02,  5.6483e-01,  7.4930e-01, -7.4795e-01],\n","        [ 8.5174e-03,  9.1072e-01,  4.4028e-01,  2.8821e-01,  1.0505e+00,\n","          1.4088e+00, -7.3491e-02,  1.4909e-01, -4.4910e-01, -4.0608e-01],\n","        [-1.3960e-01,  3.1691e-01,  3.0521e-01, -7.5844e-01, -6.6717e-01,\n","          4.8042e-01,  6.2631e-01, -9.4110e-01,  3.3673e-01, -2.8771e-01],\n","        [-1.3971e+00, -5.0831e-01, -1.4917e+00, -1.0789e+00, -7.6584e-01,\n","          4.2966e-01,  9.1073e-01, -4.9637e-01,  9.6438e-01, -9.3307e-01],\n","        [-2.1740e-04,  4.3706e-05,  3.1458e-06, -1.3301e-04, -2.6512e-04,\n","          1.2437e-04,  4.8987e-05,  2.9496e-05,  1.3751e-04,  8.6568e-05],\n","        [-3.6665e-01, -6.0552e-02, -3.2943e-01, -3.0693e-01,  3.1631e-01,\n","          1.9777e-02,  1.8925e-01, -5.5743e-02, -5.8457e-01, -2.6525e-01]])\n","linear_hidden.0.bias tensor([-5.0993e-01, -6.6753e-01, -1.4108e+00,  8.3550e-01,  8.1123e-01,\n","        -1.1637e+00,  5.4085e-01,  1.7181e+00,  3.3309e-04, -6.8595e-01])\n","linear_output.weight tensor([[ 1.6949e+00,  8.2368e-01,  2.3719e+00, -9.8905e-01, -5.0242e-01,\n","          1.3206e+00, -3.5520e-01, -2.5574e+00,  7.2837e-05,  9.3976e-01]])\n","linear_output.bias tensor([-0.3107])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yH3jfmeHU625","colab_type":"code","outputId":"faee473b-7b95-439a-ba93-b14e0f4429ad","executionInfo":{"status":"ok","timestamp":1567048143501,"user_tz":-420,"elapsed":6003,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls /usr/local/lib/python3.6/dist-packages/botorch/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ls: cannot access '/usr/local/lib/python3.6/dist-packages/botorch/': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QaA_x4e0VhwV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"outputId":"c8d39f24-342a-4001-c724-bdc09fcd5b30","executionInfo":{"status":"error","timestamp":1567067629073,"user_tz":-420,"elapsed":622,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}}},"source":["with open('hyperparameters.pl', 'rb') as f:\n","    x = pickle.load(f)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"EOFError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-146f2f6298d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hyperparameters.pl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mEOFError\u001b[0m: Ran out of input"]}]},{"cell_type":"code","metadata":{"id":"tSkjTcdXeQGW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3bf38fa9-a919-4f12-d2e7-112a6d6a78a0","executionInfo":{"status":"ok","timestamp":1567067159610,"user_tz":-420,"elapsed":589,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}}},"source":["objects"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"RlD8t6QGezlS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}