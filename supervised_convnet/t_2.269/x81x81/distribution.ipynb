{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"distribution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gVq8WMGt6hio","colab_type":"code","colab":{}},"source":["!pip install sqlalchemy\n","!pip install setproctitle\n","!pip install ray[rllib]  # also recommended: ray[debug]\n","!pip uninstall -y pyarrow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HrbCG_Qc8cKg","colab_type":"code","outputId":"346c7e66-862d-422f-f2f4-543ed9378fb9","executionInfo":{"status":"ok","timestamp":1568555937709,"user_tz":-420,"elapsed":31352,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd  /content/drive/My\\ Drive/Year\\ 4\\:\\ Synthesis/NS162\\:\\ Statistical\\ Mechanics/Renormalization\\ Research\\ Project/renormalization/supervised_convnet/t_2.269/81x81/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Year 4: Synthesis/NS162: Statistical Mechanics/Renormalization Research Project/renormalization/supervised_convnet/t_2.269/81x81\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_5iNza3_8jQc","colab_type":"code","outputId":"f271bcec-3db5-460a-ba04-2dff060354b1","executionInfo":{"status":"ok","timestamp":1568568435577,"user_tz":-420,"elapsed":608254,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"12HPvMBfXwnxne32wWiHNjVZYDY_6f73w"}},"source":["# %%capture\n","import train#, frozen\n","import sys\n","# sys.path.insert(0, \"../../\")\n","import supervised_convnet\n","import pickle\n","from collections import defaultdict\n","import numpy as np\n","import time\n","import torch\n","\n","mode = \"run\"\n","run_mode = \"multilayer_1channel\"\n","save_loops = 50\n","use_cuda = True\n","results = []\n","conv_params = {}\n","filename = \"\"\n","def save_progress(results, conv_params, filename, epoch = 500):\n","    new_results = {}\n","    new_results[\"best_val_acc_hist\"] = results\n","    new_results[\"conv_params\"] = conv_params\n","    new_results[\"annotation\"] = \"bias + weights histogram saved\"\n","    new_results[\"epoch\"] = epoch\n","    # new_results[\"first_epo    ch_validate_accuracy_list\"] = first_epoch_validate_accuracy_list\n","    with open(filename, \"wb\") as handle:\n","        pickle.dump(new_results, handle, protocol = pickle.HIGHEST_PROTOCOL)\n","\"\"\"\n","Default activation function: sigmoid\n","\"\"\"\n","\n","# GPU for use in colab\n","def model_to_cuda(model):\n","    if use_cuda and torch.cuda.is_available():\n","        model = model.cuda()\n","    return model\n","  \n","if mode == \"run\":\n","    hidden_size = 10\n","    out_channels = 1\n","    num_hidden_layers = 1\n","    num_conv_layers = 3\n","    conv_params = defaultdict(list)\n","    \n","\n","    if run_mode == \"multilayer_1channel\":\n","        \"\"\"\n","        \"\"\"\n","        out_channels = 1\n","        filename = \"multilayer_1channel.pl\"\n","        try:\n","            with open(filename, \"rb\") as handle:\n","                results = pickle.load(handle)\n","        except:\n","            results = []\n","        results = []\n","        for _ in range(500):\n","            model = supervised_convnet.MultiLayerConvNet(filter_size = 3, square_size = 3, \\\n","                    hidden_size = hidden_size, out_channels = out_channels,\n","                    first_activation = \"tanh\", activation_func = \"relu\",\n","                    num_hidden_layers = num_hidden_layers, num_conv_layers = num_conv_layers, seed = time.time() + _)\n","            model = model_to_cuda(model)\n","            best_val_acc, param_dict = train.trainer(model = model, batch_size = 534, betas0= 1-0.009367812437484689, betas1=1- 0.0011636320996308597,\n","                                                    lr= 0.18602018785651322, n_epochs= 165, train_size= 2459, weight_decay= 0.0034972858295468723, use_cuda = use_cuda)\n","            results.append(best_val_acc)\n","            for i in range(3):\n","                conv_params[f\"weight_{i}\"].append(param_dict[f\"conv_layers.{i}.weight\"])\n","                conv_params[f\"bias_{i}\"].append(param_dict[f\"conv_layers.{i}.bias\"])\n","            if (_ % save_loops) == (0):\n","                save_progress(results, conv_params, filename, _)\n","\n","\n","save_progress(results, conv_params, filename)\n","\n"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"dNnF4m19AcOs","colab_type":"code","outputId":"112fcd8d-98c9-46f5-eb8e-f076b2256b5c","executionInfo":{"status":"ok","timestamp":1568554109439,"user_tz":-420,"elapsed":1409,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sys.path.pop(0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'../../'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"oWv2KS1TV6Jw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"outputId":"74a694f0-c368-460e-ff02-0bce0fb771c3","executionInfo":{"status":"ok","timestamp":1568568436033,"user_tz":-420,"elapsed":62,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}}},"source":["sys.path"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/env/python',\n"," '/usr/lib/python36.zip',\n"," '/usr/lib/python3.6',\n"," '/usr/lib/python3.6/lib-dynload',\n"," '/usr/local/lib/python3.6/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n"," '/root/.ipython']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"7u6P2drsHFXI","colab_type":"code","outputId":"0a39bac8-415f-4945-f0aa-121b12a19a3f","executionInfo":{"status":"ok","timestamp":1568568436463,"user_tz":-420,"elapsed":501191,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["supervised_convnet.__file__"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Year 4: Synthesis/NS162: Statistical Mechanics/Renormalization Research Project/renormalization/supervised_convnet/t_2.269/81x81/supervised_convnet.py'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"g8jNLzF4HP0T","colab_type":"code","outputId":"69d9d1eb-3d22-484c-dba0-a55b20831f3d","executionInfo":{"status":"ok","timestamp":1568555194066,"user_tz":-420,"elapsed":3828,"user":{"displayName":"Huy Nguyen","photoUrl":"","userId":"05352249580863404953"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!cat supervised_convnet.py\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["import torch\n","import torch.nn as nn\n","import torch.autograd as autograd\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch.utils.data.dataset import Dataset\n","import math\n","import numpy as np\n","import sys\n","import time\n","\n","class SupervisedConvNet(nn.Module):\n","    def __init__(self, filter_size, square_size, hidden_size, num_hidden_layers,\n","                first_activation = \"tanh\", activation_func = \"sigmoid\",\n","                out_channels = 1, seed = time.time()):\n","        \"\"\"\n","        Arguments:\n","        filter_size ~ size of the convolution kernel (3 x 3)\n","        square size ~ how many strides of convolution in the input\n","        \"\"\"\n","        torch.manual_seed(seed)\n","        super(SupervisedConvNet, self).__init__()\n","        self.filter_size = filter_size\n","        self.square_size = square_size\n","        self.hidden_size = hidden_size\n","        self.out_channels = out_channels\n","        if first_activation == \"tanh\":\n","            self.first_activation = torch.tanh\n","        elif first_activation == \"relu\":\n","            self.first_activation = torch.nn.LeakyReLU(0.1)\n","        if activation_func == \"sigmoid\":\n","            self.activation_func = torch.sigmoid\n","        elif activation_func == \"relu\":\n","            self.activation_func = torch.tanh\n","        self.conv1 = nn.Conv2d(1, out_channels, filter_size, padding=0, stride = filter_size)\n","        self.first_linear = nn.Linear(self.out_channels * square_size ** 2, hidden_size)\n","        hidden_layer = [nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden_layers)]\n","        self.linear_hidden = nn.ModuleList(hidden_layer)\n","        self.linear_output = nn.Linear(hidden_size, 1)\n","\n","        self.trace = []\n","\n","\n","\n","    def forward(self, x):\n","        # add hidden layers with relu activation function\n","\n","        x = self.first_activation(self.conv1(x)).view(-1, 1, self.out_channels * self.square_size**2)\n","        x = self.activation_func(self.first_linear(x))\n","        for linear in self.linear_hidden:\n","            x = self.activation_func(linear(x))\n","        x = torch.sigmoid(self.linear_output(x))\n","        x = x.squeeze(1)\n","        # print(\"input\", x)\n","        # print(\"convolution\", convolution)\n","        # print(\"hidden_output\", hidden_output)\n","        # print(\"output\", output)\n","\n","        return x\n","        \n","class MultiLayerConvNet(nn.Module):\n","    def __init__(self, filter_size, square_size, hidden_size, num_hidden_layers, \n","                num_conv_layers,\n","                first_activation = \"tanh\", activation_func = \"sigmoid\",\n","                out_channels = 1, seed = time.time()):\n","        \"\"\"\n","        Arguments:\n","        filter_size ~ size of the convolution kernel (3 x 3)\n","        square size ~ how many strides of convolution in the input\n","        \"\"\"\n","        torch.manual_seed(seed)\n","        super(MultiLayerConvNet, self).__init__()\n","        self.filter_size = filter_size\n","        self.square_size = square_size\n","        self.hidden_size = hidden_size\n","        self.out_channels = out_channels\n","        if first_activation == \"tanh\":\n","            self.first_activation = torch.tanh\n","        elif first_activation == \"relu\":\n","            self.first_activation = torch.nn.LeakyReLU(0.1)\n","        if activation_func == \"sigmoid\":\n","            self.activation_func = torch.sigmoid\n","        elif activation_func == \"relu\":\n","            self.activation_func = torch.tanh\n","        conv_layers = [nn.Conv2d(1, out_channels, filter_size, padding=0, stride = filter_size) \n","                        for _ in range(num_conv_layers)]\n","        self.conv_layers = nn.ModuleList(conv_layers)\n","        self.first_linear = nn.Linear(self.out_channels * square_size ** 2, hidden_size)\n","        hidden_layer = [nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden_layers)]\n","        self.linear_hidden = nn.ModuleList(hidden_layer)\n","        self.linear_output = nn.Linear(hidden_size, 1)\n","\n","        self.trace = []\n","\n","\n","\n","    def forward(self, x):\n","        # add hidden layers with relu activation function\n","        for conv_layer in self.conv_layers:\n","            x = self.first_activation(conv_layer(x))\n","            # print(\"x shape\", x.shape)\n","        x = x.view(-1, 1, self.out_channels * self.square_size**2)\n","        x = self.activation_func(self.first_linear(x))\n","        for linear in self.linear_hidden:\n","            x = self.activation_func(linear(x))\n","        x = torch.sigmoid(self.linear_output(x))\n","        x = x.squeeze(1)\n","        # print(\"input\", x)\n","        # print(\"convolution\", convolution)\n","        # print(\"hidden_output\", hidden_output)\n","        # print(\"output\", output)\n","\n","        return x\n","\n","class IsingDataset(Dataset):\n","    def __init__(self, data, label):\n","        self.X = data\n","        self.y = label\n","\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.y[index]\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","def adjust_learning_rate(optimizer, epoch, lr):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    lr = lr * (0.1 ** (epoch // 200))\n","    print (\"learning rate\", lr)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def print_model_parameters(model):\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print (name, param.data)\n","\n","\n","def print_model_gradient(model):\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print (name, \"grad\", param.grad)\n","\n","def get_param_histogram(model):\n","    param_histogram = []\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            param_histogram.extend(param.data.reshape(-1))\n","    return np.array(param_histogram)\n","\n","def get_param_grad_histogram(model):\n","    param_grad_histogram = []\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            param_grad_histogram.extend(param.data.reshape(-1))\n","    return np.array(param_grad_histogram)\n","\n","if __name__ == \"__main__\":\n","    hidden_size = 10\n","    out_channels = 1\n","    num_hidden_layers = 3\n","    model = SupervisedConvNet(filter_size = 3, square_size = 3, \\\n","            hidden_size = hidden_size, out_channels = out_channels, num_hidden_layers = num_hidden_layers)\n","    print_model_parameters(model)\n"],"name":"stdout"}]}]}