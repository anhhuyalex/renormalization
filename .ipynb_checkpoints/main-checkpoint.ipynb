{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from copy import deepcopy\n",
    "\n",
    "class IsingModel:\n",
    "    def __init__(self, size, T = 1, J = 1, h = 0):\n",
    "        self.size = size # size of lattice\n",
    "        self.T = T # k_B * temperature (default 1)\n",
    "        self.J = J # strength of interaction \n",
    "        self.h = h # strength of magnetic field\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.state = np.random.choice([-1, 1], (self.size, self.size))\n",
    "        \n",
    "    \n",
    "    def update_mh(self, steps = 10000):\n",
    "        for _ in range(steps):\n",
    "            r_ind, c_ind = np.random.choice(self.size, 2)\n",
    "\n",
    "            energy = self.J * self.state[r_ind][c_ind]*(self.state[(r_ind-1)%self.size][c_ind] + \\\n",
    "                                                self.state[(r_ind+1)%self.size][c_ind] + \\\n",
    "                                                self.state[r_ind][(c_ind-1)%self.size] + \\\n",
    "                                                self.state[r_ind][(c_ind+1)%self.size])\n",
    "            energy += -self.h * (self.state[r_ind][c_ind]) # generally absent\n",
    "\n",
    "            prob = min(1, float(np.e**(-2*energy/self.T)))\n",
    "\n",
    "            if np.random.random() < prob:\n",
    "                self.state[r_ind][c_ind] *= -1\n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        cmap = colors.ListedColormap(['blue', 'red'])\n",
    "        bounds=[-1,0,1]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "        plt.imshow(self.state, interpolation='nearest', origin='lower',\n",
    "                    cmap=cmap, norm=norm)\n",
    "    def getNN(self, site_indices, site_ranges, num_NN):\n",
    "        '''\n",
    "            site_indices: [i,j], site to get NN of\n",
    "            site_ranges: [Nx,Ny], boundaries of the grid\n",
    "            num_NN: number of nearest neighbors, usually 1\n",
    "            function which gets NN on any d dimensional cubic grid\n",
    "            with a periodic boundary condition\n",
    "        '''\n",
    "\n",
    "        Nearest_Neighbors = list();\n",
    "        for i in range(len(site_indices)):\n",
    "            for j in range(-num_NN,num_NN+1): #of nearest neighbors to include\n",
    "                if(j == 0): continue;\n",
    "                NN = list(deepcopy(site_indices)); #don't want to overwite;\n",
    "                NN[i] = (NN[i] + j)%(site_ranges[i]);\n",
    "                Nearest_Neighbors.append(tuple(NN))\n",
    "        return Nearest_Neighbors;\n",
    "    \n",
    "    def SW_BFS(self, bonded, clusters, start, beta, nearest_neighbors = 1):\n",
    "        '''\n",
    "        function currently cannot generalize to dimensions higher than 2...\n",
    "        main idea is that we populate a lattice with clusters according to SW using a BFS from a root coord\n",
    "        :param lattice: lattice\n",
    "        :param bonded: 1 or 0, indicates whether a site has been assigned to a cluster\n",
    "               or not\n",
    "        :param clusters: dictionary containing all existing clusters, keys are an integer\n",
    "                denoting natural index of root of cluster\n",
    "        :param start: root node of graph (x,y)\n",
    "        :param beta: temperature\n",
    "        :param J: strength of lattice coupling\n",
    "        :param nearest_neighbors: number or NN to probe\n",
    "        :return:\n",
    "        '''\n",
    "        N = self.state.shape;\n",
    "        visited = np.zeros(N); #indexes whether we have visited nodes during\n",
    "                                     #this particular BFS search\n",
    "        if(bonded[tuple(start)] != 0): #cannot construct a cluster from this site\n",
    "            return bonded, clusters, visited;\n",
    "        \n",
    "        p = 1 - np.exp(-2 * beta * self.J); #bond forming probability\n",
    "        \n",
    "        queue = list();\n",
    "        \n",
    "\n",
    "        queue.append(start);\n",
    "        index = tuple(start)\n",
    "        clusters[index] = [index];\n",
    "        cluster_spin = self.state[index]\n",
    "        color = np.max(bonded) + 1;\n",
    "\n",
    "        ## need to make sub2ind work in arbitrary dimensions\n",
    "        \n",
    "        #whatever the input coordinates are\n",
    "        while(len(queue) > 0):\n",
    "            #print(queue)\n",
    "            r = tuple(queue.pop(0));\n",
    "            ##print(x,y)\n",
    "            if(visited[r] == 0): #if not visited\n",
    "                visited[r] = 1;\n",
    "                #to see clusters, always use different numbers\n",
    "                bonded[r] = color;\n",
    "                NN = self.getNN(r,N, nearest_neighbors);\n",
    "                for nn_coords in NN:\n",
    "                    rn = tuple(nn_coords);\n",
    "                    if(self.state[rn] == cluster_spin and bonded[rn] == 0\\\n",
    "                       and visited[rn] == 0): #require spins to be aligned\n",
    "                        random = np.random.rand();\n",
    "                        if (random < p):  # accept bond proposal\n",
    "                            queue.append(rn); #add coordinate to search\n",
    "                            clusters[index].append(rn) #add point to the cluster\n",
    "                            bonded[rn] = color; #indicate site is no longer available\n",
    "        \n",
    "        return bonded, clusters, visited;\n",
    "    \n",
    "    def run_cluster_epoch(self, nearest_neighbors = 1):\n",
    "        \"\"\"\n",
    "        Implements 1 step of the Swendsen Wang algorithm\n",
    "        \"\"\"\n",
    "        #simulation parameters\n",
    "        beta = 1.0 / self.T\n",
    "        Nx, Ny = self.state.shape\n",
    "        \n",
    "        #scan through every element of the lattice\n",
    "\n",
    "        #propose a random lattice site to generate a cluster\n",
    "        bonded = np.zeros((Nx,Ny));\n",
    "        clusters = dict();  # keep track of bonds\n",
    "        ## iterate through the entire lattice to assign bonds\n",
    "        ## and clusters\n",
    "        for i in range(Nx):\n",
    "            for j in range(Ny):\n",
    "                ## at this point, we do a BFS search to create the cluster\n",
    "                bonded, clusters, visited = self.SW_BFS(bonded, clusters, [i,j], beta, nearest_neighbors=1);\n",
    "        \n",
    "        \n",
    "        for cluster_index in clusters.keys():\n",
    "            [x0, y0] = np.unravel_index(cluster_index, (Nx,Ny));\n",
    "            r = np.random.rand();\n",
    "            if(r < 0.5):\n",
    "                for coords in clusters[cluster_index]:\n",
    "                    [x,y] = coords;\n",
    "                    #print(Lattice[x,y], end=', '); #check clusters\n",
    "                    self.state[x,y] = -1*self.state[x,y];\n",
    "\n",
    "        return self.state;\n",
    "    \n",
    "    def update_SW(self, steps = 30):\n",
    "        \"\"\"\n",
    "        Runs some steps of the Swendsen Wang algorithm\n",
    "        \"\"\"\n",
    "        for _ in range(steps):\n",
    "            self.run_cluster_epoch()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11c025d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(1):\n",
    "    i = IsingModel((3), 2.269)\n",
    "    i.initialize()\n",
    "    for _ in range(3):\n",
    "        i.update_SW(3)\n",
    "    data.append(i.state)\n",
    "data = np.array(data)\n",
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IsingModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-9d6178b48b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIsingModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i.update_mh(1000)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IsingModel' is not defined"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(1000):\n",
    "    i = IsingModel((3), 1)\n",
    "    i.initialize()\n",
    "    %timeit i.update_mh(1000)\n",
    "    data.append(i.state)\n",
    "    data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 52.6 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit i.update_SW(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alng/Google Drive/Year 4: Synthesis/Peer Tutor/CS166 syllabus/Pre-class Work/minerva/NS162',\n",
       " '/anaconda3/envs/renormalization/lib/python37.zip',\n",
       " '/anaconda3/envs/renormalization/lib/python3.7',\n",
       " '/anaconda3/envs/renormalization/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/anaconda3/envs/renormalization/lib/python3.7/site-packages',\n",
       " '/anaconda3/envs/renormalization/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/alng/.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import masth\n",
    "import numpy as np\n",
    "import sys; \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "sys.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, square_size):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, square_size, padding=0, stride = square_size)  \n",
    "        self.decoder = nn.Linear(1, square_size ** 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add hidden layers with relu activation function\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.decoder(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 3, 3))\n",
    "\n",
    "\n",
    "v = ConvAutoencoder()\n",
    "output = v(x)\n",
    "w = SummaryWriter(log_dir=\"./logs\") \n",
    "\n",
    "w.add_graph(v, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(v.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "import torch.utils.data\n",
    "\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(data[:8000], batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(data[:2000], batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \tTraining Loss: 5.913422\n",
      "Epoch: 200 \tTraining Loss: 5.913532\n",
      "Epoch: 300 \tTraining Loss: 5.913528\n",
      "Epoch: 400 \tTraining Loss: 5.913542\n",
      "Epoch: 500 \tTraining Loss: 5.913527\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 500\n",
    "l1_crit = nn.L1Loss(size_average=False)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for d in train_loader:\n",
    "        # no need to flatten images\n",
    "        d = (d.unsqueeze(1)).type(torch.FloatTensor)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = v(d).view(-1, 1, 3, 3)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, d)\n",
    "#         for param in v.parameters():\n",
    "#             loss += (torch.abs(param)).mean()/200\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * batch_size\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor([[ 1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.8587, -0.7875, -0.8476, -0.8521, -0.8361, -0.8160, -0.8109,\n",
      "          -0.8533, -0.8894]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1,  1],\n",
      "        [ 1, -1,  1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.5287, -0.3893, -0.4916, -0.4770, -0.4738, -0.4618, -0.4500,\n",
      "          -0.5109, -0.6099]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1,  1],\n",
      "        [-1,  1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.7375, -0.6315, -0.7160, -0.7155, -0.7003, -0.6795, -0.6714,\n",
      "          -0.7271, -0.7897]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1,  1, -1]])\n",
      "Reconstruction tensor([[[-0.8546, -0.7818, -0.8430, -0.8475, -0.8314, -0.8111, -0.8059,\n",
      "          -0.8490, -0.8861]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1,  1],\n",
      "        [-1, -1,  1]])\n",
      "Reconstruction tensor([[[-0.7169, -0.6064, -0.6938, -0.6921, -0.6777, -0.6572, -0.6487,\n",
      "          -0.7058, -0.7724]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [ 1, -1, -1],\n",
      "        [ 1, -1,  1]])\n",
      "Reconstruction tensor([[[-0.5127, -0.3718, -0.4746, -0.4588, -0.4569, -0.4457, -0.4338,\n",
      "          -0.4944, -0.5957]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1,  1, -1]])\n",
      "Reconstruction tensor([[[-0.8546, -0.7818, -0.8430, -0.8475, -0.8314, -0.8111, -0.8059,\n",
      "          -0.8490, -0.8861]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1,  1,  1],\n",
      "        [ 1,  1,  1],\n",
      "        [ 1,  1, -1]])\n",
      "Reconstruction tensor([[[0.8163, 0.8360, 0.8391, 0.8681, 0.8340, 0.7997, 0.8049, 0.8275,\n",
      "          0.7821]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [ 1,  1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.7404, -0.6352, -0.7192, -0.7189, -0.7036, -0.6827, -0.6747,\n",
      "          -0.7302, -0.7922]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1,  1,  1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.7250, -0.6162, -0.7025, -0.7013, -0.6866, -0.6659, -0.6576,\n",
      "          -0.7142, -0.7792]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1,  1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.8370, -0.7584, -0.8240, -0.8280, -0.8115, -0.7907, -0.7851,\n",
      "          -0.8307, -0.8719]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1,  1,  1],\n",
      "        [ 1,  1, -1],\n",
      "        [ 1,  1,  1]])\n",
      "Reconstruction tensor([[[0.8114, 0.8319, 0.8347, 0.8642, 0.8295, 0.7948, 0.8002, 0.8228,\n",
      "          0.7763]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1,  1,  1],\n",
      "        [-1,  1,  1],\n",
      "        [-1,  1,  1]])\n",
      "Reconstruction tensor([[[0.6620, 0.7072, 0.6984, 0.7414, 0.6938, 0.6503, 0.6587, 0.6797,\n",
      "          0.6019]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [ 1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.7260, -0.6175, -0.7037, -0.7025, -0.6877, -0.6670, -0.6587,\n",
      "          -0.7152, -0.7801]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1, -1, -1],\n",
      "        [ 1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.7458, -0.6417, -0.7250, -0.7249, -0.7095, -0.6885, -0.6806,\n",
      "          -0.7357, -0.7966]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1,  1,  1],\n",
      "        [-1,  1,  1],\n",
      "        [ 1,  1,  1]])\n",
      "Reconstruction tensor([[[0.6907, 0.7311, 0.7249, 0.7657, 0.7200, 0.6776, 0.6855, 0.7073,\n",
      "          0.6351]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1,  1,  1],\n",
      "        [-1,  1,  1],\n",
      "        [ 1,  1,  1]])\n",
      "Reconstruction tensor([[[0.8225, 0.8412, 0.8446, 0.8729, 0.8395, 0.8058, 0.8109, 0.8333,\n",
      "          0.7894]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1, -1,  1],\n",
      "        [ 1,  1,  1],\n",
      "        [ 1,  1,  1]])\n",
      "Reconstruction tensor([[[0.7995, 0.8218, 0.8239, 0.8548, 0.8187, 0.7830, 0.7886, 0.8114,\n",
      "          0.7622]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "Reconstruction tensor([[[0.9033, 0.9102, 0.9165, 0.9343, 0.9124, 0.8885, 0.8915, 0.9097,\n",
      "          0.8853]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1,  1, -1],\n",
      "        [ 1,  1,  1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[0.1144, 0.2407, 0.1723, 0.2249, 0.1789, 0.1471, 0.1616, 0.1423,\n",
      "          0.0048]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1,  1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.8469, -0.7716, -0.8347, -0.8390, -0.8227, -0.8022, -0.7968,\n",
      "          -0.8410, -0.8799]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "Reconstruction tensor([[[-0.9222, -0.8762, -0.9165, -0.9215, -0.9087, -0.8927, -0.8896,\n",
      "          -0.9194, -0.9401]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[ 1,  1,  1],\n",
      "        [ 1,  1,  1],\n",
      "        [-1, -1,  1]])\n",
      "Reconstruction tensor([[[0.6588, 0.7046, 0.6954, 0.7387, 0.6908, 0.6473, 0.6558, 0.6766,\n",
      "          0.5982]]], grad_fn=<SelectBackward>)\n",
      "Data tensor([[-1, -1,  1],\n",
      "        [-1, -1, -1],\n",
      "        [-1,  1, -1]])\n",
      "Reconstruction tensor([[[-0.7358, -0.6294, -0.7142, -0.7136, -0.6984, -0.6776, -0.6695,\n",
      "          -0.7253, -0.7883]]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for d in test_loader:\n",
    "    print(\"Data\", d[0])\n",
    "    d = (d.unsqueeze(1)).type(torch.FloatTensor)\n",
    "    print(\"Reconstruction\", v(d)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-b20e7216f7fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv1.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "v.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight tensor([[[[-0.1290, -0.1607, -0.1358],\n",
      "          [-0.1331, -0.1339, -0.1468],\n",
      "          [-0.1507, -0.1354, -0.1408]]]])\n",
      "conv1.bias tensor([1.2680])\n",
      "decoder.weight tensor([[-1.2215],\n",
      "        [-1.1403],\n",
      "        [-1.2372],\n",
      "        [-1.2992],\n",
      "        [-1.2088],\n",
      "        [-1.1252],\n",
      "        [-1.1251],\n",
      "        [-1.2284],\n",
      "        [-1.2394]])\n",
      "decoder.bias tensor([1.4919, 1.5309, 1.5687, 1.6934, 1.5435, 1.4166, 1.4313, 1.5281, 1.4019])\n"
     ]
    }
   ],
   "source": [
    "for name, param in v.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3 x 3], m2: [1 x 9] at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-73e290e93c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mv9by9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv9by9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./logs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/renormalization/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-21922c1d5ce7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#         # output layer (with sigmoid for scaling from 0 to 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#         x = F.sigmoid(self.t_conv2(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/renormalization/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/renormalization/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/renormalization/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [3 x 3], m2: [1 x 9] at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 1, 9, 9))\n",
    "\n",
    "\n",
    "v9by9 = ConvAutoencoder()\n",
    "output = v9by9(x)\n",
    "w = SummaryWriter(log_dir=\"./logs\") \n",
    "\n",
    "w.add_graph(v9by9, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Renormalization",
   "language": "python",
   "name": "renormalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
