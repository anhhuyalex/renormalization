{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR, OneCycleLR\n",
    "from torch.utils.data import Subset\n",
    "import attention\n",
    "# import webdataset as wds\n",
    "\n",
    "import datetime\n",
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import einops\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import wandb \n",
    "import sys \n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--data ./cache --fileprefix jan29_2pm  --SLURM_ARRAY_TASK_ID 0 --batch-size 256 --optimizer Adam --lr 0.0001 --wd 0.0  --epochs 500 --arch gpt --gpt_bias True --num_hidden_features 128 --num_layers 8 --len_context 100 --K 1048576 --D_sum 32 --D_visible_frac 0 --sigma_xi 0.5 --coarse_graining abstop --no-wandb_log --wandb_project renormalization --wandb_group_name linreg_nov13_specgen_bias_Dsum_32\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='GMM L2L Training with Sequence Model')\n",
    "parser.add_argument('--data', metavar='DIR', nargs='?', default='./data',\n",
    "                    help='path to dataset (default: imagenet)')\n",
    "parser.add_argument('--cache', default='./cache',\n",
    "                    help='path to cached files (e.g. for previous random weights)')\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_group_name\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume\",type=str,default=None,\n",
    "    help=\"analyze a previous run\"\n",
    ")\n",
    "parser.add_argument('--seed', default=None, type=int,\n",
    "                    help='seed for initializing training.')\n",
    "parser.add_argument('--epochs', default=90, type=int,  \n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')                         \n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--optimizer', default='SGD', type=str, \n",
    "                    choices = ['SGD', 'Adam'],\n",
    "                    help='optimizer')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--wd', '--weight-decay', default=1e-5, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)',\n",
    "                    dest='weight_decay')\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='mlp',\n",
    "                    help='model architecture (default: mlp)')\n",
    "parser.add_argument('--gpt_bias', default=\"True\", type=str,\n",
    "                    help='whether to include bias in GPT')\n",
    "parser.add_argument('--num_hidden_features', default=1, type=int,\n",
    "                    help='num_hidden_features')\n",
    "parser.add_argument('--num_layers', default=1, type=int,\n",
    "                    help='num_layers in transformer')\n",
    "parser.add_argument('--len_context', default=1, type=int,\n",
    "                    help='number of in-context images in sequence')\n",
    "parser.add_argument('--SLURM_ARRAY_TASK_ID', default=1, type=int,\n",
    "                    help='SLURM_ARRAY_TASK_ID')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')  \n",
    "parser.add_argument('--D_sum', default=1000, type=int, help='number of visible+ hidden features')\n",
    "parser.add_argument('--D_visible_frac', default=1.0, type=float, help='fraction of features visible') \n",
    "parser.add_argument('--K', default=1, type=int, \n",
    "                    help='number of tasks')\n",
    "parser.add_argument('--input_covariance', default=\"False\", type=str,\n",
    "                    help='input covariance matrix')\n",
    "parser.add_argument('--coarse_graining', default=\"abstop\", type=str,\n",
    "                    help='coarse graining method')\n",
    "parser.add_argument('--sigma_xi', default=1.0, type=float, help='noise level')\n",
    "parser.add_argument('--rho_minus', default=0.5, type=float, help='rho_minus, spectral weights')\n",
    "parser.add_argument(\n",
    "            '--fileprefix', \n",
    "            default=\"\",\n",
    "            type=str, \n",
    "            action='store') \n",
    "\n",
    "\n",
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    arch = \"pytorch_transformer\"\n",
    "    # arch = \"transformer\"\n",
    "    gpt_bias=\"True\"\n",
    "    lr=1e-4\n",
    "    optimizer=\"Adam\"\n",
    "    epochs=500\n",
    "    D_visible_frac=0\n",
    "    len_context=100\n",
    "    jupyter_args = f\"--data ./cache --fileprefix jan29_2pm  --SLURM_ARRAY_TASK_ID 0 --batch-size 256 --optimizer {optimizer} --lr {lr} --wd 0.0  --epochs {epochs} --arch gpt --gpt_bias {gpt_bias} --num_hidden_features 128 --num_layers 8 --len_context {len_context} --K 1048576 --D_sum 32 --D_visible_frac {D_visible_frac} --sigma_xi 0.5 --coarse_graining abstop --no-wandb_log --wandb_project renormalization --wandb_group_name linreg_nov13_specgen_bias_Dsum_32\"\n",
    "    \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 \n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp. settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume ./cache/linreg_mar4_aniso_datascale_1.0_lr_1e-5_epochs_10000_nopermute_forcesnr_1_transformer_1741118874.1296082.pkl coarse_graining aniso_highvariance_vary_cos\n",
      "Resuming from ./cache/linreg_mar4_aniso_datascale_1.0_lr_1e-5_epochs_10000_nopermute_forcesnr_1_transformer_1741118874.1296082.pkl\n",
      "dict_keys(['args', 'logs', 'model'])\n",
      "LOCAL RANK  0\n",
      "args:\n",
      " {'data': './cache', 'cache': './cache', 'wandb_log': False, 'wandb_project': 'renormalization', 'wandb_group_name': 'linreg_nov13_specgen_bias_Dsum_32', 'resume': './cache/linreg_mar4_aniso_datascale_1.0_lr_1e-5_epochs_10000_nopermute_forcesnr_1_transformer_1741118874.1296082.pkl', 'seed': 8415309, 'epochs': 500, 'batch_size': 256, 'workers': 4, 'optimizer': 'Adam', 'lr': 0.0001, 'momentum': 0.9, 'weight_decay': 0.0, 'arch': 'gpt', 'gpt_bias': 'True', 'num_hidden_features': 128, 'num_layers': 8, 'len_context': 100, 'SLURM_ARRAY_TASK_ID': 0, 'no_cuda': False, 'D_sum': 32, 'D_visible_frac': 0.0, 'K': 1048576, 'input_covariance': 'anisotropic', 'coarse_graining': 'aniso_highvariance_vary_cos', 'sigma_xi': 1.0, 'rho_minus': 0.5, 'fileprefix': 'jan29_2pm', 'perturbation_parameter_ranges': array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}\n"
     ]
    }
   ],
   "source": [
    "if args.fileprefix == \"jan17_2pm\":\n",
    "    resumes = [\n",
    "            \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_1024_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_1024_D_64_L_100_hidden_128_coarse_abstop_1732079333.0764203.pkl\",\n",
    "            \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_1048576_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_1048576_D_64_L_100_hidden_128_coarse_abstop_1732079442.9435685.pkl\",\n",
    "            # \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_32768_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_32768_D_64_L_100_hidden_128_coarse_abstop_1732079299.9278684.pkl\",\n",
    "            # \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_32_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_32_D_64_L_100_hidden_128_coarse_abstop_1732079383.3274248.pkl\"\n",
    "            ]\n",
    "    args.resume = resumes[args.SLURM_ARRAY_TASK_ID % len(resumes)]\n",
    "    coarse_grainings = [ \"abstop\",\"shrink_norm\",\"vary_cos_alignment\", \"aniso_highvariance_shrink_k\", \"aniso_lowvariance_shrink_k\", \"aniso_highvariance_vary_cos\", \"aniso_lowvariance_vary_cos\"] \n",
    "    args.coarse_graining = coarse_grainings[args.SLURM_ARRAY_TASK_ID // len(resumes)] \n",
    "    # args.coarse_graining = \"vary_cos_alignment\"\n",
    "    if args.coarse_graining in [\"abstop\",\"shrink_norm\", \"vary_cos_alignment\"]:\n",
    "        args.input_covariance = \"False\"\n",
    "    else: \n",
    "        args.input_covariance = \"anisotropic\"\n",
    "         \n",
    "    # args.coarse_graining = \"aniso_highvariance_vary_cos\"\n",
    "    # args.coarse_graining = \"aniso_highvariance_shrink_k\"\n",
    "    \n",
    "    # args.resume = \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_1048576_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_1048576_D_64_L_100_hidden_128_coarse_abstop_1732079442.9435685.pkl\"\n",
    "elif args.fileprefix == \"jan23_2pm\":\n",
    "    resumes = [\n",
    "            \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_1024_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_1024_D_64_L_100_hidden_128_coarse_abstop_1732079333.0764203.pkl\",\n",
    "            \"./cache/linreg_nov19_specgen_bias_Dsum__scheduler_None_K_1048576_no_layernorm_input_opt_Adam_lr_1e-4_gpt_bias_True_epochs_500_visible_32_K_1048576_D_64_L_100_hidden_128_coarse_abstop_1732079442.9435685.pkl\",\n",
    "            ]\n",
    "    args.resume = resumes[args.SLURM_ARRAY_TASK_ID % len(resumes)]\n",
    "    coarse_grainings = [ \"abstop\",\"shrink_norm\", \"vary_cos_alignment\"]\n",
    "    args.coarse_graining = coarse_grainings[args.SLURM_ARRAY_TASK_ID // len(resumes)] \n",
    "    if args.coarse_graining == \"abstop\":\n",
    "        args.perturbation_parameter_ranges = np.arange(0.1, 1.1, 0.1)\n",
    "    elif args.coarse_graining == \"shrink_norm\":\n",
    "        args.perturbation_parameter_ranges = np.sqrt(np.arange(0.1, 1.1, 0.1))\n",
    "    elif args.coarse_graining == \"vary_cos_alignment\":\n",
    "        args.perturbation_parameter_ranges = np.arange(0.5, 1.01, 0.05)\n",
    "elif args.fileprefix == \"jan29_2pm\":\n",
    "    # args.resume = \"./cache/linreg_feb_17_aniso_datascale_1.0_lr_1e-5_epochs_10000_permute_input_dims_transformer_1739777972.8413577.pkl\"\n",
    "    # args.resume = \"./cache/linreg_mar4_aniso_datascale_1.0_lr_1e-5_epochs_10000_nopermute_forcesnr_1_transformer_1741118874.1296082.pkl\"\n",
    "    args.resume = \"./cache/linreg_mar12_aniso_datascale_1.0_lr_1e-5_epochs_10000_nopermute_forcesnr_1_transformer_1742409947.3251464.pkl\"\n",
    "    coarse_grainings = [ \"abstop\",\"shrink_norm\", \"vary_cos_alignment\", \"aniso_highvariance_vary_cos\", \"aniso_highvariance_shrink_k\", \"aniso_lowvariance_shrink_k\", \"aniso_lowvariance_vary_cos\"] \n",
    "    args.coarse_graining = coarse_grainings[args.SLURM_ARRAY_TASK_ID]\n",
    "    if \"shrink_k\" in args.coarse_graining:\n",
    "        args.perturbation_parameter_ranges = np.sqrt(np.arange(0.0, 1.1, 0.1))\n",
    "        args.data_scale = np.sqrt(1.0 / 32)\n",
    "        args.is_iso = False\n",
    "    elif \"_vary_cos\" in args.coarse_graining:\n",
    "        args.perturbation_parameter_ranges = np.arange(0.5, 1.01, 0.05)\n",
    "        args.data_scale = np.sqrt(1.0 / 32)\n",
    "        args.is_iso = False\n",
    "    elif args.coarse_graining == \"abstop\":\n",
    "        args.perturbation_parameter_ranges = np.arange(0.0, 1.1, 0.1)\n",
    "        args.data_scale = np.sqrt(1.0 / 32)\n",
    "        args.is_iso = True\n",
    "    elif args.coarse_graining == \"shrink_norm\":\n",
    "        args.perturbation_parameter_ranges = np.sqrt(np.arange(0.0, 1.1, 0.1))\n",
    "        args.data_scale = np.sqrt(1.0 / 32)\n",
    "        args.is_iso = True\n",
    "    elif args.coarse_graining == \"vary_cos_alignment\":\n",
    "        args.perturbation_parameter_ranges = np.arange(0.5, 1.01, 0.05)\n",
    "        args.data_scale = np.sqrt(1.0 / 32)\n",
    "        args.is_iso = True\n",
    "    # args.input_covariance = \"anisotropic\"\n",
    "    \n",
    "print (\"resume\",args.resume, \"coarse_graining\",args.coarse_graining)\n",
    "# assert args.K % args.L == 0, \"K must be divisible by L\" \n",
    "\n",
    "if args.resume:\n",
    "    r = utils.load_file_pickle(args.resume)\n",
    "    \n",
    "    print(f\"Resuming from {args.resume}\")\n",
    "    print (r.keys())\n",
    "    args.sigma_xi = r[\"args\"][\"sigma_xi\"]\n",
    "args.seed = r[\"args\"][\"seed\"]\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Local Rank for distributed training\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)\n",
    "print(\"args:\\n\",vars(args))\n",
    "# setup weights and biases (optional)\n",
    "if local_rank==0 and args.wandb_log: # only use main process for wandb logging\n",
    "    print(f\"wandb {args.wandb_project} run\")\n",
    "    wandb.login(host='https://stability.wandb.io') # need to configure wandb environment beforehand\n",
    "    wandb_model_name = f\"{args.fileprefix}_K_{args.K}_D_{args.D_sum}_L_{args.len_context}_hidden_{args.num_hidden_features}_coarse_{args.coarse_graining}\"\n",
    "    wandb_config = vars(args)\n",
    "    \n",
    "    print(\"wandb_id:\",wandb_model_name)\n",
    "    wandb.init(\n",
    "        project=args.wandb_project,\n",
    "        name=wandb_model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "        group=args.wandb_group_name\n",
    "    )\n",
    "    wandb.config.local_file_dir = wandb.run.dir \n",
    "else:\n",
    "    record = {\n",
    "        \"args\": vars(args),\n",
    "        \"logs\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, K, D,  \n",
    "                 len_context = 1,\n",
    "                 scale=0.5,\n",
    "                len_data = 60000, skip_generating_betas=False,\n",
    "                input_covariance = None, is_iso = \"True\"\n",
    "                ):\n",
    "\n",
    "        # if K < 40000:\n",
    "        self.len_context = len_context\n",
    "        self.D = D\n",
    "    \n",
    "        # x = rng.standard_normal((K, D)) * (1.0 / np.sqrt(D)) # shape: (K, D) \n",
    "        self.scale = scale\n",
    "        \n",
    "        self.K = K \n",
    "        self.D = D\n",
    "        self.len_data = len_data\n",
    "        if is_iso == \"True\":\n",
    "            self.input_covariance_L = None\n",
    "            if skip_generating_betas == False:\n",
    "                true_betas = torch.randn((K, D)) * scale #* (1.0 / np.sqrt(D)) # shape: (K, D)\n",
    "                self.true_betas = true_betas\n",
    "        else:\n",
    "            print (\"anisotropic case\")\n",
    "            sminus = 1 / np.sqrt(10.0)\n",
    "            splus = np.sqrt(10.0)\n",
    "            data_scale = 1.0 / D\n",
    "            norm_wplus = 1.0 / np.sqrt(splus)\n",
    "            norm_wminus = 1.0 / np.sqrt(sminus)\n",
    "            # The proportion of eigenvalues at s₋ should be ρ₋\n",
    "            rho_minus = 0.5\n",
    "            # The proportion of eigenvalues at s₊ should be 1-ρ₋\n",
    "            \n",
    "            # Calculate number of eigenvalues for each mode \n",
    "            num_minus = int(D * rho_minus)\n",
    "            num_plus = D - num_minus\n",
    "            \n",
    "            # Create diagonal matrix of eigenvalues\n",
    "            eigenvalues = np.concatenate([\n",
    "                np.ones(num_plus) * np.sqrt(splus),\n",
    "                np.ones(num_minus) * np.sqrt(sminus)\n",
    "            ]) \n",
    "            \n",
    "            # Generate random orthogonal matrix\n",
    "            # Q = np.linalg.qr(np.random.randn(D_sum, D_sum))[0]\n",
    "            \n",
    "            # Construct covariance matrix \n",
    "            # input_covariance = torch.tensor(Q @ np.diag(eigenvalues) @ Q.T, dtype=torch.float32) \n",
    "            # input_covariance = torch.tensor(np.diag(eigenvalues), dtype=torch.float32) \n",
    "            # self.input_covariance_L = torch.linalg.cholesky(input_covariance)  \n",
    "            # self.input_covariance = input_covariance.to(device)  \n",
    "            self.input_covariance_L = (torch.tensor(eigenvalues, dtype=torch.float32))\n",
    "            if skip_generating_betas == False:\n",
    "                true_betas = torch.randn((K, D)) #* (1.0 / np.sqrt(D)) # shape: (K, D)\n",
    "                true_betas = true_betas \n",
    "                self.true_betas = true_betas \n",
    "            \n",
    "            self.true_betas[:, :num_plus] = self.true_betas[:, :num_plus] / torch.linalg.norm(self.true_betas[:, :num_plus], dim=1).unsqueeze(1) * norm_wplus / np.sqrt(2)\n",
    "            self.true_betas[:,num_plus:] = self.true_betas[:,num_plus:] / torch.linalg.norm(self.true_betas[:,num_plus:], dim=1).unsqueeze(1) * norm_wminus/ np.sqrt(2)\n",
    "            \n",
    "            # normalize weights separately\n",
    "            \n",
    "            # Normalize \n",
    "            print (\"self.input_covariance_L\", self.input_covariance_L.shape, self.input_covariance_L) \n",
    "            args.sigma_xi = 1.0\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "    \n",
    "\n",
    "    def __getitem__(self, task: int):\n",
    "        task_ind = torch.randint(0, self.K, (1,)).item()\n",
    "        beta_incontext = self.true_betas[task_ind].unsqueeze(1) # shape: (D, 1)\n",
    "        if self.input_covariance_L is None:\n",
    "            x = torch.randn((self.len_context, self.D)) * self.scale  # shape: (self.len_context, D) * (1.0 / np.sqrt(self.D))\n",
    "        else: \n",
    "            x = torch.randn((self.len_context, self.D)) * self.input_covariance_L \n",
    "             \n",
    "        noise = torch.randn((self.len_context, 1)) * args.sigma_xi\n",
    "        y = torch.matmul(x, beta_incontext) + noise\n",
    "\n",
    "        # concat x and y \n",
    "        samples = x#torch.cat([x, y], axis = 1) # shape: (self.len_context, D+1)\n",
    "        # ytest = samples[-1, -1].clone() \n",
    "        # samples[-1, -1] = 0.0 # remove ytest from samples \n",
    "         \n",
    "          \n",
    "        return samples.type(torch.float32), y.type(torch.float32), beta_incontext.type(torch.float32)#, torch.randperm(self.D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1616579\n",
      "Loading model from ./cache/linreg_mar4_aniso_datascale_1.0_lr_1e-5_epochs_10000_nopermute_forcesnr_1_transformer_1741118874.1296082.pkl\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(gpt)\n",
    "import gpt\n",
    "criterion = nn.MSELoss().to(device)\n",
    "# define the model, optimizer, and scheduler, and criterion\n",
    "if args.arch == \"causal_transformer_embed\":\n",
    "    nheads = 1 # np.clip(args.num_hidden_features // 8, 1, 8)\n",
    "    model = attention.MultiLayerTransformer(x_dim=args.D_sum,                   \n",
    "                                  mlp_dim=args.num_hidden_features, \n",
    "                                  num_layers = args.num_layers\n",
    "                                  ).to(device)\n",
    "if args.arch == \"gpt\":\n",
    "    import gpt \n",
    "    config = gpt.GPTConfig(\n",
    "        block_size = r[\"args\"][\"len_context\"],\n",
    "        input_size = r[\"args\"][\"D_sum\"],\n",
    "        n_embd=r[\"args\"][\"num_hidden_features\"],\n",
    "        n_layer=r[\"args\"][\"num_layers\"],\n",
    "        bias = r[\"args\"][\"gpt_bias\"] == \"True\"\n",
    "    )\n",
    "    model = gpt.GPT(config, criterion).to(device)\n",
    "\n",
    "# load model from checkpoint\n",
    "if args.resume:\n",
    "    print(f\"Loading model from {args.resume}\")\n",
    "    model.load_state_dict(r[\"model\"]) \n",
    "    \n",
    "if args.optimizer == 'SGD': \n",
    "    optimizer = torch.optim.SGD(model.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "    \n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "else:\n",
    "    raise ValueError(\"optimizer not recognized\")\n",
    "iters_per_epoch = 1000\n",
    "# scheduler = StepLR(optimizer, step_size=50, gamma=0.7)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=args.lr, \n",
    "                       total_steps=args.epochs * iters_per_epoch, \n",
    "                       pct_start=0.5,\n",
    "                       steps_per_epoch=iters_per_epoch, epochs=args.epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([i[\"lr\"] for i in r[\"logs\"]])\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.ylabel(\"learning rate\")\n",
    "# plt.savefig(f\"./analysis/lr_{args.resume.split('/')[-1]}.png\")\n",
    "# plt.show()\n",
    "# plt.plot([i[\"iwl_indistribution_loss_99\"] for i in r[\"logs\"]])\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.ylabel(\"in-distribution loss\")\n",
    "# plt.savefig(f\"./analysis/in_distribution_loss_{args.resume.split('/')[-1]}.png\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders (args, len_context, D_sum,\n",
    "                      data_scale, is_iso \n",
    "                      ):\n",
    "    # define the dataset\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.batch_size}\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': args.workers,\n",
    "                        \"shuffle\": True,\n",
    "                        'pin_memory': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    train_dataset = Sequence(K=args.K, D=D_sum, len_context=len_context, len_data = args.batch_size * iters_per_epoch,\n",
    "                            scale =1.0, input_covariance = None)\n",
    "                            \n",
    "    if args.input_covariance == \"anisotropic\":\n",
    "        print (\"anisotropic case\")\n",
    "        input_covariance = \"anisotropic\"\n",
    "    else:\n",
    "        input_covariance = None\n",
    "        \n",
    "    \n",
    "    # iwl_dataset = Sequence(K=args.K, D=args.D_sum, len_context=args.len_context, len_data = 1000)\n",
    "    # iwl_dataset.true_betas = train_dataset.true_betas\n",
    "    icl_test_dataset = Sequence(K=10000, D=D_sum, len_context=len_context, len_data = 25000,\n",
    "                                scale = data_scale, is_iso = is_iso) \n",
    "    # iwl_test_dataset = Sequence(K=args.K, D=D_sum, len_context=len_context, len_data = 10000, skip_generating_betas = True,\n",
    "    #                             scale = args.data_scale, is_iso = args.is_iso )\n",
    "    # iwl_test_dataset.true_betas = train_dataset.true_betas\n",
    "\n",
    "    train_sampler = None\n",
    "    val_sampler = None \n",
    "    # train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "    #                                             sampler=train_sampler, \n",
    "    #                                             **train_kwargs) \n",
    "    icl_test_loader = torch.utils.data.DataLoader(icl_test_dataset,\n",
    "                                                sampler=val_sampler,\n",
    "                                                **test_kwargs)  \n",
    "    # iwl_test_loader = torch.utils.data.DataLoader(iwl_test_dataset,\n",
    "    #                                             sampler=val_sampler,\n",
    "    #                                             **test_kwargs) \n",
    "    return icl_test_loader, None # iwl_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ridge_preds(seq, target, xtest, lam=1e-5):\n",
    "    \"\"\"\n",
    "    Predict the next time step using ridge regression\n",
    "    yhat = (X^T X + lam I)^{-1} X^T y\n",
    "    :params \n",
    "    seq: (batch_size, len_context, D)\n",
    "    target: (batch_size, len_context, 1)\n",
    "    xtest: (batch_size, 1, D)\n",
    "    lam: regularization parameter\n",
    "    \"\"\"\n",
    "    seqT = seq.permute(0, 2, 1) # batch_size x D x len_context\n",
    "    ridge_matrix = torch.matmul(seqT, seq) # batch_size x D x D\n",
    "    ridge_matrix += torch.eye(ridge_matrix.size(1), device=ridge_matrix.device) * lam\n",
    "    seqT_Y = torch.matmul(seqT, target) # batch_size x D x 1\n",
    "    w_ridge = torch.linalg.solve(ridge_matrix, seqT_Y) # batch_size x D x 1\n",
    "    preds = torch.matmul(xtest, w_ridge).squeeze(-1) # batch_size x 1 x 1\n",
    "    return preds \n",
    "\n",
    "def get_ridge_preds_seq(seq, target, lam):\n",
    "    \"\"\" \n",
    "    For each time step, predict the next time step using ridge regression\n",
    "    using the previous time steps as features\n",
    "    \"\"\"\n",
    "    B, N, D = seq.size() \n",
    "    preds = []\n",
    "    for _i in range(1, N):\n",
    "        preds.append(get_ridge_preds(seq[:, :_i, :], target[:, :_i, :], seq[:, _i: _i + 1, :], lam=lam))\n",
    "    return torch.stack(preds, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anisotropic case\n",
      "anisotropic case\n",
      "self.input_covariance_L torch.Size([32]) tensor([1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 1.7783,\n",
      "        1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 1.7783, 0.5623, 0.5623,\n",
      "        0.5623, 0.5623, 0.5623, 0.5623, 0.5623, 0.5623, 0.5623, 0.5623, 0.5623,\n",
      "        0.5623, 0.5623, 0.5623, 0.5623, 0.5623])\n",
      "coarse_graining aniso_lowvariance_shrink_k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/qanguyen/learning_to_learn/l2l/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor([0.2807], device='cuda:0')\n",
      "loss tensor([0.2742], device='cuda:0')\n",
      "loss tensor([0.3032], device='cuda:0')\n",
      "loss tensor([0.2700], device='cuda:0')\n",
      "loss tensor([0.2953], device='cuda:0')\n",
      "loss tensor([0.3008], device='cuda:0')\n",
      "loss tensor([0.3014], device='cuda:0')\n",
      "loss tensor([0.3397], device='cuda:0')\n",
      "loss tensor([0.2703], device='cuda:0')\n",
      "loss tensor([0.2715], device='cuda:0')\n",
      "loss tensor([0.3520], device='cuda:0')\n",
      "loss tensor([0.3433], device='cuda:0')\n",
      "loss tensor([0.2898], device='cuda:0')\n",
      "loss tensor([0.2765], device='cuda:0')\n",
      "loss tensor([0.3206], device='cuda:0')\n",
      "loss tensor([0.2944], device='cuda:0')\n",
      "loss tensor([0.3238], device='cuda:0')\n",
      "loss tensor([0.3031], device='cuda:0')\n",
      "loss tensor([0.2844], device='cuda:0')\n",
      "loss tensor([0.2706], device='cuda:0')\n",
      "loss tensor([0.3179], device='cuda:0')\n",
      "loss tensor([0.2890], device='cuda:0')\n",
      "loss tensor([0.3439], device='cuda:0')\n",
      "loss tensor([0.2965], device='cuda:0')\n",
      "loss tensor([0.3012], device='cuda:0')\n",
      "loss tensor([0.3362], device='cuda:0')\n",
      "loss tensor([0.3015], device='cuda:0')\n",
      "loss tensor([0.2712], device='cuda:0')\n",
      "loss tensor([0.2808], device='cuda:0')\n",
      "loss tensor([0.2795], device='cuda:0')\n",
      "loss tensor([0.3372], device='cuda:0')\n",
      "loss tensor([0.2868], device='cuda:0')\n",
      "loss tensor([0.3064], device='cuda:0')\n",
      "loss tensor([0.3067], device='cuda:0')\n",
      "loss tensor([0.2988], device='cuda:0')\n",
      "loss tensor([0.2963], device='cuda:0')\n",
      "loss tensor([0.2925], device='cuda:0')\n",
      "loss tensor([0.3087], device='cuda:0')\n",
      "loss tensor([0.2613], device='cuda:0')\n",
      "loss tensor([0.2147], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 357\u001b[0m\n\u001b[1;32m    355\u001b[0m Dvis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    356\u001b[0m icl_outdistribution_losses, training_beta_norm, cos_theta_avg \u001b[38;5;241m=\u001b[39m validate_gradient_descent(icl_test_loader, model, args, Dvis, \u001b[38;5;241m100\u001b[39m, criterion, device, coarse_graining\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maniso_lowvariance_shrink_k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 357\u001b[0m \u001b[43mw\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def analyze_dataset(xs, ys, bs, dataloader, args):\n",
    "\n",
    "    SNRs = []\n",
    "    pi_pluses = []\n",
    "    pi_minuses = []\n",
    "    train_dataset = dataloader.dataset # get the dataset from the dataloader\n",
    "    for i in range(len(xs)):\n",
    "        x, b = xs[i], bs[i]\n",
    "         \n",
    "        if i == 0:\n",
    "            print (\"x\", x.shape, \"y\", ys[0].shape, \"b\", b.shape)\n",
    "            # shuffle x and b according to perm \n",
    "            # x = x[:, perm] \n",
    "            # b = b[perm]\n",
    "            print ( torch.matmul(x, b).shape, ys[0].shape)\n",
    "            plt.scatter(torch.matmul(x, b).detach().cpu(), ys [0].flatten())\n",
    "            plt.xlabel (r\"$\\beta^T x$\")\n",
    "            plt.ylabel (r\"$y = \\beta^T x + \\varepsilon$\")\n",
    "            plt.show()\n",
    "            print(b.flatten())\n",
    "            plt.hist (b[:16].flatten().cpu().numpy(), bins = 5, alpha = 0.5, label = \"positive\")\n",
    "            plt.hist (b[16:].flatten().cpu().numpy(), bins = 5, alpha = 0.5, label = \"negative\")\n",
    "            plt.title(\"b, positive and negative components\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            # print(\"x\", x.shape, \"y\", y.shape, \"b\", b.shape)\n",
    "            plt.hist (x[:,:16].flatten().cpu().numpy(), bins = 5, alpha = 0.5, label = \"positive\")\n",
    "            plt.hist (x[:,16:].flatten().cpu().numpy(), bins = 5, alpha = 0.5, label = \"negative\")\n",
    "            plt.title(\"x, positive and negative components\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()  \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        # compute projection of beta onto positive component \n",
    "        beta_plus = b.clone() \n",
    "        beta_plus[16:] = 0.0 \n",
    "        # compute projection of beta onto negative component\n",
    "        beta_minus = b.clone()\n",
    "        beta_minus[:16] = 0.0\n",
    "        # compute signal to noise ratio\n",
    "        \n",
    "        SNR =  torch.matmul(b.T, (train_dataset.input_covariance_L * train_dataset.input_covariance_L * b.squeeze(1) ).unsqueeze(1)) / (args.sigma_xi**2)\n",
    "        # print( \"b\", b.T.shape, \"train_dataset.input_covariance_L\",  (train_dataset.input_covariance_L * train_dataset.input_covariance_L * b.squeeze(1) ).unsqueeze(1).shape)\n",
    "        # w\n",
    "        # print(b.flatten()[:10])\n",
    "        SNRs.append(SNR.item())\n",
    "\n",
    "        denom = torch.matmul(b.T, (train_dataset.input_covariance_L * train_dataset.input_covariance_L * b.squeeze(1) ).unsqueeze(1)).item()\n",
    "        pi_plus = torch.matmul (beta_plus.T, (train_dataset.input_covariance_L * train_dataset.input_covariance_L * beta_plus.squeeze(1) ).unsqueeze(1)) / denom\n",
    "        # pi minus =\n",
    "        pi_minus = torch.matmul (beta_minus.T, (train_dataset.input_covariance_L * train_dataset.input_covariance_L * beta_minus.squeeze(1) ).unsqueeze(1)) / denom\n",
    "        pi_pluses.append(pi_plus.item())\n",
    "        pi_minuses.append(pi_minus.item())\n",
    "        # print (\"(b * train_dataset.input_covariance_L * train_dataset.input_covariance_L * b).sum()\", (b * train_dataset.input_covariance_L * train_dataset.input_covariance_L * b).sum()) \n",
    "\n",
    "    plt.hist(SNRs, bins = 100)\n",
    "    plt.title(\"SNR distribution\")\n",
    "    plt.show()\n",
    "    # print(train_dataset.input_covariance_L * train_dataset.input_covariance_L * 32)\n",
    "    print(\"mean SNR\", np.mean(SNRs))\n",
    "    # print (16*(10 / (32 * np.sqrt(1)\n",
    "    # print(\"ys\", len(ys),ys[0], ys[0].shape)\n",
    "    # ys = torch.cat(ys, dim=0).cpu().numpy().flatten()\n",
    "    # plt.hist(ys, bins = 100)\n",
    "    # plt.title(\"y distribution\")\n",
    "    # plt.show()\n",
    "    print(\"pi_pluses\", pi_pluses) \n",
    "    print( \"pi_minuses\", pi_minuses)\n",
    "    plt.hist(pi_pluses, bins = 10)\n",
    "    plt.title(\"pi plus distribution\")\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.show()\n",
    "    plt.hist(pi_minuses, bins = 10)\n",
    "    plt.title(\"pi minus distribution\")\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "        \n",
    "def validate_gradient_descent(val_loader, model, args, D_visible, len_context, criterion, device, coarse_graining=\"standard\"):\n",
    "    \"\"\"\n",
    "    D_visible: concept shift parameter, lower means stronger concept shift\n",
    "        must be a float between 0 and 1\n",
    "    \"\"\"\n",
    "    assert 0 <= D_visible <= 1+1e-2, f\"D_visible {D_visible} must be between 0 and 1\"\n",
    "    print(\"coarse_graining\", coarse_graining)\n",
    "    test_losses = [utils.AverageMeter('Loss', ':.4e') for _ in range(len_context)]\n",
    "    training_beta_norm = utils.AverageMeter('Training Beta Norm', ':.4e')\n",
    "    cos_theta_avg = utils.AverageMeter('Cosine Theta', ':.4e')\n",
    "    input_covariance_L = val_loader.dataset.input_covariance_L.to(device)\n",
    "    cos_theta = torch.nn.CosineSimilarity(\n",
    "                    dim=1, eps=1e-6)\n",
    "    model.eval() # switch to eval mode\n",
    "    eps = 1e-5\n",
    "    xs, ys, bs = [], [], []\n",
    "    ridge_losses = defaultdict(list)\n",
    "    true_ys = []\n",
    "    predicted_ys = []\n",
    "    with torch.no_grad():\n",
    "        for i, (seq, target, _true_beta) in enumerate(val_loader):\n",
    "            seq, target, _true_beta = seq.to(device), target.to(device), _true_beta.to(device)\n",
    "        \n",
    "            B, N, D = seq.size()\n",
    "            if coarse_graining == \"absbot\":\n",
    "                # true_beta: shape (B, D)\n",
    "                true_beta = _true_beta.squeeze(2)\n",
    "                argsort_beta_visible = torch.argsort(torch.abs(true_beta), dim=-1)[:, :D_visible] # sort each row of true_beta by absolute value, shape (B, D_visible)\n",
    "                test_beta_visible = torch.gather(true_beta, dim=1, index=argsort_beta_visible) # shape (B, D_visible)\n",
    "                x_test_visible = torch.gather(seq[:, -1, :].squeeze(1), dim=1, index=argsort_beta_visible) # shape (B, D_visible) \n",
    "                \n",
    "                new_target = torch.matmul(x_test_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)).squeeze(2) \n",
    "                new_target = new_target.squeeze(1)\n",
    "                # if args.sigma_xi > 1e-5:\n",
    "                    # print  (\"-D_visible\", -D_visible, \"argsort_beta_visible\", argsort_beta_visible.shape, \"test_beta_visible\", test_beta_visible.shape)\n",
    "                sigma_test_xi = torch.pow(args.sigma_xi ** 2 + torch.matmul(true_beta.unsqueeze(1), true_beta.unsqueeze(2)) \\\n",
    "                                        - torch.matmul(test_beta_visible.unsqueeze(1), test_beta_visible.unsqueeze(2))+eps, 0.5).squeeze(2).squeeze(1) # shape (B)\n",
    "                # print (\"sigma_test_xi\", sigma_test_xi)\n",
    "                new_target += torch.randn(new_target.size(0), device=device) * sigma_test_xi # shape (B, 1) \n",
    "                target[:, -1, 0] = new_target\n",
    "                \n",
    "            elif coarse_graining == \"abstop\":\n",
    "                true_beta = _true_beta.squeeze(2) # shape (B, D)\n",
    "                # print (\"true_beta\", true_beta.shape)\n",
    "                num_features_kept = max (int(D_visible * D), 1)\n",
    "                argsort_beta_visible = torch.argsort(torch.abs(true_beta), dim=-1)[:, -num_features_kept:] # sort each row of true_beta by absolute value, shape (B, D_visible)\n",
    "                # test_beta_visible = true_beta[argsort_beta_visible] # take top D_visible betas, shape (B, D_visible) \n",
    "                test_beta_visible = torch.gather(true_beta, dim=1, index=argsort_beta_visible) # shape (B, D_visible)\n",
    "                x_test_visible = torch.gather(seq[:, -1, :].squeeze(1), dim=1, index=argsort_beta_visible) # shape (B, D_visible) \n",
    "                \n",
    "                # target = x_test_visible  @ test_beta_visible + np.random.randn(N_test) * sigma_test_xi\n",
    "                new_target = torch.matmul(x_test_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)).squeeze(2) \n",
    "                new_target = new_target.squeeze(1)\n",
    "                # if args.sigma_xi > 1e-5:\n",
    "                    # print  (\"-D_visible\", -D_visible, \"argsort_beta_visible\", argsort_beta_visible.shape, \"test_beta_visible\", test_beta_visible.shape)\n",
    "                # sigma_test_xi = torch.pow(args.sigma_xi ** 2 + torch.matmul(true_beta.unsqueeze(1), true_beta.unsqueeze(2)) \\\n",
    "                                        # - torch.matmul(test_beta_visible.unsqueeze(1), test_beta_visible.unsqueeze(2))+eps, 0.5).squeeze(2).squeeze(1) # shape (B)\n",
    "                # print (\"sigma_test_xi\", sigma_test_xi)\n",
    "                # new_target += torch.randn(new_target.size(0), device=device) * sigma_test_xi # shape (B, 1) \n",
    "                # print (\"new_target\", new_target, \"sigma_test_xi\", sigma_test_xi )\n",
    "                target[:, -1, 0] = new_target\n",
    "                \n",
    "            elif coarse_graining == \"shrink_norm\": \n",
    "                true_beta = _true_beta.squeeze(2) \n",
    "                x_test_visible = seq[:, -1, :].squeeze(1)\n",
    "                # test beta is beta but with smaller norm\n",
    "                test_beta_visible = true_beta * (D_visible) # shape (B, D) \n",
    "                # print (\"test_beta_visible\", test_beta_visible.unsqueeze(1).shape, \"x_test_visible\", x_test_visible.unsqueeze(2).shape)\n",
    "                new_target = torch.matmul(x_test_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)).squeeze(2) \n",
    "                # print (\"new_target\", new_target.shape, \"args.sigma_xi\", args.sigma_xi)\n",
    "                # new_target += args.sigma_xi * torch.randn_like(new_target, device=device) # shape (B, 1)\n",
    "                new_target = new_target.squeeze(1) \n",
    "                \n",
    "                target[:, -1, 0] = new_target \n",
    "                \n",
    "            elif coarse_graining == \"vary_cos_alignment\": \n",
    "                # get x, target, beta_incontext\n",
    "                x = seq \n",
    "                beta_incontext = torch.ones_like(_true_beta)  \n",
    "                # normalize beta_incontext \n",
    "                beta_incontext = beta_incontext / torch.linalg.norm(beta_incontext, dim=1).unsqueeze(1) * torch.linalg.norm(_true_beta, dim=1).unsqueeze(1)\n",
    "                training_beta_incontext = beta_incontext.detach().clone()\n",
    "                # compute target y = x @ beta + noise\n",
    "                target = torch.matmul(x, beta_incontext) \n",
    "                noise = torch.randn_like(target) * args.sigma_xi\n",
    "                target += noise\n",
    "                \n",
    "                # concept shift: vary the cosine of the high variance features and compute new target\n",
    "                num_features_flipped = int(x.shape[-1] * (1-D_visible)) # if D_visible is small, then every feature is flipped\n",
    "                beta_incontext[:, :num_features_flipped, :] = -beta_incontext[:, :num_features_flipped, :] \n",
    "                new_target = torch.matmul(x[:, -1, :].unsqueeze(1), beta_incontext).squeeze(2).squeeze(1) \n",
    "                # new_target += args.sigma_xi * torch.randn_like(new_target, device=device) # shape (B, 1)\n",
    "                target[:, -1, -1] = new_target \n",
    "                \n",
    "                # Compute cos theta\n",
    "                cos_theta_val = cos_theta(training_beta_incontext.squeeze(-1), beta_incontext.squeeze(-1))\n",
    "                cos_theta_avg.update(cos_theta_val.mean().item(), B)\n",
    "                \n",
    "                \n",
    "                \n",
    "            elif coarse_graining == \"aniso_highvariance_shrink_k\":\n",
    "                # get x, target, beta_incontext\n",
    "                x = seq \n",
    "                high_variance_features_id = int(args.rho_minus * x.shape[-1])\n",
    "                beta_incontext = _true_beta # torch.randn_like(_true_beta) * scale\n",
    "\n",
    "                # balance signal fraction of the low variance features\n",
    "                # beta_incontext[:,high_variance_features_id:,:] = beta_incontext[:,high_variance_features_id:,:] * (np.sqrt(10))\n",
    "                # normalize beta_incontext \n",
    "                \n",
    "                # beta_incontext = beta_incontext / torch.linalg.norm(beta_incontext, dim=1).unsqueeze(1) * torch.linalg.norm(_true_beta, dim=1).unsqueeze(1)\n",
    "                \n",
    "                # compute target y = x @ beta + noise\n",
    "\n",
    "                 \n",
    "                target = torch.matmul(x, beta_incontext) \n",
    "                args.sigma_xi = 1.0 # anisotropic case has noise std of 1.0\n",
    "                noise = torch.randn_like(target) * args.sigma_xi\n",
    "                target += noise\n",
    "                \n",
    "                # concept shift: shrink the high variance features and compute new target\n",
    "                test_beta_visible = beta_incontext.detach().clone()\n",
    "                test_beta_visible[:, :high_variance_features_id, :] = test_beta_visible[:, :high_variance_features_id, :] * (D_visible) \n",
    "                new_target = torch.matmul(x[:, -1, :].unsqueeze(1), test_beta_visible).squeeze(2).squeeze(1) \n",
    "                # print (\"x\", x.shape) \n",
    "                # print (\"target\", target.shape, \"new_target\", new_target.shape)\n",
    "                # print (\"beta_incontext\", beta_incontext.shape) \n",
    "                \n",
    "                # new_target += args.sigma_xi * torch.randn_like(new_target, device=device) # shape (B, 1)\n",
    "                 \n",
    "                target[:, -1, 0] = new_target  \n",
    "                SNR =  torch.matmul(beta_incontext.permute(0,2,1), (input_covariance_L * input_covariance_L * beta_incontext ) ) / (args.sigma_xi**2).reshape(-1)\n",
    "                # print(\"SNR\", SNR.shape)\n",
    "            elif coarse_graining == \"aniso_lowvariance_shrink_k\":\n",
    "                # get x, target, beta_incontext\n",
    "                x = seq \n",
    "                high_variance_features_id = int(args.rho_minus * x.shape[-1])\n",
    "                beta_incontext = _true_beta # torch.randn_like(_true_beta) * scale\n",
    "\n",
    "                # balance signal fraction of the low variance features\n",
    "                # beta_incontext[:,high_variance_features_id:,:] = beta_incontext[:,high_variance_features_id:,:] * (np.sqrt(10))\n",
    "                # normalize beta_incontext \n",
    "                \n",
    "                # beta_incontext = beta_incontext / torch.linalg.norm(beta_incontext, dim=1).unsqueeze(1) * torch.linalg.norm(_true_beta, dim=1).unsqueeze(1)\n",
    "                \n",
    "                # compute target y = x @ beta + noise\n",
    "                target = torch.matmul(x, beta_incontext) \n",
    "                args.sigma_xi = 1.0 # anisotropic case has noise std of 1.0\n",
    "                noise = torch.randn_like(target) * args.sigma_xi\n",
    "                target += noise\n",
    "                \n",
    "                # concept shift: shrink the low variance features and compute new target\n",
    "                test_beta_visible = beta_incontext.detach().clone()\n",
    "                test_beta_visible[:, high_variance_features_id:, :] = test_beta_visible[:, high_variance_features_id:, :] * (D_visible) \n",
    "                # print (\"beta_incontext\", beta_incontext[0].flatten())\n",
    "                # print (\"test_beta_visible\", test_beta_visible.shape)\n",
    "                # plt.hist(test_beta_visible[:,:16].flatten().cpu().numpy(), bins = 25, alpha = 0.5, label = \"positive\")\n",
    "                # plt.hist(test_beta_visible[:,16:].flatten().cpu().numpy(), bins = 25, alpha = 0.5, label = \"negative\")\n",
    "                # plt.title(\"test_beta_visible, positive and negative components\")\n",
    "                # plt.legend()\n",
    "                # plt.show()\n",
    "                # print (\"x\", x[0, -1, :] )\n",
    "                new_target = torch.matmul(x[:, -1, :].unsqueeze(1), test_beta_visible).squeeze(2).squeeze(1) \n",
    "                # new_target += args.sigma_xi * torch.randn_like(new_target, device=device) # shape (B, 1)\n",
    "                target[:, -1, 0] = new_target \n",
    "                # print (\"input_covariance_L\", input_covariance_L * input_covariance_L)\n",
    "                SNR =  torch.matmul(beta_incontext.permute(0,2,1), (input_covariance_L.reshape(1,-1,1) * input_covariance_L.reshape(1,-1,1) * beta_incontext ) ).reshape(-1)  / (args.sigma_xi**2) \n",
    "                # print (\"SNR\", (beta_incontext.permute(0,2,1)).shape, (input_covariance_L*input_covariance_L*beta_incontext).shape)\n",
    "                # print (\"SNR\", torch.allclose(SNR, torch.ones_like(SNR)))\n",
    "                \n",
    "            elif coarse_graining == \"aniso_highvariance_vary_cos\":\n",
    "                # get x, target, beta_incontext\n",
    "                x = seq \n",
    "                high_variance_features_id = int(args.rho_minus * x.shape[-1])\n",
    "                beta_incontext = torch.ones_like(_true_beta)\n",
    "\n",
    "                # balance signal fraction of the low variance features\n",
    "                beta_incontext[:,high_variance_features_id:,:] = beta_incontext[:,high_variance_features_id:,:] * (np.sqrt(10))\n",
    "                # normalize beta_incontext \n",
    "                beta_incontext = beta_incontext / torch.linalg.norm(beta_incontext, dim=1).unsqueeze(1)  * torch.linalg.norm(_true_beta, dim=1).unsqueeze(1)\n",
    "                \n",
    "                # compute target y = x @ beta + noise\n",
    "                target = torch.matmul(x, beta_incontext) \n",
    "                args.sigma_xi = 1.0 # anisotropic case has noise std of 1.0\n",
    "                noise = torch.randn_like(target) * args.sigma_xi\n",
    "                target += noise\n",
    "                \n",
    "                # concept shift: vary the cosine of the high variance features and compute new target\n",
    "                num_features_flipped = int(high_variance_features_id * (1-D_visible)) # if D_visible is small, then every feature is flipped\n",
    "                test_beta_visible = beta_incontext.detach().clone()\n",
    "                test_beta_visible[:, :num_features_flipped, :] = -test_beta_visible[:, :num_features_flipped, :] \n",
    "                new_target = torch.matmul(x[:, -1, :].unsqueeze(1), test_beta_visible).squeeze(2).squeeze(1) \n",
    "                # new_target += args.sigma_xi * torch.randn_like(new_target, device=device) # shape (B, 1)\n",
    "                target[:, -1, 0] = new_target \n",
    "\n",
    "                cos_theta_val = cos_theta(test_beta_visible.squeeze(-1), beta_incontext.squeeze(-1))\n",
    "                cos_theta_avg.update(cos_theta_val.mean().item(), B)\n",
    "                 \n",
    "                \n",
    "            elif coarse_graining == \"aniso_lowvariance_vary_cos\":\n",
    "                # get x, target, beta_incontext\n",
    "                x = seq \n",
    "                high_variance_features_id = int(args.rho_minus * x.shape[-1])\n",
    "                beta_incontext = torch.ones_like(_true_beta)\n",
    "\n",
    "                # balance signal fraction of the low variance features\n",
    "                beta_incontext[:,high_variance_features_id:,:] = beta_incontext[:,high_variance_features_id:,:] * (np.sqrt(10))\n",
    "                # normalize beta_incontext \n",
    "                beta_incontext = beta_incontext / torch.linalg.norm(beta_incontext, dim=1).unsqueeze(1) * torch.linalg.norm(_true_beta, dim=1).unsqueeze(1)\n",
    "                \n",
    "                # compute target y = x @ beta + noise\n",
    "                target = torch.matmul(x, beta_incontext) \n",
    "                args.sigma_xi = 1.0 # anisotropic case has noise std of 1.0\n",
    "                noise = torch.randn_like(target) * args.sigma_xi\n",
    "                target += noise\n",
    "                \n",
    "                # concept shift: vary the cosine of the low variance features and compute new target\n",
    "                num_features_flipped = int(high_variance_features_id * (1-D_visible)) # if D_visible is small, then every feature is flipped\n",
    "                test_beta_visible = beta_incontext.detach().clone()\n",
    "                # print (\"test_beta_visible\", test_beta_visible.shape, \"high_variance_features_id\", high_variance_features_id, \"num_features_flipped\", num_features_flipped)\n",
    "                test_beta_visible[:, (high_variance_features_id):(high_variance_features_id+num_features_flipped), :] = -test_beta_visible[:, (high_variance_features_id):(high_variance_features_id+num_features_flipped), :]\n",
    "                new_target = torch.matmul(x[:, -1, :].unsqueeze(1), test_beta_visible).squeeze(2).squeeze(1)\n",
    "                # new_target += args.sigma_xi * torch.randn_like(new_target, device=device) # shape (B, 1)\n",
    "                target[:, -1, 0] = new_target \n",
    "                # SNR =  torch.matmul(beta_incontext.permute(0,2,1), (input_covariance_L * input_covariance_L * beta_incontext ) ) / (args.sigma_xi**2)\n",
    "\n",
    "                # Compute cos theta\n",
    "                cos_theta_val = cos_theta(test_beta_visible.squeeze(-1), beta_incontext.squeeze(-1))\n",
    "                cos_theta_avg.update(cos_theta_val.mean().item(), B)\n",
    "            elif coarse_graining == \"standard\": \n",
    "                SNR = torch.matmul(_true_beta.permute(0,2,1), (input_covariance_L * input_covariance_L * _true_beta ) ) / (args.sigma_xi**2)\n",
    "            output = model(seq, target) \n",
    "            # print (\"seq\", seq.shape, \"target\", target.shape, \"output\", output.shape )\n",
    "            preds = output[:, ::2, :]\n",
    "            # distance to ridge_preds \n",
    "            # if coarse_graining == \"standard\":\n",
    "            # gamma = 1000\n",
    "            # lam = gamma  \n",
    "            # ridge_preds = get_ridge_preds_seq(seq, target, lam) # shape: (B, N-1, 1)\n",
    "            # ridge_loss = (ridge_preds - target[:, 1:, :]).pow(2).mean(dim=0)\n",
    "            # ridge_losses[\"ridge\"].extend(ridge_loss.cpu().numpy().flatten())\n",
    "            # ridge_losses[\"pos\"].extend(np.arange(0, N-1))\n",
    "            # dist_to_ridge = (preds[:,:-1, :] - ridge_preds).pow(2).mean(dim=0)\n",
    "            true_ys.extend(target[:, -1, :].flatten().cpu().numpy())\n",
    "            predicted_ys.extend(preds[:, -1, :].flatten().cpu().numpy())\n",
    "            training_beta_norm.update(torch.linalg.norm(_true_beta, dim=1).mean().item(), B)\n",
    "            risk = (preds - target).pow(2).mean(dim=0) # mean across batches\n",
    "            # ridge_losses[\"risk\"].extend(risk[:(N-1)].cpu().numpy().flatten())\n",
    "            # print (\"ridge_loss\", ridge_loss.shape, \"risk\", risk.shape)\n",
    "            # print (\"output\", output[0].flatten())\n",
    "\n",
    "            # loss = (risk / training_beta_norm_sq).mean(dim=0) \n",
    "            loss = (risk )#.mean(dim=0) \n",
    "            # print(\"loss\", loss[99])\n",
    "            [test_losses[_].update(loss[_], target.size(0)) for _ in range(N)]\n",
    "\n",
    "            # acc1 = utils.accuracy(output, seq_target, topk=[1])\n",
    "            # test_top1[seq_len].update(acc1[0], target.size(0))\n",
    "            # acc1 = torch.mean(((output.squeeze(1) * (seq_target*2-1)) > 0).float()).item()\n",
    "            # test_top1[seq_len].update(acc1, target.size(0))\n",
    "    # analyze_dataset(xs, ys, bs, icl_test_loader, args)\n",
    "    # df = pd.DataFrame(ridge_losses) \n",
    "    # sns.lineplot(x=\"pos\", y=\"ridge\", data=df, label=\"ridge\")\n",
    "    # sns.lineplot(x=\"pos\", y=\"risk\", data=df, label=\"risk\")\n",
    "    # plt.legend()\n",
    "    # plt.title(rf\"Ridge Loss $\\gamma$= {gamma}\")\n",
    "    return test_losses, training_beta_norm, cos_theta_avg, true_ys, predicted_ys\n",
    "D_sum=r[\"args\"][\"D_sum\"]\n",
    "icl_test_loader, iwl_test_loader = get_data_loaders(args, 100, D_sum, args.data_scale, args.is_iso)\n",
    "Dvis=1.0\n",
    "icl_outdistribution_losses, training_beta_norm, cos_theta_avg, true_ys, predicted_ys  = validate_gradient_descent(icl_test_loader, model, args, Dvis, 100, criterion, device, coarse_graining=\"aniso_lowvariance_shrink_k\")\n",
    "icl_outdistribution_losses, training_beta_norm, cos_theta_avg, true_ys, predicted_ys = validate_gradient_descent(icl_test_loader, model, args, Dvis, 100, criterion, device, coarse_graining=\"standard\")\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# import matplotlib.pyplot as plt\n",
    "exp_name = f\"./analysis/{args.fileprefix}_coarsegraining_{args.coarse_graining}_{args.resume.split('/')[-1]}\"\n",
    "D_sum=r[\"args\"][\"D_sum\"]\n",
    "for len_context in np.linspace(1, args.len_context, 50).astype(int):\n",
    "    icl_test_loader, iwl_test_loader = get_data_loaders(args, len_context, D_sum, args.data_scale, args.is_iso)\n",
    "    for D_visible in args.perturbation_parameter_ranges:\n",
    "        \n",
    "        model.load_state_dict(r[\"model\"])\n",
    "        icl_outdistribution_losses, training_beta_norm, cos_theta_avg, true_ys, predicted_ys = validate_gradient_descent(icl_test_loader, model, args, D_visible, len_context, criterion, device, coarse_graining=args.coarse_graining)\n",
    "        icl_indistribution_losses, _, _, _, _ = validate_gradient_descent(icl_test_loader, model, args, D_visible, len_context, criterion, device, coarse_graining=\"standard\")\n",
    "        # icl_indistribution_losses = None\n",
    "        # iwl_indistribution_losses = validate_gradient_descent(iwl_test_loader, model, args, D_visible, len_context, criterion, device, coarse_graining=\"standard\")\n",
    "        # iwl_outdistribution_losses = validate_gradient_descent(iwl_test_loader, model, args, D_visible, len_context, criterion, device, coarse_graining=args.coarse_graining)\n",
    "        \n",
    "        \n",
    "\n",
    "        # save metrics\n",
    "        # print(\"output\",  torch.argsort(output, dim=-1), \"target\", target )\n",
    "        # print(\"Current average loss\", losses.avg, top1.avg, \"epoch\", epoch) \n",
    "        # seen_val_losses, seen_val_top1 = validate_gradient_descent(icl_loader, seen_projs_permutations_loader, model, args, criterion, device)\n",
    "        \n",
    "        # Compute unseen val loss\n",
    "        # unseen_val_losses, unseen_val_top1 = validate_gradient_descent(icl_loader, seen_projs_permutations_loader, model, args, criterion, device)\n",
    "        logs = {\n",
    "                \"len_context\": len_context,\n",
    "                \"D_visible\": D_visible,\n",
    "                \"true_ys\": true_ys,\n",
    "                \"predicted_ys\": predicted_ys,\n",
    "                # \"icl_indistribution_loss\": icl_indistribution_losses.avg,\n",
    "                # \"icl_outdistribution_loss\": icl_outdistribution_losses.avg,\n",
    "                # \"iwl_indistribution_loss\": iwl_indistribution_losses.avg,\n",
    "                # \"iwl_outdistribution_loss\": iwl_outdistribution_losses.avg,\n",
    "            }\n",
    "        for _ in range(len_context):\n",
    "            logs[f\"icl_indistribution_loss_{_}\"] = icl_indistribution_losses[_].avg\n",
    "            logs[f\"icl_outdistribution_loss_{_}\"] = icl_outdistribution_losses[_].avg\n",
    "            # logs[f\"training_beta_norm\"] = training_beta_norm.avg\n",
    "        # if \"vary_cos\" in args.coarse_graining:\n",
    "            # logs[f\"D_visible\"] = cos_theta_avg.avg\n",
    "            # logs[f\"iwl_indistribution_loss_{_}\"] = iwl_indistribution_losses[_].avg\n",
    "            # logs[f\"iwl_outdistribution_loss_{_}\"] = iwl_outdistribution_losses[_].avg\n",
    "        record[\"logs\"].append(logs)\n",
    "\n",
    "    with open(exp_name, \"wb\") as f:\n",
    "        pickle.dump(record, f)\n",
    "    # # print(logs) \n",
    "    # if args.wandb_log:\n",
    "    #     wandb.log(logs)\n",
    "    # else:\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "#     if epoch % 10 == 0 and args.wandb_log != True:\n",
    "#         record[\"model\"] = copy.deepcopy(model.state_dict())  \n",
    "#         with open(exp_name, \"wb\") as f:\n",
    "#             pickle.dump(record, f)\n",
    "\n",
    "# if args.wandb_log != True:\n",
    "with open(exp_name, \"wb\") as f:\n",
    "    pickle.dump(record, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (l2l)",
   "language": "python",
   "name": "l2l"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
