{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset\n",
    "import attention\n",
    "# import webdataset as wds\n",
    "\n",
    "import datetime\n",
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import einops\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import wandb \n",
    "import sys \n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='GMM L2L Training with Sequence Model')\n",
    "parser.add_argument('--data', metavar='DIR', nargs='?', default='./data',\n",
    "                    help='path to dataset (default: imagenet)')\n",
    "parser.add_argument('--cache', default='./cache',\n",
    "                    help='path to cached files (e.g. for previous random weights)')\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_group_name\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument('--seed', default=None, type=int,\n",
    "                    help='seed for initializing training.')\n",
    "parser.add_argument('--epochs', default=90, type=int,  \n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')                         \n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--optimizer', default='SGD', type=str, \n",
    "                    choices = ['SGD', 'Adam'],\n",
    "                    help='optimizer')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--wd', '--weight-decay', default=1e-5, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)',\n",
    "                    dest='weight_decay')\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='mlp',\n",
    "                    help='model architecture (default: mlp)')\n",
    "parser.add_argument('--num_hidden_features', default=1, type=int,\n",
    "                    help='num_hidden_features')\n",
    "parser.add_argument('--len_context', default=1, type=int,\n",
    "                    help='number of in-context images in sequence')\n",
    "parser.add_argument('--SLURM_ARRAY_TASK_ID', default=1, type=int,\n",
    "                    help='SLURM_ARRAY_TASK_ID')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')  \n",
    "parser.add_argument('--D', default=63, type=int, \n",
    "                    help='number of features in each input')\n",
    "parser.add_argument('--K', default=1, type=int, \n",
    "                    help='number of tasks')\n",
    "parser.add_argument('--coarse_graining', default=\"abstop\", type=str,\n",
    "                    help='coarse graining method')\n",
    "parser.add_argument(\n",
    "            '--fileprefix', \n",
    "            default=\"\",\n",
    "            type=str, \n",
    "            action='store') \n",
    "    \n",
    "\n",
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    arch = \"pytorch_transformer\"\n",
    "    # arch = \"transformer\"\n",
    "    jupyter_args = f\"--data ./cache --fileprefix transformer1layer_lr_0.01_no_posenc --position_encoding False --SLURM_ARRAY_TASK_ID 5 --batch-size 128 --optimizer SGD --lr 0.01 --wd 1e-10 --epsilon 0.0 --burstiness 1 --init_scale_qkv 1.0 --scale_target 1.0 --is_all_tasks_seen True --is_equalize_classes True --L 2  --epochs 1000 --arch causal_transformer_embed --is_layer_norm True --num_hidden_features 512 --is_train_random_length_seqs False --len_context 100 --wandb_log --wandb_project l2l --is_temperature_fixed False --wandb_group_name gmm_sep19_exp5_equalize0s1\"\n",
    "    \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 \n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# assert args.K % args.L == 0, \"K must be divisible by L\"\n",
    "if args.seed is None:\n",
    "    args.seed = np.random.randint(0, 10000000)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Local Rank for distributed training\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)\n",
    "print(\"args:\\n\",vars(args))\n",
    "# setup weights and biases (optional)\n",
    "if local_rank==0 and args.wandb_log: # only use main process for wandb logging\n",
    "    print(f\"wandb {args.wandb_project} run\")\n",
    "    wandb.login(host='https://stability.wandb.io') # need to configure wandb environment beforehand\n",
    "    wandb_model_name = f\"{args.fileprefix}_K_{args.K}_D_{args.D}_L_{args.len_context}_hidden_{args.num_hidden_features}_coarse_{args.coarse_graining}\"\n",
    "    wandb_config = vars(args)\n",
    "    \n",
    "    print(\"wandb_id:\",wandb_model_name)\n",
    "    wandb.init(\n",
    "        project=args.wandb_project,\n",
    "        name=wandb_model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "        group=args.wandb_group_name\n",
    "    )\n",
    "    wandb.config.local_file_dir = wandb.run.dir \n",
    "else:\n",
    "    record = {\n",
    "        \"args\": vars(args),\n",
    "        \"logs\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, K, D,  \n",
    "                 len_context = 1,\n",
    "                len_data = 60000):\n",
    "\n",
    "        # if K < 40000:\n",
    "        self.len_context = len_context\n",
    "        self.D = D\n",
    "    \n",
    "        # x = rng.standard_normal((K, D)) * (1.0 / np.sqrt(D)) # shape: (K, D) \n",
    "        true_betas = torch.randn((K, D)) * (1.0 / np.sqrt(D)) # shape: (K, D)\n",
    "        self.true_betas = true_betas\n",
    "        self.K = K \n",
    "        self.D = D\n",
    "        self.len_data = len_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, task: int):\n",
    "        task_ind = torch.randint(0, self.K, (1,)).item()\n",
    "        beta_incontext = self.true_betas[task_ind].unsqueeze(1) # shape: (D, 1)\n",
    "        x = torch.randn((self.len_context, self.D)) * (1.0 / np.sqrt(self.D)) # shape: (self.len_context, D) \n",
    "        y = torch.matmul(x, beta_incontext)\n",
    "                        #  .T) # shape: (self.len_context, 1) \n",
    "        # concat x and y \n",
    "        samples = torch.cat([x, y], axis = 1) # shape: (self.len_context, D+1)\n",
    "        xtest = torch.randn((1, self.D)) * (1.0 / np.sqrt(self.D)) # test x\n",
    "        ytest = torch.matmul(xtest, beta_incontext.T) \n",
    "        test_samples = torch.cat([xtest, 0], axis = 1) # test samples, shape (1, D+1)\n",
    "        samples = torch.cat([samples, test_samples], axis = 0) # shape: (self.len_context+1, D+1)\n",
    "          \n",
    "        return samples.type(torch.float32), ytest.type(torch.float32), beta_incontext.type(torch.float32)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(attention)\n",
    "# define the model, optimizer, and scheduler, and criterion\n",
    "if args.arch == \"causal_transformer_embed\":\n",
    "    nheads = 1 # np.clip(args.num_hidden_features // 8, 1, 8)\n",
    "    model = attention.CausalTransformerOneMinusOneEmbed(x_dim=args.D+1,                   \n",
    "                                  mlp_dim=args.num_hidden_features\n",
    "                                  ).to(device)\n",
    "\n",
    "if args.optimizer == 'SGD': \n",
    "    optimizer = torch.optim.SGD(model.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "else:\n",
    "    raise ValueError(\"optimizer not recognized\")\n",
    "\n",
    "criterion = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.batch_size}\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': args.workers,\n",
    "                    \"shuffle\": True,\n",
    "                    'pin_memory': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "train_dataset = Sequence(K=args.K, D=args.D, len_context=args.len_context) \n",
    "# iwl_dataset = Sequence(K=args.K, D=args.D, len_context=args.len_context, len_data = 1000)\n",
    "# iwl_dataset.true_betas = train_dataset.true_betas\n",
    "icl_test_dataset = Sequence(K=args.K, D=args.D, len_context=args.len_context, len_data = 1000)\n",
    "\n",
    "iwl_test_dataset = Sequence(K=args.K, D=args.D, len_context=args.len_context, len_data = 1000)\n",
    "iwl_test_dataset.true_betas = train_dataset.true_betas\n",
    "\n",
    "train_sampler = None\n",
    "val_sampler = None \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            sampler=train_sampler, \n",
    "                                            **train_kwargs) \n",
    "icl_test_loader = torch.utils.data.DataLoader(icl_test_dataset,\n",
    "                                            sampler=val_sampler,\n",
    "                                            **test_kwargs)  \n",
    "iwl_test_loader = torch.utils.data.DataLoader(iwl_test_dataset,\n",
    "                                            sampler=val_sampler,\n",
    "                                            **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gradient_descent(epoch, val_loader, model, args, criterion, device, coarse_graining=\"standard\"):\n",
    "    # seq_lens = list(range(1, args.len_context+1, 5)) \n",
    "   \n",
    "    test_losses = utils.AverageMeter('Loss', ':.4e') \n",
    "    \n",
    "    model.eval() # switch to eval mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (seq, target, true_beta) in enumerate(val_loader):\n",
    "            seq, target = seq.to(device), target.to(device)\n",
    "            if coarse_graining == \"absbot\":\n",
    "                # true_beta: shape (B, D)\n",
    "                argsort_beta_visible = torch.argsort(torch.abs(true_beta), dim=-1)[:, :args.D_visible] # sort each row of true_beta by absolute value, shape (B, D_visible)\n",
    "                test_beta_visible = true_beta[argsort_beta_visible] # take bottom D_visible betas, shape (B, D_visible)\n",
    "                sigma_test_xi = torch.pow(args.sigma_xi ** 2 + torch.matmul(true_beta.unsqueeze(1), true_beta.unsqueeze(2)) \\\n",
    "                                        - torch.matmul(test_beta_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)), 0.5)\n",
    "                x_test_visible = seq[:, -1, :-1].squeeze(1)[argsort_beta_visible] # shape (B, D_visible)\n",
    "                # target = x_test_visible  @ test_beta_visible + np.random.randn(N_test) * sigma_test_xi\n",
    "                target = torch.matmul(x_test_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)).squeeze(2) + torch.randn_like(target) * sigma_test_xi # shape (B, 1)\n",
    "            elif coarse_graining == \"abstop\":\n",
    "                argsort_beta_visible = torch.argsort(torch.abs(true_beta), dim=-1)[:, -args.D_visible:] # sort each row of true_beta by absolute value, shape (B, D_visible)\n",
    "                test_beta_visible = true_beta[argsort_beta_visible] # take top D_visible betas, shape (B, D_visible) \n",
    "                sigma_test_xi = torch.pow(args.sigma_xi ** 2 + torch.matmul(true_beta.unsqueeze(1), true_beta.unsqueeze(2)) \\\n",
    "                                        - torch.matmul(test_beta_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)), 0.5)\n",
    "                x_test_visible = seq[:, -1, :-1].squeeze(1)[argsort_beta_visible] # shape (B, D_visible)\n",
    "                # target = x_test_visible  @ test_beta_visible + np.random.randn(N_test) * sigma_test_xi\n",
    "                target = torch.matmul(x_test_visible.unsqueeze(1), test_beta_visible.unsqueeze(2)).squeeze(2) + torch.randn_like(target) * sigma_test_xi # shape (B, 1)\n",
    "            elif coarse_graining == \"standard\":\n",
    "                pass\n",
    "            output = model(seq)  \n",
    "            loss = criterion(output, target)\n",
    "            test_losses.update(loss.item(), target.size(0))\n",
    "            # acc1 = utils.accuracy(output, seq_target, topk=[1])\n",
    "            # test_top1[seq_len].update(acc1[0], target.size(0))\n",
    "            # acc1 = torch.mean(((output.squeeze(1) * (seq_target*2-1)) > 0).float()).item()\n",
    "            # test_top1[seq_len].update(acc1, target.size(0))\n",
    "\n",
    "    return test_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# import matplotlib.pyplot as plt\n",
    "exp_name = f\"./cache/{args.wandb_group_name}_K_{args.K}_{time.time()}.pkl\"\n",
    "for epoch in range(args.epochs):\n",
    "    model.train() # switch to train mode\n",
    "    losses = utils.AverageMeter('Loss', ':.4e')\n",
    "    top1 = utils.AverageMeter('Acc@1', ':6.2f')\n",
    "\n",
    " \n",
    "    for i, (seq, target, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        seq, target = seq.to(device), target.to(device)\n",
    "        output = model(seq) \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), target.size(0)) \n",
    "        # acc1 = utils.accuracy(output, (seq_target), topk=[1])\n",
    "        # print (\"output\", output.shape, output[0], seq_target[0], loss, acc1, model.temperature)\n",
    "        # top1.update(acc1[0], target.size(0))\n",
    "        # acc1 = torch.mean(((output.squeeze(1) * (seq_target*2-1)) > 0).float()).item()\n",
    "        # top1.update(acc1, target.size(0))\n",
    " \n",
    "    # step scheduler\n",
    "    # scheduler.step()\n",
    "\n",
    "    # save metrics\n",
    "    # print(\"output\",  torch.argsort(output, dim=-1), \"target\", target )\n",
    "    # print(\"Current average loss\", losses.avg, top1.avg, \"epoch\", epoch) \n",
    "    # seen_val_losses, seen_val_top1 = validate_gradient_descent(icl_loader, seen_projs_permutations_loader, model, args, criterion, device)\n",
    "    icl_indistribution_losses = validate_gradient_descent(epoch, icl_test_loader, model, args, criterion, device, coarse_graining=\"standard\")\n",
    "    icl_outdistribution_losses = validate_gradient_descent(epoch, icl_test_loader, model, args, criterion, device, coarse_graining=args.coarse_graining)\n",
    "    iwl_indistribution_losses = validate_gradient_descent(epoch, iwl_test_loader, model, args, criterion, device, coarse_graining=\"standard\")\n",
    "    iwl_outdistribution_losses = validate_gradient_descent(epoch, iwl_test_loader, model, args, criterion, device, coarse_graining=args.coarse_graining)\n",
    "    \n",
    "    # Compute unseen val loss\n",
    "    # unseen_val_losses, unseen_val_top1 = validate_gradient_descent(icl_loader, seen_projs_permutations_loader, model, args, criterion, device)\n",
    "    logs = {\n",
    "            \"train_loss\": losses.avg,\n",
    "            \"epoch\": epoch,\n",
    "            \"icl_indistribution_loss\": icl_indistribution_losses.avg,\n",
    "            \"icl_outdistribution_loss\": icl_outdistribution_losses.avg,\n",
    "            \"iwl_indistribution_loss\": iwl_indistribution_losses.avg,\n",
    "            \"iwl_outdistribution_loss\": iwl_outdistribution_losses.avg,\n",
    "        }\n",
    "    \n",
    "    print(logs) \n",
    "    if args.wandb_log:\n",
    "        wandb.log(logs)\n",
    "    else:\n",
    "        record[\"logs\"].append(logs)\n",
    "    \n",
    " \n",
    "    # save phi_xt_list_epoch \n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    #     if args.arch == \"phenomenologicalcausal_transformer\":\n",
    "    #         with open(f\"./cache/{args.wandb_group_name}_betastd_{args.beta_std}_{time.time()}.pkl\", \"wb\") as f:\n",
    "    #             pickle.dump(record, f)\n",
    "    #     else:\n",
    "    #         with open(exp_name, \"wb\") as f:\n",
    "    #             pickle.dump(record, f)\n",
    "  \n",
    "if args.wandb_log != True:\n",
    "    with open(exp_name, \"wb\") as f:\n",
    "        pickle.dump(record, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
