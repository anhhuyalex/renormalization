{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d721f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to maximize the dot product\n",
    "# Step 1: Add the position encoding (content + position)\n",
    "# INFONCE: A and B are complete unconstrained, instead of optimizing over pi\n",
    "# Check: Does the model localize the right memory\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "class DAM(nn.Module):\n",
    "    def __init__(self, N, H, M, eta=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            N: Sequence length (bits).\n",
    "            H: Hidden dimension (number of heads).\n",
    "            M: Memory capacity (number of sequences).\n",
    "            eta: Inverse temperature parameter.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.eta = eta\n",
    "\n",
    "        # A(n) parameters: (N, H, N). We will mask unused parts.\n",
    "        # We need independent parameters for each length n.\n",
    "        # For length n (predicting n-th bit, 0-indexed), we use inputs 0..n-1.\n",
    "        # So we have separate weights for each n.\n",
    "        self.A_logits = nn.Parameter(torch.rand(N, H, N) * 0.02) # small positive weights\n",
    "\n",
    "        # B parameters: (H, N).\n",
    "        self.B_logits = nn.Parameter(torch.rand(H, N) * 0.02) # small positive weights\n",
    "\n",
    "        # Memory state\n",
    "        # We store binary sequences\n",
    "        self.memory = torch.zeros(0, N)\n",
    "        self.is_memory_empty = True\n",
    "\n",
    "    def update_memory(self, sequences):\n",
    "        \"\"\"\n",
    "        Updates the DAM with new sequences.\n",
    "        The memory is a queue so we only keep the last M sequences.\n",
    "        Args:\n",
    "            sequences: (Batch, N) tensor of binary sequences (-1, +1).\n",
    "        \"\"\"\n",
    "        new_memory = torch.cat([self.memory, sequences.detach().to(self.memory.device)], dim=0)\n",
    "\n",
    "        if new_memory.shape[0] > self.M:\n",
    "            new_memory = new_memory[-self.M:]\n",
    "\n",
    "        self.memory = new_memory\n",
    "        self.is_memory_empty = False\n",
    "\n",
    "    def get_A(self, n):\n",
    "        \"\"\"\n",
    "        Returns normalized A(n) matrix of shape (H, n).\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return torch.zeros(self.H, 0, device=self.A_logits.device)\n",
    "\n",
    "        # Select params for predictng n+1 th bit (index n).\n",
    "        # We use slice 0:n because we attend to first n bits.\n",
    "        logits = self.A_logits[n, :, :n] # (H, n)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def get_B(self):\n",
    "        \"\"\"\n",
    "        Returns normalized B matrix of shape (H, N).\n",
    "        \"\"\"\n",
    "        return F.softmax(self.B_logits, dim=1)\n",
    "\n",
    "    def forward_step(self, zeta, n):\n",
    "        \"\"\"\n",
    "        Predict probability of (n+1)-th bit (index n) being +1.\n",
    "\n",
    "        Args:\n",
    "            zeta: (Batch, N) full sequences (we only peek up to n).\n",
    "            n: int, current length (0 to N-1). We verify up to zeta[:, :n].\n",
    "               We want to predict zeta[:, n].\n",
    "\n",
    "        Returns:\n",
    "            probs: (Batch,) probability that next bit is +1.\n",
    "        \"\"\"\n",
    "        B_val = zeta.shape[0] # batch size\n",
    "\n",
    "        # handle empty memory\n",
    "        if self.is_memory_empty: # shape: (M, N) where M = memory capacity, N = sequence length\n",
    "            return torch.full((B_val,), 0.5, device=zeta.device)\n",
    "\n",
    "        A_n = self.get_A(n) # shape: (H, n)\n",
    "        # context is the input sequence up to position n (exclusive)\n",
    "        context = zeta[:, :n] # shape: (Batch, n)\n",
    "        # hat_phi = sum A_i * zeta_i.\n",
    "        hat_phi = torch.einsum('bi,hi->bh', context, A_n)\n",
    "        \n",
    "\n",
    "        # 2. Compute phi_mu (M, H)\n",
    "        # phi_mu = B chi_mu\n",
    "        B_mat = self.get_B() # (H, N)\n",
    "        if n == 5:\n",
    "        #     print(\"A_n\", A_n[0].flatten(), A_n.shape)\n",
    "        #     print(\"context: \", context[0,:10], \"hat_phi: \", hat_phi[0,:10])\n",
    "            print(\"B_mat\", )\n",
    "        chi = self.memory # (M, N)\n",
    "        phi_mu = torch.einsum('hn,mn->mh', B_mat, chi) # (M, H)\n",
    "\n",
    "        # 3. Retrieval probabilities\n",
    "        # score: (Batch, M)\n",
    "        # score = eta * hat_phi^T phi_mu\n",
    "        score = self.eta * torch.einsum('bh,mh->bm', hat_phi, phi_mu)\n",
    "        pi = F.softmax(score, dim=1) # (Batch, M)\n",
    "        # print(\"n: \", n, \"pi: \", pi[0])\n",
    "        # 4. Predict next bit\n",
    "        # We want prob that (n)-th bit is +1.\n",
    "        # memory_bits: (M,)\n",
    "        memory_bits_at_n = self.memory[:, n] # +1 or -1\n",
    "\n",
    "        # We want sum of pi where memory bit is +1.\n",
    "        # Indicator: (memory_bits_at_n == 1).float()\n",
    "        plus_one_mask = (memory_bits_at_n > 0).float()\n",
    "        # prob = sum(pi * mask)\n",
    "        prob_plus_one = torch.sum(pi * plus_one_mask.unsqueeze(0), dim=1)\n",
    "\n",
    "        # print(\"prob_plus_one: \", prob_plus_one)\n",
    "        return prob_plus_one\n",
    "\n",
    "    def train_batch(self, sequences, optimizer):\n",
    "        \"\"\"\n",
    "        Computes the BCE loss averaged over all positions.\n",
    "        Args:\n",
    "             sequences: (Batch, N)\n",
    "        Returns:\n",
    "             loss: scalar\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        if self.is_memory_empty:\n",
    "            return total_loss, total_accuracy\n",
    "        for n in range(1,self.N):\n",
    "            if self.is_memory_empty == False:\n",
    "                optimizer.zero_grad()\n",
    "            # Predict n-th bit (0-indexed) using 0..n-1 history\n",
    "            prob_plus_one = self.forward_step(sequences, n) # shape: (Batch,)\n",
    "            # print(\"sequences: \", sequences)\n",
    "            # Target\n",
    "            target = sequences[:, n] # -1 or +1, shape: (Batch,)\n",
    "            # Convert target to 0/1 for BCE\n",
    "            target_01 = (target > 0)\n",
    "\n",
    "            # numeric stability\n",
    "            prob_plus_one = torch.clamp(prob_plus_one, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "            loss_n = F.binary_cross_entropy(prob_plus_one, target_01.float())\n",
    "            if self.is_memory_empty == False:\n",
    "                loss_n.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss_n\n",
    "\n",
    "            # accuracy\n",
    "            accuracy_n = ((prob_plus_one > 0.5) == target_01)\n",
    "            total_accuracy += accuracy_n.float().mean()\n",
    "        print(\"avg loss: \", total_loss / self.N, \"avg accuracy: \", total_accuracy / self.N)\n",
    "\n",
    "        return total_loss / self.N, total_accuracy / self.N\n",
    "\n",
    "def initialize_record(args):\n",
    "    record = {\n",
    "        \"args\": args,\n",
    "        \"logs\": [],\n",
    "    }\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DAM...\n",
      "Memory initialized with shape: torch.Size([10000, 100])\n",
      "A_n tensor([0.1988, 0.1992, 0.2019, 0.1991, 0.2010], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1.,  1., -1.]) hat_phi:  tensor([-0.1980, -0.2009, -0.2001, -0.2015, -0.2007, -0.2016, -0.2016, -0.2038,\n",
      "        -0.2007, -0.2011], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--eta ETA] [--lr LR] [--N N] [--H H]\n",
      "                             [--M M] [--K K] [--BATCH_SIZE BATCH_SIZE]\n",
      "                             [--NUM_STEPS NUM_STEPS]\n",
      "                             [--NUM_ITERS_PER_LOG NUM_ITERS_PER_LOG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /mnt/cup/people/qanguyen/.local/share/jupyter/runtime/kernel-30c4c5c2-e381-4e5a-b671-82a0b1901d4c.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss:  tensor(6.8608, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.4934)\n",
      "A_logits_diff tensor(0.)\n",
      "Step 0: Loss = 6.8608, Accuracy = 0.4934\n",
      "A_n tensor([0.1988, 0.1992, 0.2019, 0.1991, 0.2010], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1.,  1.,  1., -1., -1.]) hat_phi:  tensor([0.1999, 0.1985, 0.1992, 0.2038, 0.2020, 0.2022, 0.2001, 0.1991, 0.1982,\n",
      "        0.2023], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(3.0641, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.4970)\n",
      "A_logits_diff tensor(41.4153)\n",
      "A_n tensor([0.1901, 0.1905, 0.1931, 0.2343, 0.1922], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1.,  1., -1., -1.,  1.]) hat_phi:  tensor([0.1454, 0.1480, 0.1472, 0.1487, 0.1475, 0.1487, 0.1486, 0.1509, 0.1476,\n",
      "        0.1481], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.2047, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5040)\n",
      "A_logits_diff tensor(62.2805)\n",
      "A_n tensor([0.1726, 0.1824, 0.2035, 0.2317, 0.2098], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([-0.2155, -0.2180, -0.2155, -0.2181, -0.2179, -0.2171, -0.2145, -0.2157,\n",
      "        -0.2163, -0.2185], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4734, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5034)\n",
      "A_logits_diff tensor(74.8092)\n",
      "A_n tensor([0.1803, 0.1812, 0.2010, 0.2359, 0.2016], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1.,  1., -1.]) hat_phi:  tensor([-0.1659, -0.1919, -0.1628, -0.1962, -0.1704, -0.1900, -0.1890, -0.1949,\n",
      "        -0.1897, -0.1914], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.9123, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5018)\n",
      "A_logits_diff tensor(86.1471)\n",
      "A_n tensor([0.1528, 0.1536, 0.1698, 0.2799, 0.2439], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1.,  1., -1.,  1.]) hat_phi:  tensor([0.1330, 0.1733, 0.1332, 0.1773, 0.1442, 0.1728, 0.1676, 0.1749, 0.1718,\n",
      "        0.1724], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(3.4909, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5140)\n",
      "A_logits_diff tensor(93.5013)\n",
      "Step 5: Loss = 3.4909, Accuracy = 0.5140\n",
      "A_n tensor([0.1363, 0.1709, 0.1688, 0.2514, 0.2726], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([-0.1130, -0.0705, -0.0951, -0.0760, -0.0787, -0.0627, -0.0551, -0.0462,\n",
      "        -0.0792, -0.0759], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(4.2010, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5080)\n",
      "A_logits_diff tensor(100.5240)\n",
      "A_n tensor([0.1187, 0.1734, 0.1708, 0.2189, 0.3181], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([0.3115, 0.3606, 0.3194, 0.3511, 0.3320, 0.3541, 0.3518, 0.3566, 0.3570,\n",
      "        0.3534], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.9751, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5244)\n",
      "A_logits_diff tensor(105.4347)\n",
      "A_n tensor([0.1251, 0.1886, 0.1570, 0.2307, 0.2986], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([-0.0256,  0.0011, -0.0073, -0.0122,  0.0093,  0.0116,  0.0306,  0.0396,\n",
      "        -0.0155, -0.0110], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.8857, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5122)\n",
      "A_logits_diff tensor(110.0472)\n",
      "A_n tensor([0.1078, 0.1718, 0.1509, 0.1934, 0.3761], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1., -1., -1.]) hat_phi:  tensor([-0.7843, -0.8070, -0.7886, -0.8078, -0.7965, -0.8112, -0.8115, -0.8144,\n",
      "        -0.8067, -0.8073], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.3537, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5168)\n",
      "A_logits_diff tensor(114.6836)\n",
      "A_n tensor([0.0822, 0.2113, 0.1855, 0.2358, 0.2853], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1.,  1.,  1., -1.]) hat_phi:  tensor([0.2650, 0.2464, 0.2492, 0.2644, 0.2348, 0.2415, 0.2008, 0.1892, 0.2644,\n",
      "        0.2600], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.6775, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5170)\n",
      "A_logits_diff tensor(119.1198)\n",
      "Step 10: Loss = 2.6775, Accuracy = 0.5170\n",
      "A_n tensor([0.0813, 0.2078, 0.1917, 0.2272, 0.2920], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1.,  1., -1.]) hat_phi:  tensor([-0.1621, -0.2031, -0.1687, -0.1953, -0.1812, -0.2074, -0.2047, -0.2117,\n",
      "        -0.1942, -0.1966], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.0042, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5096)\n",
      "A_logits_diff tensor(122.2435)\n",
      "A_n tensor([0.0838, 0.2164, 0.2014, 0.2184, 0.2800], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1., -1., -1.]) hat_phi:  tensor([-0.5971, -0.6282, -0.5987, -0.6261, -0.6070, -0.6277, -0.6247, -0.6289,\n",
      "        -0.6281, -0.6272], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(3.9306, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.4992)\n",
      "A_logits_diff tensor(129.2620)\n",
      "A_n tensor([0.0802, 0.1998, 0.2221, 0.2033, 0.2946], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1.,  1.,  1.]) hat_phi:  tensor([0.4400, 0.4300, 0.4512, 0.4202, 0.4562, 0.4322, 0.4782, 0.4826, 0.4211,\n",
      "        0.4234], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.8699, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5036)\n",
      "A_logits_diff tensor(133.5496)\n",
      "A_n tensor([0.0803, 0.1991, 0.2204, 0.2046, 0.2956], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([0.1609, 0.1708, 0.1740, 0.1485, 0.1830, 0.1677, 0.2164, 0.2162, 0.1564,\n",
      "        0.1584], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.0624, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5060)\n",
      "A_logits_diff tensor(136.5027)\n",
      "A_n tensor([0.0792, 0.1990, 0.2221, 0.2041, 0.2956], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1.,  1., -1.,  1.]) hat_phi:  tensor([0.1938, 0.1798, 0.2023, 0.1650, 0.2036, 0.1790, 0.2241, 0.2244, 0.1651,\n",
      "        0.1686], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.0278, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5100)\n",
      "A_logits_diff tensor(138.6235)\n",
      "Step 15: Loss = 2.0278, Accuracy = 0.5100\n",
      "A_n tensor([0.0778, 0.1972, 0.2280, 0.2064, 0.2906], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1.,  1., -1.,  1.]) hat_phi:  tensor([0.1928, 0.1828, 0.1982, 0.1664, 0.2004, 0.1808, 0.2172, 0.2204, 0.1645,\n",
      "        0.1685], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6397, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5168)\n",
      "A_logits_diff tensor(140.9520)\n",
      "A_n tensor([0.0780, 0.1963, 0.2291, 0.2060, 0.2906], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1.,  1., -1.,  1., -1.]) hat_phi:  tensor([-0.0394, -0.0430, -0.0450, -0.0315, -0.0528, -0.0445, -0.0789, -0.0880,\n",
      "        -0.0266, -0.0305], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.5341, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5178)\n",
      "A_logits_diff tensor(142.3482)\n",
      "A_n tensor([0.0784, 0.1994, 0.2266, 0.2049, 0.2907], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1., -1.]) hat_phi:  tensor([-0.6012, -0.5583, -0.6038, -0.5591, -0.5941, -0.5562, -0.6142, -0.6173,\n",
      "        -0.5565, -0.5581], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.2185, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5180)\n",
      "A_logits_diff tensor(144.6180)\n",
      "A_n tensor([0.0768, 0.1936, 0.2134, 0.2156, 0.3006], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1.,  1.,  1.]) hat_phi:  tensor([0.4195, 0.4569, 0.4336, 0.4474, 0.4453, 0.4619, 0.4734, 0.4791, 0.4511,\n",
      "        0.4523], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6271, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5390)\n",
      "A_logits_diff tensor(146.7437)\n",
      "A_n tensor([0.0764, 0.1889, 0.2220, 0.2148, 0.2980], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1.,  1.,  1., -1.]) hat_phi:  tensor([ 0.0262, -0.0367,  0.0190, -0.0294,  0.0110, -0.0390, -0.0482, -0.0491,\n",
      "        -0.0333, -0.0341], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.8335, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5326)\n",
      "A_logits_diff tensor(148.8063)\n",
      "Step 20: Loss = 1.8335, Accuracy = 0.5326\n",
      "A_n tensor([0.0761, 0.1930, 0.2218, 0.2178, 0.2912], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1.,  1.,  1., -1., -1.]) hat_phi:  tensor([-0.0180,  0.0028, -0.0323, -0.0118, -0.0204, -0.0092, -0.0925, -0.1054,\n",
      "        -0.0097, -0.0129], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.3578, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5236)\n",
      "A_logits_diff tensor(150.7635)\n",
      "A_n tensor([0.0780, 0.2002, 0.2128, 0.2098, 0.2991], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1., -1.,  1.]) hat_phi:  tensor([-0.2457, -0.2863, -0.2268, -0.2902, -0.2475, -0.2823, -0.1989, -0.1853,\n",
      "        -0.2923, -0.2881], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6222, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5224)\n",
      "A_logits_diff tensor(152.8487)\n",
      "A_n tensor([0.0786, 0.2043, 0.1933, 0.2189, 0.3049], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1., -1., -1.]) hat_phi:  tensor([-0.6134, -0.6060, -0.6281, -0.5842, -0.6275, -0.6130, -0.6398, -0.6388,\n",
      "        -0.6006, -0.6010], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.5696, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5206)\n",
      "A_logits_diff tensor(154.7320)\n",
      "A_n tensor([0.0815, 0.2114, 0.1784, 0.2305, 0.2982], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([0.2205, 0.1431, 0.2305, 0.1456, 0.1758, 0.1313, 0.2615, 0.2433, 0.1530,\n",
      "        0.1562], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.7574, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5132)\n",
      "A_logits_diff tensor(156.5297)\n",
      "A_n tensor([0.0780, 0.2188, 0.1710, 0.2306, 0.3016], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([0.2204, 0.1382, 0.2315, 0.1470, 0.1739, 0.1273, 0.2584, 0.2410, 0.1497,\n",
      "        0.1536], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6900, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5292)\n",
      "A_logits_diff tensor(158.4742)\n",
      "Step 25: Loss = 1.6900, Accuracy = 0.5292\n",
      "A_n tensor([0.0759, 0.2251, 0.1756, 0.2234, 0.3001], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1., -1.,  1.]) hat_phi:  tensor([-0.2480, -0.3020, -0.2549, -0.3205, -0.3158, -0.3501, -0.2125, -0.2622,\n",
      "        -0.3137, -0.3139], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.7684, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5294)\n",
      "A_logits_diff tensor(161.0933)\n",
      "A_n tensor([0.0778, 0.2365, 0.1893, 0.2073, 0.2890], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([0.1484, 0.0835, 0.1474, 0.0808, 0.1020, 0.0528, 0.1945, 0.1491, 0.0751,\n",
      "        0.0774], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4421, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5218)\n",
      "A_logits_diff tensor(163.5582)\n",
      "A_n tensor([0.0782, 0.2218, 0.1955, 0.2125, 0.2919], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1.,  1.,  1., -1.]) hat_phi:  tensor([0.2597, 0.2737, 0.2755, 0.3601, 0.3649, 0.3843, 0.2047, 0.3514, 0.3259,\n",
      "        0.3288], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.8890, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5276)\n",
      "A_logits_diff tensor(165.6566)\n",
      "A_n tensor([0.0817, 0.1904, 0.2099, 0.2316, 0.2864], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([-0.0464, -0.0787, -0.0475, -0.1294, -0.1200, -0.1376, -0.0224, -0.1033,\n",
      "        -0.1072, -0.1070], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6647, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5466)\n",
      "A_logits_diff tensor(167.7751)\n",
      "A_n tensor([0.0879, 0.2109, 0.2146, 0.2097, 0.2769], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1., -1.,  1.,  1.]) hat_phi:  tensor([-0.0269, -0.0977, -0.0280, -0.0781, -0.0965, -0.1209,  0.0261, -0.0084,\n",
      "        -0.0943, -0.0888], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.5576, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5406)\n",
      "A_logits_diff tensor(170.0871)\n",
      "Step 30: Loss = 1.5576, Accuracy = 0.5406\n",
      "A_n tensor([0.0780, 0.2421, 0.1980, 0.2222, 0.2596], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([ 0.0035, -0.0314,  0.0037, -0.1132, -0.0749, -0.0913,  0.0207, -0.0814,\n",
      "        -0.0602, -0.0626], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.1527, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5148)\n",
      "A_logits_diff tensor(172.5684)\n",
      "A_n tensor([0.0779, 0.2335, 0.1991, 0.2330, 0.2565], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1.,  1.,  1., -1., -1.]) hat_phi:  tensor([ 0.0210,  0.0773,  0.0296,  0.0745,  0.0659,  0.1020, -0.0324,  0.0046,\n",
      "         0.0824,  0.0836], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6778, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5272)\n",
      "A_logits_diff tensor(174.8497)\n",
      "A_n tensor([0.0763, 0.2458, 0.1966, 0.2269, 0.2544], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([ 0.1152,  0.0672,  0.0978, -0.0040,  0.0848,  0.0053,  0.1706,  0.0290,\n",
      "         0.0322,  0.0185], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.5247, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5296)\n",
      "A_logits_diff tensor(178.2778)\n",
      "A_n tensor([0.0727, 0.2393, 0.2040, 0.2300, 0.2539], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1., -1., -1.]) hat_phi:  tensor([-0.8545, -0.8479, -0.8657, -0.8751, -0.8691, -0.8789, -0.8617, -0.9001,\n",
      "        -0.8708, -0.8714], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(2.2839, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5302)\n",
      "A_logits_diff tensor(181.3944)\n",
      "A_n tensor([0.0753, 0.2391, 0.2144, 0.2272, 0.2440], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([-0.0338, -0.1002, -0.0260, -0.1432, -0.1263, -0.1562, -0.0026, -0.1447,\n",
      "        -0.1019, -0.0988], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6531, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5476)\n",
      "A_logits_diff tensor(183.8056)\n",
      "Step 35: Loss = 1.6531, Accuracy = 0.5476\n",
      "A_n tensor([0.0700, 0.2372, 0.2199, 0.2232, 0.2499], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1.,  1.,  1.,  1.]) hat_phi:  tensor([0.8601, 0.8636, 0.8762, 0.8666, 0.8872, 0.8963, 0.8763, 0.9147, 0.8798,\n",
      "        0.8800], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4768, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5420)\n",
      "A_logits_diff tensor(186.0161)\n",
      "A_n tensor([0.0742, 0.2174, 0.1952, 0.2370, 0.2762], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([ 0.1748,  0.0601,  0.1587,  0.1533,  0.0783, -0.0278,  0.1364,  0.0286,\n",
      "         0.0790,  0.0716], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.5167, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5348)\n",
      "A_logits_diff tensor(187.7682)\n",
      "A_n tensor([0.0787, 0.2375, 0.1557, 0.2549, 0.2731], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1., -1., -1.,  1.]) hat_phi:  tensor([-0.4537, -0.5310, -0.4534, -0.6105, -0.5806, -0.6148, -0.3801, -0.5472,\n",
      "        -0.5531, -0.5549], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.5045, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5552)\n",
      "A_logits_diff tensor(190.2230)\n",
      "A_n tensor([0.0835, 0.2217, 0.1715, 0.2184, 0.3048], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1.,  1., -1.,  1.]) hat_phi:  tensor([0.3962, 0.2499, 0.4181, 0.2936, 0.3748, 0.3513, 0.4129, 0.4716, 0.2563,\n",
      "        0.2800], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4208, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5462)\n",
      "A_logits_diff tensor(192.8533)\n",
      "A_n tensor([0.0723, 0.2141, 0.1602, 0.2284, 0.3249], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1.,  1.,  1.]) hat_phi:  tensor([0.4270, 0.4203, 0.4689, 0.4301, 0.4601, 0.4808, 0.4905, 0.5511, 0.4196,\n",
      "        0.4249], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.3790, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5542)\n",
      "A_logits_diff tensor(195.1909)\n",
      "Step 40: Loss = 1.3790, Accuracy = 0.5542\n",
      "A_n tensor([0.0762, 0.2083, 0.1633, 0.2269, 0.3252], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1., -1., -1.,  1.]) hat_phi:  tensor([ 0.0671, -0.1439,  0.0525, -0.1138, -0.1021, -0.2014, -0.0078, -0.1502,\n",
      "        -0.1304, -0.1303], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4541, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5512)\n",
      "A_logits_diff tensor(198.0701)\n",
      "A_n tensor([0.0757, 0.2009, 0.1579, 0.2155, 0.3501], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1., -1.,  1.,  1.]) hat_phi:  tensor([ 0.1311,  0.0540,  0.1275,  0.0435,  0.0136, -0.0376,  0.0959, -0.0361,\n",
      "         0.0581,  0.0507], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.3231, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5616)\n",
      "A_logits_diff tensor(200.0674)\n",
      "A_n tensor([0.0715, 0.1864, 0.1500, 0.2225, 0.3696], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1., -1., -1., -1.]) hat_phi:  tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.5026, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5568)\n",
      "A_logits_diff tensor(202.5935)\n",
      "A_n tensor([0.0752, 0.1911, 0.1547, 0.2286, 0.3503], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1., -1., -1., -1.]) hat_phi:  tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4930, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5306)\n",
      "A_logits_diff tensor(204.7685)\n",
      "A_n tensor([0.0792, 0.2155, 0.1475, 0.2467, 0.3111], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1., -1., -1., -1.]) hat_phi:  tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.6019, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5696)\n",
      "A_logits_diff tensor(206.6186)\n",
      "Step 45: Loss = 1.6019, Accuracy = 0.5696\n",
      "A_n tensor([0.0771, 0.2250, 0.1560, 0.2535, 0.2884], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1., -1.,  1.,  1.]) hat_phi:  tensor([ 0.2380,  0.1394,  0.2089,  0.1498,  0.1091, -0.0253,  0.2427,  0.0220,\n",
      "         0.1016,  0.0898], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.7042, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5560)\n",
      "A_logits_diff tensor(209.0505)\n",
      "A_n tensor([0.0676, 0.2731, 0.1398, 0.2161, 0.3035], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1.,  1.,  1.,  1., -1.]) hat_phi:  tensor([0.2578, 0.3045, 0.2949, 0.4387, 0.3728, 0.4867, 0.2616, 0.4413, 0.3811,\n",
      "        0.3996], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.4895, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5592)\n",
      "A_logits_diff tensor(211.0579)\n",
      "A_n tensor([0.0696, 0.2191, 0.1484, 0.2310, 0.3318], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([-1., -1.,  1., -1., -1.]) hat_phi:  tensor([-0.7031, -0.6706, -0.7131, -0.6583, -0.6115, -0.5722, -0.6491, -0.5756,\n",
      "        -0.6653, -0.6680], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.7393, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5508)\n",
      "A_logits_diff tensor(212.8730)\n",
      "A_n tensor([0.0666, 0.2363, 0.1646, 0.2120, 0.3205], grad_fn=<SelectBackward0>) torch.Size([30, 5])\n",
      "context:  tensor([ 1., -1.,  1., -1., -1.]) hat_phi:  tensor([-0.5376, -0.4252, -0.5037, -0.4285, -0.3612, -0.3471, -0.5144, -0.4201,\n",
      "        -0.4395, -0.4357], grad_fn=<SliceBackward0>)\n",
      "avg loss:  tensor(1.7067, grad_fn=<DivBackward0>) avg accuracy:  tensor(0.5730)\n",
      "A_logits_diff tensor(214.9392)\n",
      "Verification complete.\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.15.2\n",
    "#   kernelspec:\n",
    "#     display_name: Python (fmri)\n",
    "#     language: python\n",
    "#     name: fmri\n",
    "# ---\n",
    "\n",
    "# +\n",
    "# We want to maximize the dot product\n",
    "# Step 1: Add the position encoding (content + position)\n",
    "# INFONCE: A and B are complete unconstrained, instead of optimizing over pi\n",
    "# Check: Does the model localize the right memory\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "class DAM(nn.Module):\n",
    "    def __init__(self, N, H, M, eta=1.0, init_std=1e-2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            N: Sequence length (bits).\n",
    "            H: Hidden dimension (number of heads).\n",
    "            M: Memory capacity (number of sequences).\n",
    "            eta: Inverse temperature parameter.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.eta = eta\n",
    "        self.init_std = init_std\n",
    "\n",
    "        # A(n) parameters: (N, H, N). We will mask unused parts.\n",
    "        # We need independent parameters for each length n.\n",
    "        # For length n (predicting n-th bit, 0-indexed), we use inputs 0..n-1.\n",
    "        # So we have separate weights for each n.\n",
    "        self.A_logits = nn.Parameter(torch.randn(N, H, N) * self.init_std) # small positive weights\n",
    "\n",
    "        # B parameters: (H, N).\n",
    "        self.B_logits = nn.Parameter(torch.randn(H, N) * self.init_std) # small positive weights\n",
    "\n",
    "        # Memory state\n",
    "        # We store binary sequences\n",
    "        self.memory = torch.zeros(0, N)\n",
    "        self.is_memory_empty = True\n",
    "\n",
    "    def update_memory(self, sequences):\n",
    "        \"\"\"\n",
    "        Updates the DAM with new sequences.\n",
    "        The memory is a queue so we only keep the last M sequences.\n",
    "        Args:\n",
    "            sequences: (Batch, N) tensor of binary sequences (-1, +1).\n",
    "        \"\"\"\n",
    "        new_memory = torch.cat([self.memory, sequences.detach().to(self.memory.device)], dim=0)\n",
    "\n",
    "        if new_memory.shape[0] > self.M:\n",
    "            new_memory = new_memory[-self.M:]\n",
    "\n",
    "        self.memory = new_memory\n",
    "        self.is_memory_empty = False\n",
    "\n",
    "    def get_A(self, n):\n",
    "        \"\"\"\n",
    "        Returns normalized A(n) matrix of shape (H, n).\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return torch.zeros(self.H, 0, device=self.A_logits.device)\n",
    "\n",
    "        # Select params for predictng n+1 th bit (index n).\n",
    "        # We use slice 0:n because we attend to first n bits.\n",
    "        logits = self.A_logits[n, :, :n] # (H, n)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def get_B(self):\n",
    "        \"\"\"\n",
    "        Returns normalized B matrix of shape (H, N).\n",
    "        \"\"\"\n",
    "        return F.softmax(self.B_logits, dim=-1)\n",
    "\n",
    "    def forward_step(self, zeta, n, phi_mu):\n",
    "        \"\"\"\n",
    "        Predict probability of (n+1)-th bit (index n) being +1.\n",
    "\n",
    "        Args:\n",
    "            zeta: (Batch, N) full sequences (we only peek up to n).\n",
    "            n: int, current length (0 to N-1). We verify up to zeta[:, :n].\n",
    "               We want to predict zeta[:, n].\n",
    "            phi_mu: (M, H) Precomputed memory keys.\n",
    "\n",
    "        Returns:\n",
    "            probs: (Batch,) probability that next bit is +1.\n",
    "        \"\"\"\n",
    "        B_val = zeta.shape[0] # batch size\n",
    "\n",
    "        # handle empty memory\n",
    "        if self.is_memory_empty: # shape: (M, N) where M = memory capacity, N = sequence length\n",
    "            return torch.full((B_val,), 0.5, device=zeta.device)\n",
    "\n",
    "        A_n = self.get_A(n) # shape: (H, n)\n",
    "        # context is the input sequence up to position n (exclusive)\n",
    "        context = zeta[:, :n] # shape: (Batch, n)\n",
    "        # hat_phi = sum A_i * zeta_i.\n",
    "        hat_phi = torch.einsum('bi,hi->bh', context, A_n)\n",
    "        # if n == 5 and np.random.rand() < 0.1:\n",
    "        #     print(\"A_n\", A_n[0].flatten(), A_n.shape, self.A_logits[n, :, :n])\n",
    "        #     print(\"context: \", context[0,:10], \"hat_phi: \", hat_phi[0,:10])\n",
    "            # print(\"B_mat\", B_mat, B_mat.shape)\n",
    "        # 3. Retrieval probabilities\n",
    "        # score: (Batch, M)\n",
    "        # score = eta * hat_phi^T phi_mu\n",
    "        score = self.eta * torch.einsum('bh,mh->bm', hat_phi, phi_mu)\n",
    "        pi = F.softmax(score, dim=1) # (Batch, M)\n",
    "        \n",
    "        # 4. Predict next bit\n",
    "        # We want prob that (n)-th bit is +1.\n",
    "        # memory_bits: (M,)\n",
    "        memory_bits_at_n = self.memory[:, n] # +1 or -1\n",
    "\n",
    "        # We want sum of pi where memory bit is +1.\n",
    "        # Indicator: (memory_bits_at_n == 1).float()\n",
    "        plus_one_mask = (memory_bits_at_n > 0).float()\n",
    "        # prob = sum(pi * mask)\n",
    "        prob_plus_one = torch.sum(pi * plus_one_mask.unsqueeze(0), dim=1)\n",
    "\n",
    "        # print(\"prob_plus_one: \", prob_plus_one)\n",
    "        return prob_plus_one\n",
    "\n",
    "    def train_batch(self, sequences, optimizer):\n",
    "        \"\"\"\n",
    "        Computes the BCE loss averaged over all positions.\n",
    "        Args:\n",
    "             sequences: (Batch, N)\n",
    "        Returns:\n",
    "             loss: scalar\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        \n",
    "        # If memory is empty, we can't really predict based on history, \n",
    "        # but we returning 0.5 loss is appropriate and no grad update.\n",
    "        if self.is_memory_empty:\n",
    "            # Just return dummy values, no update\n",
    "            bce = -torch.log(torch.tensor(0.5))\n",
    "            return bce, torch.tensor(0.5)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_batch = 0.0\n",
    "        \n",
    "        # Precompute phi_mu for the batch (Constant for this batch update)\n",
    "        # phi_mu = B (H, N) x memory^T (N, M) -> (H, M) -> transpose to (M, H)\n",
    "        # einsum: hn, mn -> mh\n",
    "        phi_mu = torch.einsum('hn,mn->mh', self.get_B(), self.memory)\n",
    "        \n",
    "        for n in range(1, self.N):\n",
    "            # Predict n-th bit (0-indexed) using 0..n-1 history\n",
    "            prob_plus_one = self.forward_step(sequences, n, phi_mu) # shape: (Batch,)\n",
    "            # Target\n",
    "            target = sequences[:, n] # -1 or +1, shape: (Batch,)\n",
    "            # Convert target to 0/1 for BCE\n",
    "            target_01 = (target > 0).float()\n",
    "\n",
    "            # numeric stability\n",
    "            prob_plus_one = torch.clamp(prob_plus_one, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "            loss_n = F.binary_cross_entropy(prob_plus_one, target_01)\n",
    "            loss_batch = loss_batch + loss_n\n",
    "\n",
    "            # accuracy\n",
    "            accuracy_n = ((prob_plus_one > 0.5) == (target_01 > 0.5))\n",
    "            total_accuracy += accuracy_n.float().mean()\n",
    "            \n",
    "        # Normalize sum of losses by number of predictions (N-1)\n",
    "        # Note: problem says 1/N sum_{n=0}^{N-1}, but code loop is range(1, N) -> n=1..N-1.\n",
    "        # This misses n=0 prediction. But n=0 has 0 context.\n",
    "        # forward_step(n=0) uses context zeta[:, :0] (empty).\n",
    "        # We can include n=0 if we want, but let's stick to existing range but fix normalization.\n",
    "        \n",
    "        loss_final = loss_batch / (self.N - 1)\n",
    "        loss_final.backward()\n",
    "        \n",
    "        # Log grad norms (optional, for debugging)\n",
    "        # total_norm = 0\n",
    "        # for p in self.parameters():\n",
    "        #    if p.grad is not None:\n",
    "        #        total_norm += p.grad.data.norm(2).item()**2\n",
    "        # total_norm = total_norm ** 0.5\n",
    "        # print(f\"Grad norm: {total_norm}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss = loss_final.item()\n",
    "        avg_accuracy = total_accuracy / (self.N - 1)\n",
    "\n",
    "        return total_loss, avg_accuracy\n",
    "\n",
    "def initialize_record(args):\n",
    "    record = {\n",
    "        \"args\": args,\n",
    "        \"logs\": [],\n",
    "    }\n",
    "    return record\n",
    "\n",
    "\n",
    "# +\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running DAM...\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--eta\", type=float, default=1e1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--INIT_STD\", type=float, default=1e0)\n",
    "    parser.add_argument(\"--N\", type=int, default=50)\n",
    "    parser.add_argument(\"--H\", type=int, default=30)\n",
    "    parser.add_argument(\"--M\", type=int, default=3000)\n",
    "    parser.add_argument(\"--K\", type=int, default=500)\n",
    "    parser.add_argument(\"--BATCH_SIZE\", type=int, default=50)\n",
    "    parser.add_argument(\"--NUM_STEPS\", type=int, default=50000)\n",
    "    parser.add_argument(\"--NUM_ITERS_PER_LOG\", type=int, default=100)\n",
    "    parser.add_argument(\"--savedir\", type=str, default=\"/scratch/qanguyen/gautam\")\n",
    "    parser.add_argument(\"--prefix\", type=str, default=\"\")\n",
    "    try:\n",
    "        args = parser.parse_args()\n",
    "    except:\n",
    "        args = parser.parse_args([])\n",
    "\n",
    "    # Create savedir if it doesn't exist\n",
    "    os.makedirs(f\"{args.savedir}/{args.prefix}\", exist_ok=True) \n",
    "    exp_name = f\"{args.prefix}/dam_{time.time()}\"\n",
    "    model = DAM(args.N, args.H, args.M, eta=args.eta, init_std=args.INIT_STD)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    record = initialize_record(args)\n",
    "\n",
    "    # Generate random sequences\n",
    "    sequences = torch.sign(torch.randn(args.K, args.N))\n",
    "\n",
    "    # Initialize memory\n",
    "    # We start with empty memory as per standard online learning or just-in-time memorization.\n",
    "    # Initializing with zeros is harmful because dot products with -1/+1 sequences are low,\n",
    "    # effectively acting as noise or bias if not masked correctly.\n",
    "    # init_mem = torch.zeros(args.M, args.N)\n",
    "    # model.update_memory(init_mem)\n",
    "    # print(f\"Memory initialized with shape: {model.memory.shape}\")\n",
    "    print(\"Memory initialized as empty.\")\n",
    "\n",
    "    # Training Loop Simulation\n",
    "    model.train()\n",
    "    for step in range(args.NUM_STEPS):\n",
    "\n",
    "        # Sample batch from sequences\n",
    "        indices = torch.randint(0, args.K, (args.BATCH_SIZE,))\n",
    "        batch = sequences[indices]\n",
    "\n",
    "        loss, accuracy = model.train_batch(batch, optimizer)\n",
    "\n",
    "        # Update memory\n",
    "        model.update_memory(batch)\n",
    "        # Compute stats\n",
    "        with torch.no_grad():\n",
    "            A_grad_norm = model.A_logits.grad.norm().item() if model.A_logits.grad is not None else 0.0\n",
    "            B_grad_norm = model.B_logits.grad.norm().item() if model.B_logits.grad is not None else 0.0\n",
    "            # A_entropy (approximate, averaged over N)\n",
    "            # Just take mean max attention as proxy for \"sharpness\"\n",
    "            A_sharpness = 0.0\n",
    "            for n in range(1, args.N):\n",
    "                A_n = model.get_A(n) # (H, n)\n",
    "                A_sharpness += A_n.max(dim=1).values.mean().item()\n",
    "            A_sharpness /= (args.N - 1)\n",
    "            \n",
    "            logs = {\n",
    "                \"step\": step,\n",
    "                \"loss\": loss,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"A_grad_norm\": A_grad_norm,\n",
    "                \"B_grad_norm\": B_grad_norm,\n",
    "                \"A_sharpness\": A_sharpness\n",
    "            }\n",
    "            record[\"logs\"].append(logs)\n",
    "\n",
    "        if step % args.NUM_ITERS_PER_LOG == 0:\n",
    "            print(f\"Step {step}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "            \n",
    "            print(f\"Stats: A_grad={A_grad_norm:.4f}, B_grad={B_grad_norm:.4f}, A_sharp={A_sharpness:.4f}\")\n",
    "\n",
    "    print(\"Verification complete.\")\n",
    "    # save model.state_dict() \n",
    "    record[\"model_state_dict\"] = model.state_dict()\n",
    "    torch.save(record, f\"{args.savedir}/{exp_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebf7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fmri)",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
