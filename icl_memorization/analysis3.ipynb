{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60107c69-1ecc-4334-9e83-1c3db06673b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "with open('./cache/memo_apr10_zipf_num_heads_8_num_layers_12_resample/memo_apr10_zipf_num_heads_8_num_layers_12_resample_transformer_K_100000_L_100_hidden_8_nheads_8_nlayers_12_1744325270.2539148.pkl', 'rb') as f:\n",
    "    data = utils.CPU_Unpickler(f).load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061534f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i['train_loss'] for i in data['logs']][:100])\n",
    "plt.plot([i['train_loss'] for i in data['logs']][100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5165dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['logs']:\n",
    "    if i['test_metrics'] != {}:\n",
    "        print(i['test_metrics'][\"logsoftmaxloss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3f6f7",
   "metadata": {},
   "source": [
    "# Apr 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e0e0c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "with open('./cache/memo_apr10_zipf_num_heads_24_num_layers_36_resample_lr_1e-4/memo_apr10_zipf_num_heads_24_num_layers_36_resample_lr_1e-4_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1744954785.908804.pkl', 'rb') as f:\n",
    "    data = utils.CPU_Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0bf97bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_appearance = [i[\"loss_per_appearance\"] for i in data[\"logs\"]] # list of list of arrays\n",
    "appearances = [i[\"appearances\"] for i in data[\"logs\"]] # list of list of arrays\n",
    "time_per_batch = []\n",
    "for epoch, batch in enumerate(data[\"logs\"]):\n",
    "    time_per_batch.append([])\n",
    "    num_batches = len(batch[\"appearances\"])\n",
    "    for i, batch_id in enumerate(batch[\"appearances\"]):\n",
    "        time_per_batch[epoch].append(epoch * num_batches + (i) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "23c84d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time, appearance, loss\", len(time_per_batch[0]), len(appearances[0]), len(loss_per_appearance[0]))\n",
    "ranks_to_plot = [0, 10, 50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "rank_times = defaultdict(list)\n",
    "rank_losses = defaultdict(list)\n",
    "num_appearances = defaultdict(float)\n",
    "rank_appearances = defaultdict(list)\n",
    "# loop through each epoch\n",
    "for t, a, l in zip(time_per_batch, appearances, loss_per_appearance):\n",
    "    # loop through each batch\n",
    "    for i in range(len(t)):\n",
    "        # loop through each rank\n",
    "        for rank in range(len(ranks_to_plot)):\n",
    "            # print(\"rank\", ranks_to_plot[rank], \"time\", t[i], \"loss\", l[i][np.where(a[i] == ranks_to_plot[rank])].mean())\n",
    "            rank_times[ranks_to_plot[rank]].append(t[i])\n",
    "            rank_losses[ranks_to_plot[rank]].append(l[i][np.where(a[i] == ranks_to_plot[rank])].mean())\n",
    "            n = np.sum(a[i] == ranks_to_plot[rank])\n",
    "            if n > 0:\n",
    "                num_appearances[ranks_to_plot[rank]] += n\n",
    "            rank_appearances[ranks_to_plot[rank]].append(num_appearances[ranks_to_plot[rank]])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53d3e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "markers = [\"o\", \"s\", \"D\", \"P\", \"X\", \"v\", \"^\", \"<\", \">\"]\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def numbers_to_viridis_colors(numbers, vmin, vmax):\n",
    "    \"\"\"\n",
    "    Convert a list of real numbers to colors using the viridis colormap.\n",
    "    \n",
    "    Args:\n",
    "        numbers (list or numpy.ndarray): List of real numbers to convert to colors\n",
    "        \n",
    "    Returns:\n",
    "        list: List of RGB colors in the format (r, g, b) where each component is between 0 and 1\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it's not already\n",
    "    numbers = np.array(numbers)\n",
    "    \n",
    "    # Create a normalizer to map the numbers to [0, 1] range\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # Get the viridis colormap\n",
    "    cmap = plt.cm.viridis\n",
    "    \n",
    "    # Convert numbers to colors\n",
    "    colors = cmap(norm(numbers))\n",
    "    \n",
    "    # Return list of RGB colors (excluding alpha channel)\n",
    "    return colors[:, :3].tolist()\n",
    "colormin = np.log10(min(np.concatenate(list(rank_appearances.values())))+1)\n",
    "colormax = np.log10(max(np.concatenate(list(rank_appearances.values())))+1)\n",
    "print(\"colormin\", colormin, \"colormax\", colormax)\n",
    "for i, rank in enumerate(ranks_to_plot):\n",
    "    # print(\"rank_appearances\",  numbers_to_viridis_colors(np.log(np.array(rank_appearances[rank])+1), colormin, colormax))\n",
    "    scatter = plt.scatter(rank_times[rank], rank_losses[rank], \n",
    "                        #   c=numbers_to_viridis_colors(np.log(np.array(rank_appearances[rank])+1), colormin, colormax),\n",
    "                        c = np.log10(np.array(rank_appearances[rank])+1),\n",
    "                          norm = mcolors.Normalize(vmin=colormin, vmax=colormax),\n",
    "                label=f\"rank {rank}\", s=2, marker=markers[i])\n",
    "    \n",
    "    cb = plt.colorbar()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa968a",
   "metadata": {},
   "source": [
    "## Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29e3909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "# with open('./cache/memo_apr6_zipf_num_heads_8_num_layers_12_forget/memo_apr6_zipf_num_heads_8_num_layers_12_forget_transformer_K_100000_L_100_hidden_8_nheads_8_nlayers_12_1745001737.7764006.pkl', 'rb') as f:\n",
    "with open('./cache/memo_apr6_zipf_num_heads_24_num_layers_36_lr_1e-4_forget/memo_apr6_zipf_num_heads_24_num_layers_36_lr_1e-4_forget_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1745000344.072605.pkl', 'rb') as f:\n",
    "    data = utils.CPU_Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdfb6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = np.stack([i[\"test_metrics\"][\"logsoftmaxloss\"] for i in data[\"logs\"]]) # list of list of arrays\n",
    "print(test_epoch.shape)\n",
    "for i in [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]:\n",
    "    rank0 = (i+300) % 2000\n",
    "    rank1 = (i+500) % 2000\n",
    "    spacing = 50\n",
    "    switch_epoch = (i+20) // 2000\n",
    "    plt.plot(test_epoch[:,i+rank0], marker=\"o\", markersize=1, linewidth=0.5, label=f\"rank {rank0 * spacing}, switch {switch_epoch}\")\n",
    "    plt.plot(test_epoch[:,i+rank1], marker=\"o\", markersize=1, linewidth=0.5, label=f\"rank {rank1 * spacing}, switch {switch_epoch}\")\n",
    "plt.title(\"Forgetting: loss vs. rank \\n Model: 24 heads, 36 layers, lr=1e-4\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=(1.1,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1056f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bee5a0",
   "metadata": {},
   "source": [
    "# May 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6312f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "# fname = './cache/memo_may3_zipf_num_heads_8_num_layers_12_zero_all_attn_weights/memo_may3_zipf_num_heads_8_num_layers_12_zero_all_attn_weights_transformer_K_100000_L_100_hidden_8_nheads_8_nlayers_12_1746418255.6068754/memo_may3_zipf_num_heads_8_num_layers_12_zero_all_attn_weights_transformer_K_100000_L_100_hidden_8_nheads_8_nlayers_12_1746418255.6068754.pkl'\n",
    "# fname = './cache/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_except_cproj_weights/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_except_cproj_weights_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1746418285.1167283/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_except_cproj_weights_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1746418285.1167283.pkl'\n",
    "fname = './cache/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1747077824.4925184/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1747077824.4925184.pkl'\n",
    "with open(fname, \n",
    "          'rb') as f:\n",
    "    data = utils.CPU_Unpickler(f).load()\n",
    "print(\"args\", data[\"args\"])\n",
    "# plot train loss\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 6))\n",
    "axs.plot(np.arange(0,len(data[\"logs\"])*1000,1000), [i[\"train_loss\"] for i in data[\"logs\"]])\n",
    "axs.set_title(\"Train loss\")\n",
    "axs.set_xlabel(\"Iteration x 1e5\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "axs.set_xticks(np.arange(0,len(data[\"logs\"])*1000,1e5))\n",
    "axs.set_xticklabels(np.arange(0,len(data[\"logs\"])*1000,1e5)/1e5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb49110",
   "metadata": {},
   "outputs": [],
   "source": [
    "logsoftmaxloss = defaultdict(list)\n",
    "for l in data[\"logs\"]: \n",
    "    logsoftmaxloss_array = l[\"test_metrics\"][\"logsoftmaxloss\"][:10000:1000]\n",
    "    logsoftmaxloss[\"logsoftmaxloss\"].extend(logsoftmaxloss_array)\n",
    "    logsoftmaxloss[\"epoch\"].extend([l[\"epoch\"]*1000] * len(logsoftmaxloss_array))\n",
    "    logsoftmaxloss[\"rank\"].extend(np.arange(0, len(logsoftmaxloss_array), 1)*10000)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "logsoftmaxloss = pd.DataFrame(logsoftmaxloss)\n",
    "logsoftmaxloss[\"rank\"] = logsoftmaxloss[\"rank\"].astype(\"category\")\n",
    "sns.lineplot(x=\"epoch\", y=\"logsoftmaxloss\", data=logsoftmaxloss, ax=ax, hue=\"rank\")\n",
    "plt.legend(loc=(1.1,0))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. rank\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154926e",
   "metadata": {},
   "source": [
    "## Analyze unfrozen zero init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d826d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "fname = './cache/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1747077824.4925184/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1747077824.4925184.pkl'\n",
    "with open(fname, \n",
    "          'rb') as f:\n",
    "    data = utils.CPU_Unpickler(f).load()\n",
    "    print(data[\"args\"][\"is_initialize_attention_weights_to_zero\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624a6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = './cache/memo_may3_zipf_num_heads_8_num_layers_12_zero_all_attn_weights/memo_may3_zipf_num_heads_8_num_layers_12_zero_all_attn_weights_transformer_K_100000_L_100_hidden_8_nheads_8_nlayers_12_1746418255.6068754/memo_may3_zipf_num_heads_8_num_layers_12_zero_all_attn_weights_transformer_K_100000_L_100_hidden_8_nheads_8_nlayers_12_1746418255.6068754.pkl'\n",
    "# fname = './cache/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_except_cproj_weights/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_except_cproj_weights_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1746418285.1167283/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_except_cproj_weights_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1746418285.1167283.pkl'\n",
    "f = './cache/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning/memo_may3_zipf_num_heads_24_num_layers_36_lr_1e-4_zero_all_attn_allow_learning_transformer_K_100000_L_100_hidden_8_nheads_24_nlayers_36_1747196536.2310433/*iter_350*'\n",
    "for fname in glob.glob(f):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = utils.CPU_Unpickler(f).load()\n",
    "    for k, v in data.items():\n",
    "        if \"attn\" in k:\n",
    "            print(fname.split(\"/\")[-1], k, v.shape, v.sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845a88d",
   "metadata": {},
   "source": [
    "## One Layer Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c53947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "# fname = './cache/memo_may10_zipf_onelayerattention_lr_1e-3/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336.pkl' \n",
    "f = './cache/memo_may10_zipf_onelayerattention_lr_1e-3_num_hidden_features_8/*'\n",
    "for f in glob(f):\n",
    "    subdir = f.split('/')[-1]\n",
    "    fname = f + '/' + subdir + '.pkl'\n",
    "    with open(fname, \n",
    "            'rb') as f:\n",
    "        data = utils.CPU_Unpickler(f).load()\n",
    "    print(\"args\", data[\"args\"][\"num_mlp_layers\"], [l[\"train_loss\"] for l in data[\"logs\"]])\n",
    "    # plot train loss\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(5, 6))\n",
    "    num_iters_per_epoch = 1000\n",
    "    axs.plot(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch), [i[\"train_loss\"] for i in data[\"logs\"]])\n",
    "    axs.set_title(\"Train loss\")\n",
    "    axs.set_xlabel(\"Iteration x 1e5\")\n",
    "    axs.set_ylabel(\"Loss\")\n",
    "    axs.set_xticks(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch*10))\n",
    "    axs.set_xticklabels(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch*10)/num_iters_per_epoch)\n",
    "    plt.show()\n",
    "    logsoftmaxloss = defaultdict(list)\n",
    "    K = data[\"args\"][\"K\"]\n",
    "    for l in data[\"logs\"]: \n",
    "        logsoftmaxloss_array = l[\"test_metrics\"][\"logsoftmaxloss\"][:K:K//30]\n",
    "        logsoftmaxloss[\"logsoftmaxloss\"].extend(logsoftmaxloss_array)\n",
    "        logsoftmaxloss[\"epoch\"].extend([l[\"epoch\"]*num_iters_per_epoch] * len(logsoftmaxloss_array))\n",
    "        logsoftmaxloss[\"rank\"].extend(np.arange(0, len(logsoftmaxloss_array), 1)*(K//30))\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    logsoftmaxloss = pd.DataFrame(logsoftmaxloss)\n",
    "    logsoftmaxloss[\"rank\"] = logsoftmaxloss[\"rank\"].astype(\"category\")\n",
    "    sns.lineplot(x=\"epoch\", y=\"logsoftmaxloss\", data=logsoftmaxloss, ax=ax, hue=\"rank\")\n",
    "    plt.legend(loc=(1.1,0))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs. rank\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = './cache/memo_may10_zipf_onelayerattention_lr_1e-3/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336.pkl' \n",
    "f = './cache/memo_may10_zipf_onelayerattention_lr_1e-3_num_hidden_features_8/*'\n",
    "import gpt \n",
    "import importlib\n",
    "importlib.reload(gpt)\n",
    "def plot_attn_weights(attn_weights, rank):\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(20, 10))\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            # axs[i, j].imshow(attn_weights[0,i*5+j].detach().cpu().numpy())\n",
    "            axs[i, j].set_title(f\"Head {i*5+j}\")\n",
    "            axs[i, j].plot(attn_weights[rank,i*5+j,10].detach().cpu().numpy().flatten(), label=\"pos=10\")\n",
    "            axs[i, j].plot(attn_weights[rank,i*5+j,50].detach().cpu().numpy().flatten(), label=\"pos=50\")\n",
    "            axs[i, j].plot(attn_weights[rank,i*5+j,99].detach().cpu().numpy().flatten(), label=\"pos=99\")\n",
    "    axs[i, j].legend()\n",
    "    fig.suptitle(f\"Attention weights at different positions for rank 0\")\n",
    "    # axs[i, j].hist(attn_weights[0,i*5+j].detach().cpu().numpy().flatten())\n",
    "    # print(attn_weights[0,i*5+j].mean)\n",
    "    plt.show()\n",
    "for f in glob(f):\n",
    "    subdir = f.split('/')[-1]\n",
    "    fname = f + '/' + subdir + '_model_iter_350.pkl'\n",
    "    record_fname = f + '/' + subdir + '.pkl'\n",
    "    data_fname = f + '/' + subdir + '_all_sequences.pkl'\n",
    "    with open(fname, \n",
    "            'rb') as f:\n",
    "        weights = utils.CPU_Unpickler(f).load()\n",
    "    \n",
    "    with open(record_fname, \n",
    "            'rb') as f:\n",
    "        record = utils.CPU_Unpickler(f).load()\n",
    "    with open(data_fname, \n",
    "            'rb') as f:\n",
    "        data = utils.CPU_Unpickler(f).load()\n",
    "    print(\"weights\", weights.keys())\n",
    "    print(\"data\", data.keys(), [len(f) for f in data[\"sequences\"]])\n",
    "    model = gpt.OneLayerAttention(record[\"args\"][\"len_context\"], \n",
    "                                  record[\"args\"][\"num_heads\"], \n",
    "                                  record[\"args\"][\"num_hidden_features\"], \n",
    "                                  record[\"args\"][\"vocab_size\"], \n",
    "                                  record[\"args\"][\"num_mlp_layers\"]) \n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    output = model(data[\"sequences\"][0], None)\n",
    "    print(\"output\", output.shape)\n",
    "    # accuracy between output[:,:-1,:] and data[\"sequences\"][1:], element-wise\n",
    "    accuracy = (output[:,:-1,:].argmax(dim=-1) == data[\"sequences\"][0][:,1:]).float().mean()\n",
    "    print(\"accuracy\", accuracy)\n",
    "    \n",
    "    attn, attn_weights = model.get_attention_weights(data[\"sequences\"][0], None)\n",
    "    print(\"attn\", attn.shape, \"attn_weights\", attn_weights.shape)\n",
    "    attn = attn.permute(0, 2, 1, 3) # B, T, H, D\n",
    "    # attn is a tensor of shape (B=Batch size, T=Sequence length, H=Number of heads, D=Number of features)\n",
    "    # For each head, we want a plot where\n",
    "    # x-axis is the position in the sequence (i.e. T)\n",
    "    # y-axis is the difference in norm between attn[0,x,h,:] and attn[0,-1,h,:]\n",
    "    # i.e. the norm of the difference between the last position and every other position\n",
    "    # since we want to see whether all heads converge to the same attention weights\n",
    "    # Do this in a vectorized way without for loops\n",
    "    \n",
    "    # Get the last position's attention for each head\n",
    "    last_pos_attn = attn[0, -1, :, :].unsqueeze(0)  # Shape: (1, H, D)\n",
    "    \n",
    "    # Calculate the difference between each position and the last position for all heads\n",
    "    diff = attn[0, :, :, :] - last_pos_attn  # Shape: (T, H, D)\n",
    "    \n",
    "    # Calculate the norm of the difference for each position and head\n",
    "    diff_norm = torch.norm(diff, dim=2)  # Shape: (T, H)\n",
    "    \n",
    "    # Create a plot for each head\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(20, 10))\n",
    "    positions = torch.arange(attn.shape[1])\n",
    "    \n",
    "    for h in range(attn.shape[2]):  # For each head\n",
    "        i, j = h // 5, h % 5\n",
    "        axs[i, j].plot(positions.cpu().numpy(), diff_norm[:, h].detach().cpu().numpy())\n",
    "        axs[i, j].set_title(f\"Head {h}\")\n",
    "        axs[i, j].set_xlabel(\"Position\")\n",
    "        axs[i, j].set_ylabel(\"Norm of difference\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f562cb9",
   "metadata": {},
   "source": [
    "# May 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b70458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "# fname = './cache/memo_may10_zipf_onelayerattention_lr_1e-3/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336.pkl' \n",
    "f = './cache/memo_may26_zipf_onelayerattention_lr_1e-3_vary_num_hidden_features_num_heads/*'\n",
    "df = defaultdict(list)\n",
    "folders = glob(f) + glob('./cache/memo_may26_zipf_onelayerattention_lr_1e-4_vary_num_hidden_features_num_heads/*')\n",
    "for folder in folders:\n",
    "    \n",
    "    subdir = folder.split('/')[-1]\n",
    "    fname = folder + '/' + subdir + '.pkl'\n",
    "    try:\n",
    "        with open(fname, \n",
    "                'rb') as f:\n",
    "            data = utils.CPU_Unpickler(f).load()\n",
    "            # if data[\"args\"][\"num_heads\"] != 1:\n",
    "                # continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    # print(\"args\", data[\"args\"][\"num_mlp_layers\"], [l[\"train_loss\"] for l in data[\"logs\"]])\n",
    "    # plot train loss\n",
    "    # fig, axs = plt.subplots(1, 1, figsize=(5, 6))\n",
    "    # num_iters_per_epoch = 1000\n",
    "    # axs.plot(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch), [i[\"train_loss\"] for i in data[\"logs\"]])\n",
    "    model = data[\"model\"]\n",
    "    \n",
    "    \n",
    "    num_heads = data[\"args\"][\"num_heads\"]\n",
    "    num_hidden_features = data[\"args\"][\"num_hidden_features\"]   \n",
    "    print(\"model\", data[\"args\"][\"lr\"], num_heads, num_hidden_features, np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]))\n",
    "    \n",
    "    # axs.set_title(f\"Train loss: {num_heads} heads, {num_hidden_features} hidden features\")\n",
    "    # axs.set_xlabel(\"Iteration x 1e5\")\n",
    "    # axs.set_ylabel(\"Loss\")\n",
    "    # axs.set_xticks(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch*10))\n",
    "    # axs.set_xticklabels(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch*10)/num_iters_per_epoch)\n",
    "    # plt.show()\n",
    "    logsoftmaxloss = defaultdict(list)\n",
    "    K = data[\"args\"][\"K\"]\n",
    "    pre_switch_last_log = len(data[\"logs\"]) // 2 - 1 \n",
    "    post_switch_last_log = len(data[\"logs\"]) - 1 \n",
    "    last_log = data[\"logs\"][-1]\n",
    "    last_accuracy = last_log[\"test_metrics\"][\"accuracy\"][:10000].mean()\n",
    "    # print(last_log[\"test_metrics\"][\"accuracy\"][9990:10010])\n",
    "    df[\"num_heads\"].append(num_heads)\n",
    "    df[\"num_hidden_features\"].append(num_hidden_features)\n",
    "    df[\"last_accuracy\"].append(last_accuracy)\n",
    "    df[\"last_train_loss\"].append(last_log[\"train_loss\"])\n",
    "    df[\"pre_switch_last_accuracy\"].append(data[\"logs\"][pre_switch_last_log][\"test_metrics\"][\"accuracy\"][:10000].mean())\n",
    "    df[\"pre_switch_last_train_loss\"].append(data[\"logs\"][pre_switch_last_log][\"train_loss\"]) \n",
    "    df[\"num_params\"].append(np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]))\n",
    "    df[\"learning_rate\"].append(data[\"args\"][\"lr\"])\n",
    "    # spacing = 10\n",
    "    # df_num_appearances = defaultdict(list)\n",
    "    # print(len(data[\"logs\"]))\n",
    "    # for log_id, l in enumerate(data[\"logs\"]): \n",
    "    #     logsoftmaxloss_array = l[\"test_metrics\"][\"logsoftmaxloss\"]\n",
    "    #     num_appearances = l[\"num_apppearances\"]\n",
    "    #     accuracy = l[\"test_metrics\"][\"accuracy\"]\n",
    "    #     for rank in [0, 1, 5, 50, 500, 5000]:\n",
    "    #         df_num_appearances[f\"rank\"].append(rank)\n",
    "    #         df_num_appearances[f\"rank_loss\"].append(logsoftmaxloss_array[rank*spacing].mean())\n",
    "    #         df_num_appearances[f\"rank_accuracy\"].append(accuracy[rank*spacing])\n",
    "    #         df_num_appearances[\"num_appearances\"].append(num_appearances[rank*spacing])\n",
    "    #         if log_id <= pre_switch_last_log:\n",
    "    #             df_num_appearances[f\"switch_epoch\"].append(\"preswitch\")\n",
    "    #         else:\n",
    "    #             df_num_appearances[f\"switch_epoch\"].append(\"postswitch\")\n",
    "    # df_num_appearances = pd.DataFrame(df_num_appearances)\n",
    "    # # display(df_num_appearances)\n",
    "    # xax, yax = 2, 3\n",
    "    # fig, axs = plt.subplots(xax, yax, figsize=(12, 6))\n",
    "    # for idx, rank in enumerate([0, 1, 5, 50, 500, 5000]):\n",
    "    #     df_group = df_num_appearances[df_num_appearances[\"rank\"] == rank]\n",
    "    #     sns.lineplot(x=\"num_appearances\", y=\"rank_loss\", hue=\"switch_epoch\", data=df_group, ax=axs[idx//yax, idx%yax])\n",
    "    #     axs[idx//yax, idx%yax].set_title(f\"Rank {rank*spacing}\")\n",
    "    #     axs[idx//yax, idx%yax].set_xlabel(\"Number of appearances\")\n",
    "    #     axs[idx//yax, idx%yax].set_xscale(\"log\")\n",
    "    #     axs[idx//yax, idx%yax].set_ylabel(\"Loss\")\n",
    "    #     axs[idx//yax, idx%yax].set_ylim(0, 0.8)\n",
    "    # plt.title(f\"Loss vs. number of appearances for num_heads={num_heads}, num_hidden_features={num_hidden_features}\")\n",
    "    # plt.show()\n",
    "    #     accuracy = l[\"test_metrics\"][\"accuracy\"] \n",
    "    #     print(\"accuracy\", accuracy.shape)\n",
    "    #     logsoftmaxloss[\"logsoftmaxloss\"].extend(logsoftmaxloss_array)\n",
    "    #     logsoftmaxloss[\"epoch\"].extend([l[\"epoch\"]*num_iters_per_epoch] * len(logsoftmaxloss_array))\n",
    "    #     logsoftmaxloss[\"rank\"].extend(np.arange(0, len(logsoftmaxloss_array), 1)*(K//30))\n",
    "    # fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # logsoftmaxloss = pd.DataFrame(logsoftmaxloss)\n",
    "    # logsoftmaxloss[\"rank\"] = logsoftmaxloss[\"rank\"].astype(\"category\")\n",
    "    # sns.lineplot(x=\"epoch\", y=\"logsoftmaxloss\", data=logsoftmaxloss, ax=ax, hue=\"rank\")\n",
    "    # plt.legend(loc=(1.1,0))\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Loss vs. rank\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e1423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('./cache/memo_may26_zipf_onelayerattention_lr_1e-*_vary_num_hidden_features_num_heads/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffc28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group df by number of heads and number of hidden features\n",
    "_df = pd.DataFrame(df)\n",
    "df_grouped = _df.groupby([\"num_heads\", \"num_hidden_features\"])\n",
    "\n",
    "# take the model with the highest last accuracy for each pair of num_heads and num_hidden_features\n",
    "df_grouped = df_grouped.apply(lambda x: x.loc[x[\"last_accuracy\"].idxmax()])\n",
    "# drop [\"num_heads\", \"num_hidden_features\"] columns \n",
    "df_grouped = df_grouped.drop(columns=[\"num_heads\", \"num_hidden_features\"])\n",
    "df_grouped[\"num_heads\"] = df_grouped[\"num_heads\"].astype(category)\n",
    "display(df_grouped)\n",
    "# convert df_grouped to a pandas dataframe\n",
    "df_grouped = df_grouped.reset_index()\n",
    "display(df_grouped)\n",
    "# plot last accuracy vs. num params\n",
    "sns.lineplot(x=\"num_params\", y=\"last_accuracy\", data=df_grouped, hue=\"num_heads\")\n",
    "plt.title(\"Last accuracy vs. num params\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "# plot last train loss vs. num params\n",
    "sns.lineplot(x=\"num_params\", y=\"last_train_loss\", data=df_grouped, hue=\"num_heads\")\n",
    "plt.title(\"Last train loss vs. num params\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8666e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"num_hidden_features\", y=\"last_accuracy\", data=df, hue=\"num_heads\")\n",
    "plt.title(\"Last accuracy vs. num hidden features\")\n",
    "plt.show()\n",
    "sns.lineplot(x=\"num_hidden_features\", y=\"last_train_loss\", data=df, hue=\"num_heads\")\n",
    "plt.title(\"Last train loss vs. num hidden features\")\n",
    "plt.show()\n",
    "sns.lineplot(x=\"num_params\", y=\"last_accuracy\", data=df, hue=\"num_heads\")\n",
    "plt.title(\"Last accuracy vs. num params\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "sns.lineplot(x=\"num_params\", y=\"last_train_loss\", data=df, hue=\"num_heads\")\n",
    "plt.title(\"Last train loss vs. num params\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "sns.lineplot(x=\"num_params\", y=\"pre_switch_last_accuracy\", data=df, hue=\"num_heads\")\n",
    "plt.title(\"Pre-switch last accuracy vs. num params\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "sns.lineplot(x=\"num_params\", y=\"pre_switch_last_train_loss\", data=df, hue=\"num_heads\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406c0b4",
   "metadata": {},
   "source": [
    "# June 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97116028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "# fname = './cache/memo_may10_zipf_onelayerattention_lr_1e-3/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336.pkl' \n",
    "f = './cache/memo_may26_zipf_onelayerattention_lr_1e-3_vary_num_hidden_features_num_heads/*'\n",
    "df = defaultdict(list)\n",
    "folders = glob(f) + glob('./cache/memo_may26_zipf_onelayerattention_lr_1e-4_vary_num_hidden_features_num_heads/*')\n",
    "for folder in folders:\n",
    "    \n",
    "    subdir = folder.split('/')[-1]\n",
    "    fname = folder + '/' + subdir + '.pkl'\n",
    "    try:\n",
    "        with open(fname, \n",
    "                'rb') as f:\n",
    "            data = utils.CPU_Unpickler(f).load()\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # print(\"args\", data[\"args\"][\"num_mlp_layers\"], [l[\"train_loss\"] for l in data[\"logs\"]])\n",
    "    # plot train loss\n",
    "    # fig, axs = plt.subplots(1, 1, figsize=(5, 6))\n",
    "    # num_iters_per_epoch = 1000\n",
    "    # axs.plot(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch), [i[\"train_loss\"] for i in data[\"logs\"]])\n",
    "    model = data[\"model\"]\n",
    "    \n",
    "    \n",
    "    num_heads = data[\"args\"][\"num_heads\"]\n",
    "    num_hidden_features = data[\"args\"][\"num_hidden_features\"]   \n",
    "    print(\"model\", data[\"args\"][\"lr\"], num_heads, num_hidden_features, np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]))\n",
    "    \n",
    "    # axs.set_title(f\"Train loss: {num_heads} heads, {num_hidden_features} hidden features\")\n",
    "    # axs.set_xlabel(\"Iteration x 1e5\")\n",
    "    # axs.set_ylabel(\"Loss\")\n",
    "    # axs.set_xticks(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch*10))\n",
    "    # axs.set_xticklabels(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch*10)/num_iters_per_epoch)\n",
    "    # plt.show()\n",
    "    logsoftmaxloss = defaultdict(list)\n",
    "    K = data[\"args\"][\"K\"]\n",
    "    pre_switch_last_log = len(data[\"logs\"]) // 2 - 1 \n",
    "    post_switch_last_log = len(data[\"logs\"]) - 1 \n",
    "    last_log = data[\"logs\"][-1]\n",
    "    last_accuracy = last_log[\"test_metrics\"][\"accuracy\"][:10000].mean()\n",
    "    # print(last_log[\"test_metrics\"][\"accuracy\"][9990:10010])\n",
    "    df[\"num_heads\"].append(num_heads)\n",
    "    df[\"num_hidden_features\"].append(num_hidden_features)\n",
    "    df[\"last_accuracy\"].append(last_accuracy)\n",
    "    df[\"last_train_loss\"].append(last_log[\"train_loss\"])\n",
    "    df[\"pre_switch_last_accuracy\"].append(data[\"logs\"][pre_switch_last_log][\"test_metrics\"][\"accuracy\"][:10000].mean())\n",
    "    df[\"pre_switch_last_train_loss\"].append(data[\"logs\"][pre_switch_last_log][\"train_loss\"]) \n",
    "    df[\"num_params\"].append(np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]))\n",
    "    df[\"learning_rate\"].append(data[\"args\"][\"lr\"])\n",
    "    # spacing = 10\n",
    "    # df_num_appearances = defaultdict(list)\n",
    "    # print(len(data[\"logs\"]))\n",
    "    # for log_id, l in enumerate(data[\"logs\"]): \n",
    "    #     logsoftmaxloss_array = l[\"test_metrics\"][\"logsoftmaxloss\"]\n",
    "    #     num_appearances = l[\"num_apppearances\"]\n",
    "    #     accuracy = l[\"test_metrics\"][\"accuracy\"]\n",
    "    #     for rank in [0, 1, 5, 50, 500, 5000]:\n",
    "    #         df_num_appearances[f\"rank\"].append(rank)\n",
    "    #         df_num_appearances[f\"rank_loss\"].append(logsoftmaxloss_array[rank*spacing].mean())\n",
    "    #         df_num_appearances[f\"rank_accuracy\"].append(accuracy[rank*spacing])\n",
    "    #         df_num_appearances[\"num_appearances\"].append(num_appearances[rank*spacing])\n",
    "    #         if log_id <= pre_switch_last_log:\n",
    "    #             df_num_appearances[f\"switch_epoch\"].append(\"preswitch\")\n",
    "    #         else:\n",
    "    #             df_num_appearances[f\"switch_epoch\"].append(\"postswitch\")\n",
    "    # df_num_appearances = pd.DataFrame(df_num_appearances)\n",
    "    # # display(df_num_appearances)\n",
    "    # xax, yax = 2, 3\n",
    "    # fig, axs = plt.subplots(xax, yax, figsize=(12, 6))\n",
    "    # for idx, rank in enumerate([0, 1, 5, 50, 500, 5000]):\n",
    "    #     df_group = df_num_appearances[df_num_appearances[\"rank\"] == rank]\n",
    "    #     sns.lineplot(x=\"num_appearances\", y=\"rank_loss\", hue=\"switch_epoch\", data=df_group, ax=axs[idx//yax, idx%yax])\n",
    "    #     axs[idx//yax, idx%yax].set_title(f\"Rank {rank*spacing}\")\n",
    "    #     axs[idx//yax, idx%yax].set_xlabel(\"Number of appearances\")\n",
    "    #     axs[idx//yax, idx%yax].set_xscale(\"log\")\n",
    "    #     axs[idx//yax, idx%yax].set_ylabel(\"Loss\")\n",
    "    #     axs[idx//yax, idx%yax].set_ylim(0, 0.8)\n",
    "    # plt.title(f\"Loss vs. number of appearances for num_heads={num_heads}, num_hidden_features={num_hidden_features}\")\n",
    "    # plt.show()\n",
    "    #     accuracy = l[\"test_metrics\"][\"accuracy\"] \n",
    "    #     print(\"accuracy\", accuracy.shape)\n",
    "    #     logsoftmaxloss[\"logsoftmaxloss\"].extend(logsoftmaxloss_array)\n",
    "    #     logsoftmaxloss[\"epoch\"].extend([l[\"epoch\"]*num_iters_per_epoch] * len(logsoftmaxloss_array))\n",
    "    #     logsoftmaxloss[\"rank\"].extend(np.arange(0, len(logsoftmaxloss_array), 1)*(K//30))\n",
    "    # fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # logsoftmaxloss = pd.DataFrame(logsoftmaxloss)\n",
    "    # logsoftmaxloss[\"rank\"] = logsoftmaxloss[\"rank\"].astype(\"category\")\n",
    "    # sns.lineplot(x=\"epoch\", y=\"logsoftmaxloss\", data=logsoftmaxloss, ax=ax, hue=\"rank\")\n",
    "    # plt.legend(loc=(1.1,0))\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Loss vs. rank\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93131c10",
   "metadata": {},
   "source": [
    "# June 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d435839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "# fname = './cache/memo_may10_zipf_onelayerattention_lr_1e-3/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336/memo_may10_zipf_onelayerattention_lr_1e-3_transformer_K_1000_L_100_hidden_8_nheads_20_nlayers_4_1746999806.3694336.pkl' \n",
    "f = './cache/memo_june28_fork_progress/*'\n",
    "df = defaultdict(list)\n",
    "folders = glob(f) \n",
    "print(len(folders))\n",
    "for folder in folders:\n",
    "    \n",
    "    subdir = folder.split('/')[-1]\n",
    "    fname = folder + '/' + subdir + '.pkl'\n",
    "    try:\n",
    "        with open(fname, \n",
    "                'rb') as f:\n",
    "            data = utils.CPU_Unpickler(f).load()\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # print(\"args\", data[\"args\"][\"num_mlp_layers\"], [l[\"train_loss\"] for l in data[\"logs\"]])\n",
    "    print(data.keys())\n",
    "    # plot train loss\n",
    "    # fig, axs = plt.subplots(1, 1, figsize=(5, 6))\n",
    "    # num_iters_per_epoch = 1000\n",
    "    # axs.plot(np.arange(0,len(data[\"logs\"])*num_iters_per_epoch,num_iters_per_epoch), [i[\"train_loss\"] for i in data[\"logs\"]])\n",
    "    model = data[\"model\"]\n",
    "    \n",
    "    \n",
    "    num_heads = data[\"args\"][\"num_heads\"]\n",
    "    num_hidden_features = data[\"args\"][\"num_hidden_features\"]   \n",
    "    print(\"model\", data[\"args\"][\"lr\"], num_heads, num_hidden_features, np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]), \n",
    "          \"len logs\", len(data[\"logs\"]), \"epochs\", (data[\"logs\"][-1][\"epoch\"]+1)*50)\n",
    "    \n",
    "    # Too few iterations, skip\n",
    "    if (data[\"logs\"][-1][\"epoch\"]+1)*50 < 20000: \n",
    "        continue\n",
    "    logsoftmaxloss = defaultdict(list)\n",
    "    K = data[\"args\"][\"K\"]\n",
    "    pre_switch_last_log = len(data[\"logs\"]) // 2 - 1 \n",
    "    post_switch_last_log = len(data[\"logs\"]) - 1 \n",
    "    last_log = data[\"logs\"][-1]\n",
    "    last_accuracy = last_log[\"test_metrics\"][\"accuracy\"][:10000].mean()\n",
    "    # print(last_log[\"test_metrics\"][\"accuracy\"][9990:10010])\n",
    "    \n",
    "    # spacing = 10\n",
    "    # df_num_appearances = defaultdict(list)\n",
    "    # print(len(data[\"logs\"]))\n",
    "    for log_id, l in enumerate(data[\"logs\"]): \n",
    "        if \"probe_metrics\" in l:\n",
    "            # print(np.stack(l[\"probe_metrics\"][\"accuracy\"]).flatten())\n",
    "            df[\"num_iters_required\"].append(l[\"probe_metrics\"][\"num_iters_required\"]) \n",
    "            df[\"num_heads\"].append(num_heads)\n",
    "            df[\"num_hidden_features\"].append(num_hidden_features)\n",
    "            df[\"num_parameters\"].append(np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]))\n",
    "            df[\"last_accuracy\"].append(last_accuracy)\n",
    "            df[\"probe_accuracy\"].append(np.stack(l[\"probe_metrics\"][\"accuracy\"]).flatten()[-10:].mean())\n",
    "            df[\"last_train_loss\"].append(last_log[\"train_loss\"])\n",
    "            df[\"pre_switch_last_accuracy\"].append(data[\"logs\"][pre_switch_last_log][\"test_metrics\"][\"accuracy\"][:10000].mean())\n",
    "            df[\"pre_switch_last_train_loss\"].append(data[\"logs\"][pre_switch_last_log][\"train_loss\"]) \n",
    "            df[\"num_params\"].append(np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()]))\n",
    "            df[\"learning_rate\"].append(data[\"args\"][\"lr\"])\n",
    "            df[\"iteration\"].append((l[\"epoch\"]+1)*50) # number of iterations \n",
    "            # plt.plot(np.stack(l[\"probe_metrics\"][\"accuracy\"]).flatten())\n",
    "            # plt.xlabel(\"Probe iteration\")\n",
    "            # plt.ylabel(\"Probe accuracy\")\n",
    "            # plt.title(f\"Probe accuracy for num_parameters={np.sum([torch.prod(torch.tensor(v.shape)).item() for k, v in model.items()])}, num_heads={num_heads}, num_hidden_features={num_hidden_features}, iter={l['epoch']*50}\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e23ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heat map of the num_iters_required vs num_heads and num_hidden_features\n",
    "df_num_iters_required = pd.DataFrame(df)\n",
    "# display(df_num_iters_required)\n",
    "_df_num_iters_required = df_num_iters_required.pivot_table(index=\"num_parameters\", columns=\"iteration\", values=\"probe_accuracy\", aggfunc=\"mean\")\n",
    "display(_df_num_iters_required)\n",
    "sns.heatmap(_df_num_iters_required, annot=False, cmap=\"YlGnBu\")\n",
    "plt.title(\"Probe accuracy vs. num parameters and iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Number of parameters\")\n",
    "# draw a vertical line at iteration 40000\n",
    "plt.axvline(x=40000, color='red', linestyle='--', label='Iteration 40000')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "_df_num_iters_required = df_num_iters_required.pivot_table(index=\"num_parameters\", columns=\"iteration\", values=\"num_iters_required\", aggfunc=\"mean\")\n",
    "sns.heatmap(_df_num_iters_required, annot=False, cmap=\"YlGnBu\")\n",
    "plt.axvline(x=40000, color='red', linestyle='--', label='Iteration 40000')\n",
    "plt.title(\"Number of iterations required vs. num parameters and iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Number of parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5537291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (l2l)",
   "language": "python",
   "name": "l2l"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
