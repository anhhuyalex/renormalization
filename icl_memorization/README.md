# Main_freeze experiments 
Main_freeze.ipynb
Validation: 
We save the state_dict of the model before switching the sequences.
```
c_attn_weight = model.c_attn.weight.detach().clone()
c_attn_bias = model.c_attn.bias.detach().clone() 
```
We generate new sequences with: `train_dataset.generate_sequences()`

We test the model on all sequences in the validation set, both the pre-switch and post-switch sequences
We only test the model on every 10th sequence in the train set

# Analysis of the experiments

A directory e.g. ./cache/memo_aug2_zipf_onelayerattention_lr_1e-3_swapmlp_eval can contain subdirectories storing the results of the experiments.
The results of experiments are stored in a pickle file with the same name as the directory but ending with ".pkl".
There is another pickle file in the same directory with the same name but ending with "_sequences.pkl" that contains the sequences used in the experiments.
You can load the results of the experiments with:
```
with open(f"{dir_name}/{dir_name}.pkl", "rb") as f:
    data = utils.CPU_Unpickler(f).load()
```

You can load the sequences with:
```
with open(f"{dir_name}/{dir_name}_sequences.pkl", "rb") as f:
    sequences = utils.CPU_Unpickler(f).load()
```
Now `sequences` is a dictionary with the following keys: 
* `sequences`: a list of sequence arrays. The length of the list is the number of switches. Each sequence array is of shape (num_sequences, length_of_sequence).
* `switch_start_iter`: the iteration at which the switch happened 


# Phenomenological Model of Memorization 
%\documentclass[preprint, onecolumn,amsmath]{revtex4}
\documentclass[12pt, a4paper, onecolumn, openright]{article}

\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{amsmath}
\usepackage{amsbsy}
%\usepackage{eufrak}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{bbm}
\usepackage{soul}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{xcolor}

%\usepackage{showkeys}
\newcommand\px[1]{{\partial_{#1}}}
\newcommand\ppx[1]{{{\partial\over{\partial {#1}}}}}
\newcommand\pypx[2]{{{{\partial {#1}}\over{\partial {#2}}}}}
\newcommand{\Cal}[1]{{\cal #1}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\p}{\partial}
\newcommand{\upa}{\uparrow}
\newcommand{\down}{\downarrow}
\newcommand{\ket}[1]{\left| #1 \right\rangle}
\newcommand{\ind}[1]{\begin{footnotesize}\mbox{#1}\end{footnotesize}}
\newcommand{\acom}[2]{\left\{#1,#2\right\}}
\newcommand{\com}[2]{\left[#1,#2\right]}
\newcommand{\esp}{\,,\hspace{0.6cm}}
\newcommand{\vm}[1]{\left\langle #1 \right\rangle}
\newcommand{\dr}[1]{\mbox{#1}}
\newcommand{\fr}[1]{\mathfrak{#1}}
\newcommand{\K}{\textsc{k}}
\newcommand{\f}{\textsc{f}}
\newcommand{\R}{\textsc{r}}
\newcommand{\ens}[2]{\left(#1\right)_{#2}}
\newcommand{\sinc}{\text{sinc}}
\def\tK{{\tilde{K}}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\KL}{\mathrm{KL}}

\renewcommand{\L}{\textsc{l}}
\renewcommand{\P}{\textsc{p}}
\def\tI{{\tilde{I}}}
\def\bA{{\bar{A}}}
\def\bB{{\bar{B}}}
\def\bC{{\bar{C}}}
\def\bD{{\bar{D}}}
\def\<{{\langle}}
\def\>{{\rangle}}
\newcommand{\grcomment}[1]{{\textcolor{blue}{\bf [GR Comment: #1]}}}
\def\vp{\varphi}
\def\vpN{\varphi^{(N)}}
\def\vpa{\varphi_{\alpha}}
\def\vpaN{\varphi_{\alpha}^{(N)}}
\def\ve{\varepsilon}
\def\vea{\varepsilon_{\alpha}}
\def\ba{\beta_{\alpha}}
\def\baN{\beta_{\alpha}^{(N)}}
\def\bN{\beta^{(N)}}
\def\cN{c^{(N)}}


\begin{document}

%opening
\title{Memorization}
\author{AN, GR}
\maketitle


\subsection*{Phenomenological Model of Memorization}
 
Suppose our dataset has $K$ sequences, $\xi^{(1)}, \xi^{(2)}, \ldots, \xi^{(K)}$, where the probability of drawing sequence $\alpha$ is $p_{\alpha}$. Each sequence has $N$ bits, where each bit is $\pm 1$. The dataset is generated by sampling each bit in each sequence uniformly and independently. We denote $\xi_{n}^{(\alpha)}$ as the state of the $n$th bit in the $\alpha$th sequence. We will use Greek letters to indicate sequence index and Latin letters to indicate positions within a sequence. 

We would like to model the probability that the transformer correctly predicts the $n+1$th bit of a given sequence $\zeta$, given the prefix $\zeta_{0:n}$, denoted $P(\zeta_{n+1}= +1 | \zeta_{0:n})$. Suppose the model has a dense associative memory (DAM) with a capacity to store at most $M$ binary sequences of length $N$ ($M \gg K$). The $M$ sequences $\chi_1,\chi_2,\dots,\chi_M$ are stored in the DAM as $H$-dimensional vectors  $\varphi_1,\varphi_2,\dots,\varphi_M$, where $\varphi_{\mu} = B \chi_{\mu}$  for $\mu = 1,2,\dots, M$. The $H\times N$ matrix $B$ is normalized such that all its elements are non-negative and the elements of each row sum to one. The state of the DAM at a particular time $t$ is summarized by the set of sequences stored in it, $\mathcal{M}_t = (\chi^{(1)},\chi^{(2)},\dots,\chi^{(M)})$. The procedure for updating the DAM is discussed further below. 

Given $\zeta_{0:n}$, we assume the model computes the $H$ dimensional vector $\hat{\varphi}$:
\begin{align}
\hat{\varphi} = \sum_{i=1}^n A_{i}(n) \zeta_i,
\end{align}
where $A(n)$ is a $H \times n$ matrix such that $A_i(n) \ge 0$ and $\sum_{i = 1}^n A_i(n) = \bm{1}$, where $\bm{1}$ is a length $H$ vector of all ones. One can view each row of $A_i(n)$ as a position-specific attention map.  This model has $H$ attention heads. The $n$ dependence is included to highlight that the attention maps are in general dependent on the length of the sequence unmasked thus far. 

\textbf{Predicting the next bit:} Given the partial sequence $\zeta_{0:n}$, the probability of retrieving sequence $\mu$ from the DAM is 
\begin{align}
\pi_{\mu} = \frac{e^{\eta \hat{\varphi}^T \varphi_{\mu} }}{\sum_{\mu'} e^{\eta \hat{\varphi}^T \varphi_{\mu'} }} = \frac{e^{\eta \zeta_{0:n}^T A(n)^T B \chi_{\mu} }}{\sum_{\mu'} e^{\eta \zeta_{0:n}^T A(n)^T B \chi_{\mu'}  }},
\end{align}
where $\eta$ is a fixed positive constant. The probability that the model assigns to the $n+1$th bit being $+1$ is given by
\begin{align}
P(\zeta_{n+1}= +1 | \zeta_{0:n}) = \sum_{\mu \in M_{n+1}^{+}} \pi_{\mu},
\end{align}
where $M_n^{+}$ is the set of sequences in the DAM that have $+1$ at the $n$th position. The interpretation is that the model first computes a probability distribution $\pi$ over sequences in the DAM based on their match with the observed partial sequence $\zeta_{0:n}$, and predicts the next bit by computing the total probability of those sequences that have a $+1$ bit in the $n+1$th location. That is, this is a two-step Bayes-like procedure, where a Bayesian posterior over possible sequences is replaced by a distribution generated from one DAM retrieval step. 

\textbf{Updating the DAM:} The DAM is structured as a queue. That is, once the full sequence $\zeta$ is observed, the DAM is updated by adding $\zeta$ as the last element. If the queue is full (i.e., it has $M$ sequences in it), the first element is removed when $\zeta$ is added. 

\textbf{Updating $A,B$:} This is a continual learning setup. If sequence $\xi^{(\alpha)}$ is drawn (with probability $p_{\alpha}$), the loss at step $t$ during training is
\begin{align}
\mathcal{L}_t(\alpha; A(n), B, \mathcal{M}_t) = \left( \frac{1}{N} \sum_{n=0}^{N-1} \mathcal {L}_{\text{BCE}}( {\alpha,n} ; A(n), B, \mathcal{M}_t)\right),
\end{align} where $\mathcal {L}_{\text{BCE}}(\alpha, n; A(n), B, \mathcal{M}_t)$ is the binary cross-entropy loss of predicting the $n+1$th bit of sequence $\xi^{(\alpha)}$ given $A(n), B$ and the current state of the DAM $\mathcal{M}_t$. In practice, it is reasonable to compute the loss over a batch of sequences and add these sequences together to the DAM after the update. 

\end{document}

## Implementation notes
- Initialization of DAM: $\mathcal{M}_0 = \vec 0$
- If no memory has been added, then we return 0.5 for both classes.
- If no bit has been revealed, i.e. $n=0$:
    - If memory has been added, then we return the probability of the first bit being +1.
-  The $M$ sequences $\chi_1, \chi_2, \ldots, \chi_M$ are stored in the DAM as $H$ dimensional vectors $\varphi_1, \varphi_2, \ldots, \varphi_M$, where $\varphi_\mu=B \chi_\mu$ for $\mu=1,2, \ldots, M$.
    - I assumed $\xi$ is a binary sequence, so $B$ is a $H \times N$ matrix and $\chi_\mu$ is a $N \times 1$-dimensional binary vector.
    - So $\varphi_\mu$ is a $H \times 1$-dimensional real vector.