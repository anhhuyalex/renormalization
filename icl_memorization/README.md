# Main_freeze experiments 
Main_freeze.ipynb
Validation: 
We save the state_dict of the model before switching the sequences.
```
c_attn_weight = model.c_attn.weight.detach().clone()
c_attn_bias = model.c_attn.bias.detach().clone() 
```
We generate new sequences with: `train_dataset.generate_sequences()`

We test the model on all sequences in the validation set, both the pre-switch and post-switch sequences
We only test the model on every 10th sequence in the train set

# Analysis of the experiments

A directory e.g. ./cache/memo_aug2_zipf_onelayerattention_lr_1e-3_swapmlp_eval can contain subdirectories storing the results of the experiments.
The results of experiments are stored in a pickle file with the same name as the directory but ending with ".pkl".
There is another pickle file in the same directory with the same name but ending with "_sequences.pkl" that contains the sequences used in the experiments.
You can load the results of the experiments with:
```
with open(f"{dir_name}/{dir_name}.pkl", "rb") as f:
    data = utils.CPU_Unpickler(f).load()
```

You can load the sequences with:
```
with open(f"{dir_name}/{dir_name}_sequences.pkl", "rb") as f:
    sequences = utils.CPU_Unpickler(f).load()
```
Now `sequences` is a dictionary with the following keys: 
* `sequences`: a list of sequence arrays. The length of the list is the number of switches. Each sequence array is of shape (num_sequences, length_of_sequence).
* `switch_start_iter`: the iteration at which the switch happened 


# Phenomenological Model of Memorization 
%\documentclass[preprint, onecolumn,amsmath]{revtex4}
\documentclass[12pt, a4paper, onecolumn, openright]{article}

\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{amsmath}
\usepackage{amsbsy}
%\usepackage{eufrak}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{bbm}
\usepackage{soul}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{xcolor}

%\usepackage{showkeys}
\newcommand\px[1]{{\partial_{#1}}}
\newcommand\ppx[1]{{{\partial\over{\partial {#1}}}}}
\newcommand\pypx[2]{{{{\partial {#1}}\over{\partial {#2}}}}}
\newcommand{\Cal}[1]{{\cal #1}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\p}{\partial}
\newcommand{\upa}{\uparrow}
\newcommand{\down}{\downarrow}
\newcommand{\ket}[1]{\left| #1 \right\rangle}
\newcommand{\ind}[1]{\begin{footnotesize}\mbox{#1}\end{footnotesize}}
\newcommand{\acom}[2]{\left\{#1,#2\right\}}
\newcommand{\com}[2]{\left[#1,#2\right]}
\newcommand{\esp}{\,,\hspace{0.6cm}}
\newcommand{\vm}[1]{\left\langle #1 \right\rangle}
\newcommand{\dr}[1]{\mbox{#1}}
\newcommand{\fr}[1]{\mathfrak{#1}}
\newcommand{\K}{\textsc{k}}
\newcommand{\f}{\textsc{f}}
\newcommand{\R}{\textsc{r}}
\newcommand{\ens}[2]{\left(#1\right)_{#2}}
\newcommand{\sinc}{\text{sinc}}
\def\tK{{\tilde{K}}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\KL}{\mathrm{KL}}

\renewcommand{\L}{\textsc{l}}
\renewcommand{\P}{\textsc{p}}
\def\tI{{\tilde{I}}}
\def\bA{{\bar{A}}}
\def\bB{{\bar{B}}}
\def\bC{{\bar{C}}}
\def\bD{{\bar{D}}}
\def\<{{\langle}}
\def\>{{\rangle}}
\newcommand{\grcomment}[1]{{\textcolor{blue}{\bf [GR Comment: #1]}}}
\def\vp{\varphi}
\def\vpN{\varphi^{(N)}}
\def\vpa{\varphi_{\alpha}}
\def\vpaN{\varphi_{\alpha}^{(N)}}
\def\ve{\varepsilon}
\def\vea{\varepsilon_{\alpha}}
\def\ba{\beta_{\alpha}}
\def\baN{\beta_{\alpha}^{(N)}}
\def\bN{\beta^{(N)}}
\def\cN{c^{(N)}}


\begin{document}

%opening
\title{Memorization}
\author{AN, GR}
\maketitle


\subsection*{Phenomenological Model of Memorization}
 
Suppose our dataset has $K$ sequences, $\xi^{(1)}, \xi^{(2)}, \ldots, \xi^{(K)}$, where the probability of drawing sequence $\alpha$ is $p_{\alpha}$. Each sequence has $N$ bits, where each bit is $\pm 1$. The dataset is generated by sampling each bit in each sequence uniformly and independently. We denote $\xi_{n}^{(\alpha)}$ as the state of the $n$th bit in the $\alpha$th sequence. We will use Greek letters to indicate sequence index and Latin letters to indicate positions within a sequence. 

We would like to model the probability that the transformer correctly predicts the $n+1$th bit of a given sequence $\zeta$, given the prefix $\zeta_{0:n}$, denoted $P(\zeta_{n+1}= +1 | \zeta_{0:n})$. Suppose the model has a dense associative memory (DAM) with a capacity to store at most $M$ binary sequences of length $N$ ($M \gg K$). The $M$ sequences $\chi_1,\chi_2,\dots,\chi_M$ are stored in the DAM as $H$-dimensional vectors  $\varphi_1,\varphi_2,\dots,\varphi_M$, where $\varphi_{\mu} = B \chi_{\mu}$  for $\mu = 1,2,\dots, M$. The $H\times N$ matrix $B$ is normalized such that all its elements are non-negative and the elements of each row sum to one. The state of the DAM at a particular time $t$ is summarized by the set of sequences stored in it, $\mathcal{M}_t = (\chi^{(1)},\chi^{(2)},\dots,\chi^{(M)})$. The procedure for updating the DAM is discussed further below. 

Given $\zeta_{0:n}$, we assume the model computes the $H$ dimensional vector $\hat{\varphi}$:
\begin{align}
\hat{\varphi} = \sum_{i=1}^n A_{i}(n) \zeta_i,
\end{align}
where $A(n)$ is a $H \times n$ matrix such that $A_i(n) \ge 0$ and $\sum_{i = 1}^n A_i(n) = \bm{1}$, where $\bm{1}$ is a length $H$ vector of all ones. One can view each row of $A_i(n)$ as a position-specific attention map.  This model has $H$ attention heads. The $n$ dependence is included to highlight that the attention maps are in general dependent on the length of the sequence unmasked thus far. 

\textbf{Predicting the next bit:} Given the partial sequence $\zeta_{0:n}$, the probability of retrieving sequence $\mu$ from the DAM is 
\begin{align}
\pi_{\mu} = \frac{e^{\eta \hat{\varphi}^T \varphi_{\mu} }}{\sum_{\mu'} e^{\eta \hat{\varphi}^T \varphi_{\mu'} }} = \frac{e^{\eta \zeta_{0:n}^T A(n)^T B \chi_{\mu} }}{\sum_{\mu'} e^{\eta \zeta_{0:n}^T A(n)^T B \chi_{\mu'}  }},
\end{align}
where $\eta$ is a fixed positive constant. The probability that the model assigns to the $n+1$th bit being $+1$ is given by
\begin{align}
P(\zeta_{n+1}= +1 | \zeta_{0:n}) = \sum_{\mu \in M_{n+1}^{+}} \pi_{\mu},
\end{align}
where $M_n^{+}$ is the set of sequences in the DAM that have $+1$ at the $n$th position. The interpretation is that the model first computes a probability distribution $\pi$ over sequences in the DAM based on their match with the observed partial sequence $\zeta_{0:n}$, and predicts the next bit by computing the total probability of those sequences that have a $+1$ bit in the $n+1$th location. That is, this is a two-step Bayes-like procedure, where a Bayesian posterior over possible sequences is replaced by a distribution generated from one DAM retrieval step. 

\textbf{Updating the DAM:} The DAM is structured as a queue. That is, once the full sequence $\zeta$ is observed, the DAM is updated by adding $\zeta$ as the last element. If the queue is full (i.e., it has $M$ sequences in it), the first element is removed when $\zeta$ is added. 

\textbf{Updating $A,B$:} This is a continual learning setup. If sequence $\xi^{(\alpha)}$ is drawn (with probability $p_{\alpha}$), the loss at step $t$ during training is
\begin{align}
\mathcal{L}_t(\alpha; A(n), B, \mathcal{M}_t) = \left( \frac{1}{N} \sum_{n=0}^{N-1} \mathcal {L}_{\text{BCE}}( {\alpha,n} ; A(n), B, \mathcal{M}_t)\right),
\end{align} where $\mathcal {L}_{\text{BCE}}(\alpha, n; A(n), B, \mathcal{M}_t)$ is the binary cross-entropy loss of predicting the $n+1$th bit of sequence $\xi^{(\alpha)}$ given $A(n), B$ and the current state of the DAM $\mathcal{M}_t$. In practice, it is reasonable to compute the loss over a batch of sequences and add these sequences together to the DAM after the update. 

\end{document}

## Implementation notes
- Initialization of DAM: $\mathcal{M}_0 = \vec 0$
- If no memory has been added, then we return 0.5 for both classes.
- If no bit has been revealed, i.e. $n=0$:
    - If memory has been added, then we return the probability of the first bit being +1.
-  The $M$ sequences $\chi_1, \chi_2, \ldots, \chi_M$ are stored in the DAM as $H$ dimensional vectors $\varphi_1, \varphi_2, \ldots, \varphi_M$, where $\varphi_\mu=B \chi_\mu$ for $\mu=1,2, \ldots, M$.
    - I assumed $\xi$ is a binary sequence, so $B$ is a $H \times N$ matrix and $\chi_\mu$ is a $N \times 1$-dimensional binary vector.
    - So $\varphi_\mu$ is a $H \times 1$-dimensional real vector.
=======
# Transformer Memorization Analysis

This project analyzes how transformer models memorize sequences with different frequencies in a Zipfian distribution. It provides tools to visualize the relationship between sequence frequency (number of presentations) and model performance (loss and accuracy).

## Overview

When training language models, some tokens or sequences appear much more frequently than others, following a Zipfian distribution. This analysis explores how a model's ability to memorize sequences correlates with their frequency in the training data.

This analysis pipeline:

1. Processes experiment data from pickled training logs
2. Extracts performance metrics for each sequence based on its frequency
3. Generates visualizations showing the relationship between:
   - Number of presentations vs. Log Loss
   - Number of presentations vs. Accuracy

## Features

- **Data Caching**: Processed data is cached to avoid redundant computations
- **Configurable Analysis**: Easily adjust visualization parameters
- **Comparative Analysis**: Compare performance across different model architectures
- **Command Line Interface**: Run analyses with different configurations via CLI arguments

## Usage

### Basic Usage

To run the basic analysis on the small model with default settings:

```bash
python analysis.py
```

### Command Line Options

```bash
python analysis.py --experiment all --rank-selection low --comparative
```

Available options:

- `--experiment`: Which experiment configuration to analyze (choices: small, medium, large_rope, large_lr, all)
- `--rank-selection`: Which rank selection to use for visualizations (choices: low, mid, high, selected)
- `--force-reprocess`: Force reprocessing of data even if cache exists
- `--no-show-plots`: Don't display plots during execution
- `--no-save-plots`: Don't save plots to files
- `--comparative`: Generate comparative visualizations across model architectures

### Configuration

Edit `config.py` to modify:

- Experiment paths
- Plot configuration (figure size, limits, etc.)
- Rank selections for visualization
- Default analysis settings

## Project Structure

- `analysis.py`: Main analysis script
- `config.py`: Configuration settings for the analysis
- `cache/`: Directory for experiment data and processed results
  - `zipf_figs/`: Output directory for generated figures
  - `processed_data/`: Cache directory for processed data

## Visualizations

The pipeline generates several types of visualizations:

1. **Loss vs. Presentations (line and scatter plots)**: Shows how log loss decreases as the model sees sequences more frequently
2. **Accuracy vs. Presentations**: Shows how accuracy increases with more presentations
3. **Comparative Visualizations** (when enabled):
   - Model comparisons
   - Architecture comparisons (number of heads, layers, embedding types)

## Performance Tips

- Use the caching system to avoid reprocessing data
- For large experiments, use the `--no-show-plots` option and view the saved figures later
- Select appropriate rank subsets to reduce visual complexity in plots

## Requirements

- Python 3.7+
- NumPy
- Pandas
- Matplotlib
- Seaborn

## References

For more information on the Zipfian distribution and how it relates to language model memorization, see:
- [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)
- Research papers on transformer memorization capabilities 

# Phenomenological Model of Memorization 
Suppose that we have $K$ sequences where each sequence $\boldsymbol{\xi} = \xi_1, \xi_2, \ldots, \xi_{N}$ has $N_i$ bits. Here we note that $\xi_i$ is the content embedding of the $i$th bit in the sequence. We want to model the probability that the transformer correctly predicts the $j+1$th bit of sequence $\boldsymbol{\xi}$, given the prefix $\boldsymbol{\xi}_{0..j}$. 

We suppose that the model generates a representation $\boldsymbol{v}$ of the sequence $\boldsymbol{\xi}$. This representation is equivalent to a vector $v_1, v_2, \ldots, v_N$ where each vector $v_i\in \mathbb{R}^d$ is a vector of $d$ features. 

Since the model is autoregressive, this representation $v_i$ for the $n$th bit is only a function of the prefix $\boldsymbol{\xi}_{0..i-1}$. Therefore, we can write $$v_i = \sum_{i=1}^{n-1} A^{(n)}_i \xi_i^{(i)}$$ where $v_i^{(i)}$ is the representation of the $i$th bit of the $i$th sequence.

The probability that the model assigns to the $n+1$-th bit being positive is given by $$P(\boldsymbol{\xi}_{n+1}=+1 | \boldsymbol{\xi}_{0..j}) = \sum_{m,n}p_m\sigma(v_j \cdot B)$$ where $\sigma$ is the sigmoid function.
The loss function is then given by $$L = -\frac{1}{MN}\sum_{m=1}^{M}\sum_{n=0}^{N-1} \mathcal {L}_{BCE}( {m,n} ; A^{(n)}, B)$$ where $\mathcal {L}_{BCE}$ is the binary cross-entropy loss.
Here $A^{(n)}$ is the matrix of the attention weights for the $n$th bit and $B$ is the matrix corresponding to the query vector.