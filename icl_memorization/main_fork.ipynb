{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR, OneCycleLR\n",
    "from torch.utils.data import Subset\n",
    "import attention\n",
    "# import webdataset as wds\n",
    "\n",
    "import datetime\n",
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import einops\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import wandb \n",
    "import sys \n",
    "import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--data ./cache --fileprefix transformer --SLURM_ARRAY_TASK_ID 0 --batch-size 256 --optimizer SGD --lr 0.1 --wd 0.0  --num_iters 500000 --arch gpt --gpt_bias True --num_hidden_features 8 --num_layers 4 --len_context 100 --K 1000 --sequence_sampling_distribution uniform --no-wandb_log --wandb_project l2l --wandb_group_name t\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "LOCAL RANK  0\n",
      "args:\n",
      " {'data': './cache', 'cache': './cache', 'wandb_log': False, 'wandb_project': 'l2l', 'wandb_group_name': 't', 'seed': 562398, 'num_iter_per_epoch': 90, 'num_iters': 500000, 'batch_size': 256, 'workers': 4, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0, 'arch': 'gpt', 'gpt_bias': 'True', 'num_hidden_features': 8, 'num_layers': 4, 'num_heads': 1, 'len_context': 100, 'SLURM_ARRAY_TASK_ID': 0, 'no_cuda': False, 'K': 1000, 'sequence_sampling_distribution': 'uniform', 'fileprefix': 'transformer', 'n_heads': 1, 'n_layers': 1}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='GMM L2L Training with Sequence Model')\n",
    "parser.add_argument('--data', metavar='DIR', nargs='?', default='./data',\n",
    "                    help='path to dataset (default: imagenet)')\n",
    "parser.add_argument('--cache', default='./cache',\n",
    "                    help='path to cached files (e.g. for previous random weights)')\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_group_name\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument('--seed', default=None, type=int,\n",
    "                    help='seed for initializing training.')\n",
    "parser.add_argument('--train_seed', default=None, type=int,\n",
    "                    help='seed for training dataset.')\n",
    "parser.add_argument('--probe_seed', default=None, type=int,\n",
    "                    help='seed for probe dataset.')\n",
    "parser.add_argument('--num_iter_per_epoch', default=100, type=int,  \n",
    "                    help='number of iters per epoch')\n",
    "parser.add_argument('--num_iters', default=5e5, type=int,  \n",
    "                    help='number of iters')\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')                         \n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--optimizer', default='SGD', type=str, \n",
    "                    choices = ['SGD', 'Adam'],\n",
    "                    help='optimizer')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--wd', '--weight-decay', default=1e-5, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)',\n",
    "                    dest='weight_decay')\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='mlp',\n",
    "                    help='model architecture (default: mlp)')\n",
    "parser.add_argument('--gpt_bias', default=\"True\", type=str,\n",
    "                    help='whether to include bias in GPT')\n",
    "parser.add_argument('--is_initialize_attention_weights_to_zero', default=\"False\", type=str,\n",
    "                    help='whether to initialize attention weights to zero')\n",
    "parser.add_argument('--num_hidden_features', default=1, type=int,\n",
    "                    help='num_hidden_features')\n",
    "parser.add_argument('--num_layers', default=1, type=int,\n",
    "                    help='num_layers in transformer')\n",
    "parser.add_argument('--num_heads', default=1, type=int,\n",
    "                    help='num_heads in transformer')\n",
    "parser.add_argument('--num_mlp_layers', default=3, type=int,\n",
    "                    help='num_mlp_layers in transformer')\n",
    "parser.add_argument('--len_context', default=1, type=int,\n",
    "                    help='number of in-context images in sequence')\n",
    "parser.add_argument('--SLURM_ARRAY_TASK_ID', default=1, type=int,\n",
    "                    help='SLURM_ARRAY_TASK_ID')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')  \n",
    " \n",
    "parser.add_argument('--K', default=1, type=int, \n",
    "                    help='number of tasks')\n",
    "parser.add_argument('--sequence_sampling_distribution', type=str,\n",
    "                    default=\"uniform\", \n",
    "                    choices = [\"uniform\", \"zipf\"],\n",
    "                    help='sequence_sampling_distribution')\n",
    "parser.add_argument('--is_resample_tasks', default=\"False\", type=str,\n",
    "                    help='whether to resample tasks')\n",
    "parser.add_argument('--is_probe', default=\"True\", type=str,\n",
    "                    help='whether to probe the model')\n",
    "parser.add_argument('--num_probe_sequences', default=1000, type=int,\n",
    "                    help='number of probe sequences')\n",
    "parser.add_argument(\n",
    "            '--fileprefix', \n",
    "            default=\"\",\n",
    "            type=str, \n",
    "            action='store') \n",
    "parser.add_argument('--resume', type=str, default=None,\n",
    "                    help='resume from checkpoint')\n",
    "    \n",
    "\n",
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    arch = \"pytorch_transformer\"\n",
    "    # arch = \"transformer\"\n",
    "    SLURM_ARRAY_TASK_ID = 0 \n",
    "    optimizer = \"SGD\"\n",
    "    lr = 1e-3\n",
    "    num_iters = int(5e5 )\n",
    "    gpt_bias = \"True\"\n",
    "    len_context = 100\n",
    "    K = 1000\n",
    "    sequence_sampling_distribution = \"uniform\"\n",
    "    jupyter_args = f\"--data ./cache --fileprefix transformer --SLURM_ARRAY_TASK_ID ${SLURM_ARRAY_TASK_ID} --batch-size 256 --optimizer ${optimizer} --lr ${lr} --wd 0.0  --num_iters ${num_iters} --arch gpt --gpt_bias ${gpt_bias} --num_hidden_features 8 --num_layers 4 --len_context ${len_context} --K ${K} --sequence_sampling_distribution ${sequence_sampling_distribution} --no-wandb_log --wandb_project l2l --wandb_group_name t\"\n",
    "    # replace $ with '' in jupyter_args \n",
    "    jupyter_args = jupyter_args.replace(\"$\",\"\")\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 \n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    " \n",
    "    \n",
    "def set_zipf_exp_one_layer_attention(args, lr, num_hidden_features, num_heads, vocab_size):\n",
    "    args.arch = \"OneLayerAttention\" \n",
    "    # args.arch = \"gpt\"\n",
    "    args.len_context = 100\n",
    "    args.num_hidden_features = num_hidden_features\n",
    "    args.num_heads = num_heads\n",
    "    args.vocab_size = vocab_size\n",
    "    \n",
    "    args.is_resample_tasks = \"True\"\n",
    "    args.num_iters = 40000 \n",
    "    args.sequence_sampling_distribution = \"zipf\"\n",
    "    args.K = 100000\n",
    "    \n",
    "    args.lr = lr\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if args.wandb_group_name == \"memo_june28_fork_progress\":\n",
    "    # num_hidden_features is log spaced from 2 to 512\n",
    "    args.train_seed = 42\n",
    "    args.probe_seed = 43\n",
    "    \n",
    "    num_hidden_features_list = 2 ** np.linspace(0, 9, 10)\n",
    "    num_hidden_features = int(num_hidden_features_list[args.SLURM_ARRAY_TASK_ID % len(num_hidden_features_list)])\n",
    "    num_heads_list = [1] + list(np.arange(2, 24, 4)) # length: 7\n",
    "    num_heads = int(num_heads_list[args.SLURM_ARRAY_TASK_ID // len(num_hidden_features_list)])\n",
    "    set_zipf_exp_one_layer_attention(args, 1e-3, num_hidden_features, num_heads, int(2))  \n",
    "\n",
    "# assert args.K % args.L == 0, \"K must be divisible by L\"\n",
    "if args.seed is None:\n",
    "    args.seed = np.random.randint(0, 10000000)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Local Rank for distributed training\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)\n",
    "print(\"args:\\n\",vars(args))\n",
    "# setup weights and biases (optional)\n",
    "if local_rank==0 and args.wandb_log: # only use main process for wandb logging\n",
    "    print(f\"wandb {args.wandb_project} run\")\n",
    "    wandb_model_name = f\"{args.fileprefix}\"\n",
    "    wandb_config = vars(args)\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\" # turn off wandb logging for now\n",
    "    os.environ[\"WANDB_API_KEY\"] = \"a421cbcaff87506c1eadd3b9e4d6424996432e38\"\n",
    "    print(\"wandb_id:\",wandb_model_name)\n",
    "    wandb.login(relogin=True, key = os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(\n",
    "        project=args.wandb_project,\n",
    "        name=wandb_model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "        group=args.wandb_group_name\n",
    "    )\n",
    "    wandb.config.local_file_dir = wandb.run.dir \n",
    "else:\n",
    "    record = {\n",
    "        \"args\": vars(args),\n",
    "        \"logs\": []\n",
    "    }\n",
    "if args.resume is not None:\n",
    "    with open(args.resume, 'rb') as f:\n",
    "        record = pickle.load(f)\n",
    "    args = record['args']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, K,   \n",
    "                 len_context = 1,\n",
    "                len_data = 60000, skip=False,\n",
    "                sequence_sampling_distribution = \"uniform\",\n",
    "                seed = None):\n",
    "\n",
    "        # if K < 40000:\n",
    "        self.len_context = len_context\n",
    "        self.K = K \n",
    "        self.len_data = len_data\n",
    "        self.sequence_sampling_distribution = sequence_sampling_distribution\n",
    "        self.seed = seed\n",
    "        if skip == False:\n",
    "            self.generate_sequences()\n",
    "        if args.sequence_sampling_distribution == \"zipf\":\n",
    "            self.p = 1.0 / np.arange(1, self.K + 1)\n",
    "            self.p /= np.sum(self.p)\n",
    "        else:\n",
    "            assert self.sequence_sampling_distribution in [\"uniform\", \"probe_uniform\"], f\"sequence_sampling_distribution must be uniform or probe_uniform, got {self.sequence_sampling_distribution}\"\n",
    "        self.skip = skip\n",
    "        \n",
    "    def generate_sequences(self):\n",
    "        \"\"\"\n",
    "        Generate sequences from the dataset.\n",
    "        If seed is not None, use the seed to generate the sequences.\n",
    "        Otherwise, use the default random generator.\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            rng = torch.Generator()\n",
    "            rng.manual_seed(self.seed)\n",
    "            self.sequences = torch.randint(0, 2, (self.K, self.len_context), generator=rng) \n",
    "            print (\"sequences\", self.sequence_sampling_distribution, self.sequences[:10])\n",
    "        else:\n",
    "            self.sequences = torch.randint(0, 2, (self.K, self.len_context)) \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, task: int):\n",
    "        if (self.sequence_sampling_distribution == \"uniform\") or (self.skip == True):\n",
    "            i = task % self.K\n",
    "        elif self.sequence_sampling_distribution == \"probe_uniform\":\n",
    "            i = np.random.choice(self.K)\n",
    "        elif self.sequence_sampling_distribution == \"zipf\":\n",
    "            i = np.random.choice(self.K, p= self.p)\n",
    "        samples = self.sequences[i]\n",
    "        # samples = torch.randint(0, 2, (self.len_context,))\n",
    "        return samples.type(torch.long), i \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1722\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(gpt)\n",
    "import gpt\n",
    "criterion = nn.NLLLoss(reduction=\"none\")\n",
    "# define the model, optimizer, and scheduler, and criterion\n",
    "if args.arch == \"gpt\":\n",
    "    import gpt \n",
    "    config = gpt.GPTConfig(\n",
    "        block_size = args.len_context,\n",
    "        n_embd=args.num_heads * args.num_hidden_features,\n",
    "        n_layer=args.num_layers,\n",
    "        n_head=args.num_heads,\n",
    "        bias = args.gpt_bias == \"True\",\n",
    "        is_initialize_attention_weights_to_zero = args.is_initialize_attention_weights_to_zero \n",
    "    )\n",
    "    get_model = lambda: gpt.GPT(config, criterion).to(device)\n",
    "    model = get_model()\n",
    "elif args.arch == \"OneLayerAttention\":\n",
    "    import gpt\n",
    "    # /jukebox/norman/qanguyen/renormalization/icl_memorization/gpt.py\n",
    "    # arguments: len_context, num_heads, num_hidden_features, vocab_size, num_mlp_layers)\n",
    "    get_model = lambda: gpt.OneLayerAttention(args.len_context, \n",
    "                                  args.num_heads, \n",
    "                                  args.num_hidden_features, \n",
    "                                  args.vocab_size, \n",
    "                                  args.num_mlp_layers).to(device)\n",
    "    model = get_model()\n",
    "if args.optimizer == 'SGD': \n",
    "    optimizer = torch.optim.SGD(model.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "else:\n",
    "    raise ValueError(\"optimizer not recognized\")\n",
    "# optimizer = get_optimizer(model.parameters())\n",
    "iters_per_epoch = 1000\n",
    "# scheduler = StepLR(optimizer, step_size=50, gamma=0.7)\n",
    "# scheduler = OneCycleLR(optimizer, max_lr=args.lr, \n",
    "#                        total_steps=args.epochs * iters_per_epoch, \n",
    "#                        pct_start=0.5,\n",
    "#                        steps_per_epoch=iters_per_epoch, epochs=args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences.sequence_sampling_distribution uniform\n"
     ]
    }
   ],
   "source": [
    "# define the dataset\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.batch_size}\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': args.workers,\n",
    "                    \"shuffle\": True,\n",
    "                    'pin_memory': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "train_dataset = Sequence(K=args.K, \n",
    "                         len_data = 1000 * args.num_iter_per_epoch,\n",
    "                         len_context=args.len_context,\n",
    "                         sequence_sampling_distribution = args.sequence_sampling_distribution,\n",
    "                         seed = args.train_seed\n",
    "                         )\n",
    "# IWL dataset is for testing the model on every 10th sequence from train_dataset\n",
    "iwl_dataset = Sequence(K=args.K, len_context=args.len_context,  skip=True, \n",
    "                       len_data = args.K // 10,\n",
    "                       sequence_sampling_distribution = \"uniform\")\n",
    "iwl_dataset.sequences = train_dataset.sequences[::10] # take every 10th sequence from train_dataset\n",
    "print (\"sequences.sequence_sampling_distribution\", iwl_dataset.sequence_sampling_distribution)\n",
    "\n",
    "# Create probe sequences independent of train_dataset\n",
    "probe_dataset = Sequence(K=args.num_probe_sequences, len_context=args.len_context,  skip=False, \n",
    "                       len_data = 2 ** 20, # infinite data for now, break when threshold accuracy is reached\n",
    "                       sequence_sampling_distribution = \"probe_uniform\",\n",
    "                       seed = args.probe_seed)\n",
    "\n",
    "train_sampler = None\n",
    "val_sampler = None \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            sampler=train_sampler, \n",
    "                                            **train_kwargs)  \n",
    "# At test time we don't shuffle and sample uniformly\n",
    "iwl_test_loader = torch.utils.data.DataLoader(iwl_dataset,\n",
    "                                            sampler=val_sampler,\n",
    "                                            **{'batch_size': args.batch_size, 'num_workers': args.workers,\n",
    "                                        \"shuffle\": False,\n",
    "                                        'pin_memory': True})\n",
    "\n",
    "print (\"sequences.sequence_sampling_distribution\", probe_dataset.sequence_sampling_distribution)\n",
    "probe_sampler = None\n",
    "probe_loader = torch.utils.data.DataLoader(probe_dataset, \n",
    "                                            sampler=probe_sampler, \n",
    "                                            **{'batch_size': args.batch_size, 'num_workers': args.workers,\n",
    "                                               \"shuffle\": True,\n",
    "                                               'pin_memory': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test dataset construction \n",
    "# ihistogram = []\n",
    "# for _, (seq, i) in enumerate(train_loader):\n",
    "#     ihistogram.extend(i.tolist()) \n",
    "#     if _ > 100: break \n",
    "# # print (\"train_dataset\", train_dataset.p, train_dataset.K)\n",
    "# print (\"ihistogram\",ihistogram)``\n",
    "# import matplotlib.pyplot as plt\n",
    "# # plot histogram, should be zipf distribution\n",
    "# # plt.hist(ihistogram, bins = 20)\n",
    "# plt.bar(np.arange(args.K), np.bincount(ihistogram, minlength=args.K))\n",
    "# plt.plot(np.arange(args.K), train_dataset.p * len(ihistogram), \"r\")\n",
    "# # plt.xlim(0, 30)\n",
    "# plt.semilogy()\n",
    "# plt.title(\"Histogram of tasks in training set, should be zipf distribution\")\n",
    "# plt.legend([\"expected: p(zipf) * n_samps\", \"empirical\"])\n",
    "# plt.xlabel(\"seq rank\")\n",
    "# plt.ylabel(\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_gradient_descent_zipf(epoch, val_loader, model, args, criterion, device):\n",
    "    # seq_lens = list(range(1, args.len_context+1, 5)) \n",
    "   \n",
    "    sequence_rank = []\n",
    "    test_losses = [] \n",
    "    test_top1 = []\n",
    "    test_metrics = defaultdict(list)\n",
    "    model.eval() # switch to eval mode\n",
    "    sequence_ranks = torch.zeros((args.K,args.len_context-1), dtype=torch.long).to(device)\n",
    "    lengths = torch.zeros((args.K,args.len_context-1), dtype=torch.long).to(device)\n",
    "    logsoftmaxlosses = torch.zeros((args.K,args.len_context-1), dtype=torch.float).to(device) \n",
    "    accuracys = torch.zeros((args.K,args.len_context-1), dtype=torch.float).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, (seq, task) in enumerate(val_loader):\n",
    "            seq, task = seq.to(device), task.to(device) \n",
    "            \n",
    "            # print(\"seq\", seq.shape, \"task\", task)\n",
    "            output = model(seq, task) # shape: B, N, D\n",
    "            # print (\"seq\", seq.shape, \"task\", task.shape, \"output\", output.shape )\n",
    "            preds = output \n",
    "            B, N, D = output.shape\n",
    "             \n",
    "            for i_seq in range(args.len_context-1): \n",
    "                preds_query = preds[:, i_seq,:].reshape(-1, D)\n",
    "                seqs_query = seq[:,(i_seq+1)].reshape(-1)\n",
    "                # losstest = criterion(preds_query, seqs_query)\n",
    "                # acc1 = utils.accuracy(preds_query, seqs_query, topk=[1]) \n",
    "                # print (\"preds_max\", preds_query.argmax(dim=-1).shape, \"seqs_query\", seqs_query.shape)\n",
    "                is_correct = (preds_query.argmax(dim=-1) == seqs_query)\n",
    "                logsoftmax = F.log_softmax(preds_query, dim=1)\n",
    "                logsoftmaxloss = F.nll_loss(logsoftmax, seqs_query, reduction=\"none\") \n",
    "                \n",
    "                # if args.K < 10000: # for small K, we can save the whole tensor \n",
    "                # test_metrics[\"sequence_rank\"].append(task.detach().cpu().numpy())\n",
    "                sequence_ranks[task, i_seq] = task \n",
    "                lengths[task, i_seq] = i_seq + 1 \n",
    "                logsoftmaxlosses[task, i_seq] = logsoftmaxloss \n",
    "                accuracys[task, i_seq] = is_correct.float() \n",
    "\n",
    "    test_metrics[\"sequence_rank\"] = sequence_ranks[:,0].detach().cpu().numpy() # average over positions\n",
    "    test_metrics[\"length\"] = 50\n",
    "    test_metrics[\"logsoftmaxloss\"] = logsoftmaxlosses.mean(dim=1).detach().cpu().numpy()\n",
    "    test_metrics[\"accuracy\"] = accuracys.mean(dim=1).detach().cpu().numpy()\n",
    "     \n",
    "    return test_metrics\n",
    "\n",
    "def probe_gradient_descent_uniform(epoch, val_loader, model, args, criterion, device, get_model):\n",
    "    # model.train()\n",
    "    # model_fork = copy.deepcopy(model).to(device)\n",
    "    model_fork = get_model()\n",
    "    model_fork.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    model_fork.train() \n",
    "    # model_fork_optimizer = get_optimizer(model_fork.parameters()) # initialize new optimizer for the forked model\n",
    "    if args.optimizer == 'Adam':\n",
    "        print(\"args.lr\", args.lr)\n",
    "        model_fork_optimizer = torch.optim.Adam(model_fork.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "    elif args.optimizer == 'SGD':\n",
    "        model_fork_optimizer = torch.optim.SGD(model_fork.parameters(),  \n",
    "                            lr=args.lr, \n",
    "                            weight_decay=args.weight_decay\n",
    "                            )\n",
    "    else:\n",
    "        raise ValueError(\"optimizer not recognized\")\n",
    "    \n",
    "    test_metrics = defaultdict(list) \n",
    "    accuracys = []\n",
    "    for num_iters, (seq, task) in enumerate(val_loader):\n",
    "        model_fork_optimizer.zero_grad()\n",
    "        seq, task = seq.to(device), task.to(device) \n",
    "        \n",
    "        # print(\"seq\", seq.shape, \"task\", task)\n",
    "        output = model_fork(seq, task) # shape: B, N, D\n",
    "        B, N, D = output.shape\n",
    "        preds = output # shape: (B, L, 2)\n",
    "\n",
    "        # loss function: cross-entropy loss\n",
    "        # output at position i should be the input at position i+1\n",
    "        \n",
    "        # Write a function to compute the binary cross-entropy for each position in the sequence\n",
    "        # But don't compute the mean, keep the vector dimension\n",
    "        logsoftmax = F.log_softmax(preds[:,:-1,:], dim=-1).reshape(B * (N-1), D)\n",
    "        logsoftmaxloss = criterion(logsoftmax, seq[:,1:].reshape(B * (N-1)))\n",
    "        # logsoftmaxloss = logsoftmaxloss.reshape(B, N-1).mean(dim=-1) # shape: (B,)\n",
    "        loss = logsoftmaxloss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        model_fork_optimizer.step() \n",
    "        # print(\"task\", task[:10], \"preds\", preds.shape, \"model_state_dict\", model_fork.state_dict()[list (model_fork.state_dict().keys())[0]].flatten()[:50])\n",
    "        # preds contains the predictions for the next position, so we need to compare it with the actual next position\n",
    "        is_correct = (preds.argmax(dim=-1)[:,:-1] == seq[:,1:]) \n",
    "        accuracy = is_correct.float().mean()\n",
    "        accuracys.append(accuracy.detach().cpu().numpy())\n",
    "        # compute the average accuracy of the last 10 items in accuracys\n",
    "        running_average_accuracy = np.mean(accuracys[-10:])\n",
    "        # if running_average_accuracy > 0.95 then stop trainin\n",
    "        if running_average_accuracy > 0.95: \n",
    "            break \n",
    "        elif num_iters > 1000:\n",
    "            break \n",
    "        print (\"loss\", loss, \"accuracy\", accuracy)\n",
    "\n",
    "    test_metrics[\"accuracy\"] = accuracys \n",
    "    test_metrics[\"num_iters_required\"] = num_iters\n",
    "     \n",
    "    return test_metrics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform\n",
      "iter 0 loss 0.6947479179700216 {'train_loss': 0.6947479179700216, 'epoch': 0.0, 'lr': 0.1}\n",
      "uniform\n",
      "iter 1 loss 0.693138923242357 {'train_loss': 0.693138923242357, 'epoch': 0.011111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 2 loss 0.6928436502456665 {'train_loss': 0.6928436502456665, 'epoch': 0.022222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 3 loss 0.6928540747006734 {'train_loss': 0.6928540747006734, 'epoch': 0.03333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 4 loss 0.6927103785938686 {'train_loss': 0.6927103785938686, 'epoch': 0.044444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 5 loss 0.6927114798863729 {'train_loss': 0.6927114798863729, 'epoch': 0.05555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 6 loss 0.6927178283903334 {'train_loss': 0.6927178283903334, 'epoch': 0.06666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 7 loss 0.6927375097168816 {'train_loss': 0.6927375097168816, 'epoch': 0.07777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 8 loss 0.692656759124332 {'train_loss': 0.692656759124332, 'epoch': 0.08888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 9 loss 0.6931133665084839 {'train_loss': 0.6931133665084839, 'epoch': 0.1, 'lr': 0.1}\n",
      "uniform\n",
      "iter 10 loss 0.6927236655871073 {'train_loss': 0.6927236655871073, 'epoch': 0.1111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 11 loss 0.692735736465454 {'train_loss': 0.692735736465454, 'epoch': 0.12222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 12 loss 0.6928143619113498 {'train_loss': 0.6928143619113498, 'epoch': 0.13333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 13 loss 0.6928146877712673 {'train_loss': 0.6928146877712673, 'epoch': 0.14444444444444443, 'lr': 0.1}\n",
      "uniform\n",
      "iter 14 loss 0.6927560699568854 {'train_loss': 0.6927560699568854, 'epoch': 0.15555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 15 loss 0.6928330535358853 {'train_loss': 0.6928330535358853, 'epoch': 0.16666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 16 loss 0.6930833161248101 {'train_loss': 0.6930833161248101, 'epoch': 0.17777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 17 loss 0.6929018078804016 {'train_loss': 0.6929018078804016, 'epoch': 0.18888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 18 loss 0.693552513133155 {'train_loss': 0.693552513133155, 'epoch': 0.2, 'lr': 0.1}\n",
      "uniform\n",
      "iter 19 loss 0.6934238116476271 {'train_loss': 0.6934238116476271, 'epoch': 0.2111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 20 loss 0.6939141211933559 {'train_loss': 0.6939141211933559, 'epoch': 0.2222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 21 loss 0.6946396182907952 {'train_loss': 0.6946396182907952, 'epoch': 0.23333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 22 loss 0.692847854455312 {'train_loss': 0.692847854455312, 'epoch': 0.24444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 23 loss 0.6927623716566298 {'train_loss': 0.6927623716566298, 'epoch': 0.25555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 24 loss 0.6926294419500563 {'train_loss': 0.6926294419500563, 'epoch': 0.26666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 25 loss 0.6926233772277832 {'train_loss': 0.6926233772277832, 'epoch': 0.2777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 26 loss 0.6928225915590922 {'train_loss': 0.6928225915590922, 'epoch': 0.28888888888888886, 'lr': 0.1}\n",
      "uniform\n",
      "iter 27 loss 0.69271173620224 {'train_loss': 0.69271173620224, 'epoch': 0.3, 'lr': 0.1}\n",
      "uniform\n",
      "iter 28 loss 0.6927370362917582 {'train_loss': 0.6927370362917582, 'epoch': 0.3111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 29 loss 0.692742782412635 {'train_loss': 0.692742782412635, 'epoch': 0.32222222222222224, 'lr': 0.1}\n",
      "uniform\n",
      "iter 30 loss 0.6926426191118028 {'train_loss': 0.6926426191118028, 'epoch': 0.3333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 31 loss 0.692667969332801 {'train_loss': 0.692667969332801, 'epoch': 0.34444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 32 loss 0.6928558527628581 {'train_loss': 0.6928558527628581, 'epoch': 0.35555555555555557, 'lr': 0.1}\n",
      "uniform\n",
      "iter 33 loss 0.6928304303381179 {'train_loss': 0.6928304303381179, 'epoch': 0.36666666666666664, 'lr': 0.1}\n",
      "uniform\n",
      "iter 34 loss 0.6927990246348911 {'train_loss': 0.6927990246348911, 'epoch': 0.37777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 35 loss 0.692963215997484 {'train_loss': 0.692963215997484, 'epoch': 0.3888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 36 loss 0.6928205530166626 {'train_loss': 0.6928205530166626, 'epoch': 0.4, 'lr': 0.1}\n",
      "uniform\n",
      "iter 37 loss 0.6929993273205227 {'train_loss': 0.6929993273205227, 'epoch': 0.4111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 38 loss 0.6929667359246148 {'train_loss': 0.6929667359246148, 'epoch': 0.4222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 39 loss 0.6929503218968709 {'train_loss': 0.6929503218968709, 'epoch': 0.43333333333333335, 'lr': 0.1}\n",
      "uniform\n",
      "iter 40 loss 0.6933410766495599 {'train_loss': 0.6933410766495599, 'epoch': 0.4444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 41 loss 0.6926506498760647 {'train_loss': 0.6926506498760647, 'epoch': 0.45555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 42 loss 0.6925322226206462 {'train_loss': 0.6925322226206462, 'epoch': 0.4666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 43 loss 0.692748534488678 {'train_loss': 0.692748534488678, 'epoch': 0.4777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 44 loss 0.6930029885821872 {'train_loss': 0.6930029885821872, 'epoch': 0.4888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 45 loss 0.692565102397071 {'train_loss': 0.692565102397071, 'epoch': 0.5, 'lr': 0.1}\n",
      "uniform\n",
      "iter 46 loss 0.6931923335817125 {'train_loss': 0.6931923335817125, 'epoch': 0.5111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 47 loss 0.6923692662874857 {'train_loss': 0.6923692662874857, 'epoch': 0.5222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 48 loss 0.6922772952185737 {'train_loss': 0.6922772952185737, 'epoch': 0.5333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 49 loss 0.6921568775494893 {'train_loss': 0.6921568775494893, 'epoch': 0.5444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 50 loss 0.6920923120498658 {'train_loss': 0.6920923120498658, 'epoch': 0.5555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 51 loss 0.6924659267849392 {'train_loss': 0.6924659267849392, 'epoch': 0.5666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 52 loss 0.6918848013877869 {'train_loss': 0.6918848013877869, 'epoch': 0.5777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 53 loss 0.6914228857252332 {'train_loss': 0.6914228857252332, 'epoch': 0.5888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 54 loss 0.690985192712148 {'train_loss': 0.690985192712148, 'epoch': 0.6, 'lr': 0.1}\n",
      "uniform\n",
      "iter 55 loss 0.690694194179111 {'train_loss': 0.690694194179111, 'epoch': 0.6111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 56 loss 0.6904496703783671 {'train_loss': 0.6904496703783671, 'epoch': 0.6222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 57 loss 0.6900108551979065 {'train_loss': 0.6900108551979065, 'epoch': 0.6333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 58 loss 0.6896467316415574 {'train_loss': 0.6896467316415574, 'epoch': 0.6444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 59 loss 0.6901359983974032 {'train_loss': 0.6901359983974032, 'epoch': 0.6555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 60 loss 0.6899351262198554 {'train_loss': 0.6899351262198554, 'epoch': 0.6666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 61 loss 0.6895550213283963 {'train_loss': 0.6895550213283963, 'epoch': 0.6777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 62 loss 0.6896764325883653 {'train_loss': 0.6896764325883653, 'epoch': 0.6888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 63 loss 0.6889626416312323 {'train_loss': 0.6889626416312323, 'epoch': 0.7, 'lr': 0.1}\n",
      "uniform\n",
      "iter 64 loss 0.6888886608547634 {'train_loss': 0.6888886608547634, 'epoch': 0.7111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 65 loss 0.6889766945627 {'train_loss': 0.6889766945627, 'epoch': 0.7222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 66 loss 0.6886046856986152 {'train_loss': 0.6886046856986152, 'epoch': 0.7333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 67 loss 0.6882738778856066 {'train_loss': 0.6882738778856066, 'epoch': 0.7444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 68 loss 0.6883944779607984 {'train_loss': 0.6883944779607984, 'epoch': 0.7555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 69 loss 0.6884756706131829 {'train_loss': 0.6884756706131829, 'epoch': 0.7666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 70 loss 0.6881785200542874 {'train_loss': 0.6881785200542874, 'epoch': 0.7777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 71 loss 0.687978978421953 {'train_loss': 0.687978978421953, 'epoch': 0.7888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 72 loss 0.6884852489259508 {'train_loss': 0.6884852489259508, 'epoch': 0.8, 'lr': 0.1}\n",
      "uniform\n",
      "iter 73 loss 0.6884694144354926 {'train_loss': 0.6884694144354926, 'epoch': 0.8111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 74 loss 0.688058650663164 {'train_loss': 0.688058650663164, 'epoch': 0.8222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 75 loss 0.6879839719136556 {'train_loss': 0.6879839719136556, 'epoch': 0.8333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 76 loss 0.6877948542700874 {'train_loss': 0.6877948542700874, 'epoch': 0.8444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 77 loss 0.6877026978280809 {'train_loss': 0.6877026978280809, 'epoch': 0.8555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 78 loss 0.6875938537279764 {'train_loss': 0.6875938537279764, 'epoch': 0.8666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 79 loss 0.6873456423229641 {'train_loss': 0.6873456423229641, 'epoch': 0.8777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 80 loss 0.6870633992936876 {'train_loss': 0.6870633992936876, 'epoch': 0.8888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 81 loss 0.6871572738223606 {'train_loss': 0.6871572738223606, 'epoch': 0.9, 'lr': 0.1}\n",
      "uniform\n",
      "iter 82 loss 0.6874050969547696 {'train_loss': 0.6874050969547696, 'epoch': 0.9111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 83 loss 0.6872168306032816 {'train_loss': 0.6872168306032816, 'epoch': 0.9222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 84 loss 0.6879067624833849 {'train_loss': 0.6879067624833849, 'epoch': 0.9333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 85 loss 0.6878878638267517 {'train_loss': 0.6878878638267517, 'epoch': 0.9444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 86 loss 0.6872851638264126 {'train_loss': 0.6872851638264126, 'epoch': 0.9555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 87 loss 0.6870453908284505 {'train_loss': 0.6870453908284505, 'epoch': 0.9666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 88 loss 0.6869798483318753 {'train_loss': 0.6869798483318753, 'epoch': 0.9777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 89 loss 0.6870117653740777 {'train_loss': 0.6870117653740777, 'epoch': 0.9888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 90 loss 0.6868227284749349 {'train_loss': 0.6868227284749349, 'epoch': 1.0, 'lr': 0.1}\n",
      "uniform\n",
      "iter 91 loss 0.6870639542791579 {'train_loss': 0.6870639542791579, 'epoch': 1.011111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 92 loss 0.6868494039429559 {'train_loss': 0.6868494039429559, 'epoch': 1.0222222222222221, 'lr': 0.1}\n",
      "uniform\n",
      "iter 93 loss 0.6867257072342766 {'train_loss': 0.6867257072342766, 'epoch': 1.0333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 94 loss 0.6869574890878466 {'train_loss': 0.6869574890878466, 'epoch': 1.0444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 95 loss 0.6867582978142632 {'train_loss': 0.6867582978142632, 'epoch': 1.0555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 96 loss 0.6866161786715189 {'train_loss': 0.6866161786715189, 'epoch': 1.0666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 97 loss 0.6868741381751167 {'train_loss': 0.6868741381751167, 'epoch': 1.0777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 98 loss 0.686755638133155 {'train_loss': 0.686755638133155, 'epoch': 1.0888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 99 loss 0.6869032214694553 {'train_loss': 0.6869032214694553, 'epoch': 1.1, 'lr': 0.1}\n",
      "uniform\n",
      "iter 100 loss 0.6869607730759515 {'train_loss': 0.6869607730759515, 'epoch': 1.1111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 101 loss 0.6868257026460436 {'train_loss': 0.6868257026460436, 'epoch': 1.1222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 102 loss 0.6869616261376275 {'train_loss': 0.6869616261376275, 'epoch': 1.1333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 103 loss 0.687221918741862 {'train_loss': 0.687221918741862, 'epoch': 1.1444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 104 loss 0.6862867574161954 {'train_loss': 0.6862867574161954, 'epoch': 1.1555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 105 loss 0.6864771706475152 {'train_loss': 0.6864771706475152, 'epoch': 1.1666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 106 loss 0.6864638544082642 {'train_loss': 0.6864638544082642, 'epoch': 1.1777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 107 loss 0.6863367024209764 {'train_loss': 0.6863367024209764, 'epoch': 1.1888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 108 loss 0.6868242721451653 {'train_loss': 0.6868242721451653, 'epoch': 1.2, 'lr': 0.1}\n",
      "uniform\n",
      "iter 109 loss 0.6872468716939291 {'train_loss': 0.6872468716939291, 'epoch': 1.211111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 110 loss 0.6869032552083333 {'train_loss': 0.6869032552083333, 'epoch': 1.2222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 111 loss 0.6862531411170959 {'train_loss': 0.6862531411170959, 'epoch': 1.2333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 112 loss 0.6862723761876424 {'train_loss': 0.6862723761876424, 'epoch': 1.2444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 113 loss 0.6865077550782098 {'train_loss': 0.6865077550782098, 'epoch': 1.2555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 114 loss 0.6866838642014398 {'train_loss': 0.6866838642014398, 'epoch': 1.2666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 115 loss 0.6867614675309923 {'train_loss': 0.6867614675309923, 'epoch': 1.2777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 116 loss 0.68729984998703 {'train_loss': 0.68729984998703, 'epoch': 1.288888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 117 loss 0.687762905852 {'train_loss': 0.687762905852, 'epoch': 1.3, 'lr': 0.1}\n",
      "uniform\n",
      "iter 118 loss 0.6877859665234883 {'train_loss': 0.6877859665234883, 'epoch': 1.3111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 119 loss 0.6875814715703329 {'train_loss': 0.6875814715703329, 'epoch': 1.3222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 120 loss 0.6866604941262139 {'train_loss': 0.6866604941262139, 'epoch': 1.3333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 121 loss 0.686276995923784 {'train_loss': 0.686276995923784, 'epoch': 1.3444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 122 loss 0.685946316157447 {'train_loss': 0.685946316157447, 'epoch': 1.3555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 123 loss 0.6860935045772129 {'train_loss': 0.6860935045772129, 'epoch': 1.3666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 124 loss 0.6860302442338732 {'train_loss': 0.6860302442338732, 'epoch': 1.3777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 125 loss 0.6858686828507318 {'train_loss': 0.6858686828507318, 'epoch': 1.3888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 126 loss 0.686520282904307 {'train_loss': 0.686520282904307, 'epoch': 1.4, 'lr': 0.1}\n",
      "uniform\n",
      "iter 127 loss 0.6861793594148424 {'train_loss': 0.6861793594148424, 'epoch': 1.4111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 128 loss 0.686302464834849 {'train_loss': 0.686302464834849, 'epoch': 1.4222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 129 loss 0.6859770805464851 {'train_loss': 0.6859770805464851, 'epoch': 1.4333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 130 loss 0.686967147509257 {'train_loss': 0.686967147509257, 'epoch': 1.4444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 131 loss 0.6865447024027507 {'train_loss': 0.6865447024027507, 'epoch': 1.4555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 132 loss 0.6881959752930535 {'train_loss': 0.6881959752930535, 'epoch': 1.4666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 133 loss 0.6869730048073662 {'train_loss': 0.6869730048073662, 'epoch': 1.4777777777777779, 'lr': 0.1}\n",
      "uniform\n",
      "iter 134 loss 0.6868992397626241 {'train_loss': 0.6868992397626241, 'epoch': 1.488888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 135 loss 0.6874229113472833 {'train_loss': 0.6874229113472833, 'epoch': 1.5, 'lr': 0.1}\n",
      "uniform\n",
      "iter 136 loss 0.6862693810674879 {'train_loss': 0.6862693810674879, 'epoch': 1.511111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 137 loss 0.6861248727056716 {'train_loss': 0.6861248727056716, 'epoch': 1.5222222222222221, 'lr': 0.1}\n",
      "uniform\n",
      "iter 138 loss 0.6860052096048991 {'train_loss': 0.6860052096048991, 'epoch': 1.5333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 139 loss 0.6858237421671549 {'train_loss': 0.6858237421671549, 'epoch': 1.5444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 140 loss 0.6860533159573873 {'train_loss': 0.6860533159573873, 'epoch': 1.5555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 141 loss 0.6862346405453152 {'train_loss': 0.6862346405453152, 'epoch': 1.5666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 142 loss 0.6860244126743741 {'train_loss': 0.6860244126743741, 'epoch': 1.5777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 143 loss 0.6859505498992072 {'train_loss': 0.6859505498992072, 'epoch': 1.5888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 144 loss 0.6861794281217787 {'train_loss': 0.6861794281217787, 'epoch': 1.6, 'lr': 0.1}\n",
      "uniform\n",
      "iter 145 loss 0.6869035674624973 {'train_loss': 0.6869035674624973, 'epoch': 1.6111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 146 loss 0.68727134797838 {'train_loss': 0.68727134797838, 'epoch': 1.6222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 147 loss 0.6863786557197571 {'train_loss': 0.6863786557197571, 'epoch': 1.6333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 148 loss 0.6861482892354329 {'train_loss': 0.6861482892354329, 'epoch': 1.6444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 149 loss 0.6856153747982449 {'train_loss': 0.6856153747982449, 'epoch': 1.6555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 150 loss 0.6863890970336066 {'train_loss': 0.6863890970336066, 'epoch': 1.6666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 151 loss 0.6868588292969597 {'train_loss': 0.6868588292969597, 'epoch': 1.6777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 152 loss 0.6863571013874478 {'train_loss': 0.6863571013874478, 'epoch': 1.6888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 153 loss 0.6858553604231941 {'train_loss': 0.6858553604231941, 'epoch': 1.7, 'lr': 0.1}\n",
      "uniform\n",
      "iter 154 loss 0.685798648262024 {'train_loss': 0.685798648262024, 'epoch': 1.711111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 155 loss 0.6858642167833117 {'train_loss': 0.6858642167833117, 'epoch': 1.7222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 156 loss 0.6857335040092468 {'train_loss': 0.6857335040092468, 'epoch': 1.7333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 157 loss 0.6856128680759006 {'train_loss': 0.6856128680759006, 'epoch': 1.7444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 158 loss 0.6856232819663154 {'train_loss': 0.6856232819663154, 'epoch': 1.7555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 159 loss 0.6854820500161912 {'train_loss': 0.6854820500161912, 'epoch': 1.7666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 160 loss 0.6861004558351305 {'train_loss': 0.6861004558351305, 'epoch': 1.7777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 161 loss 0.6859221044010586 {'train_loss': 0.6859221044010586, 'epoch': 1.788888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 162 loss 0.6858862147225274 {'train_loss': 0.6858862147225274, 'epoch': 1.8, 'lr': 0.1}\n",
      "uniform\n",
      "iter 163 loss 0.6855919895701938 {'train_loss': 0.6855919895701938, 'epoch': 1.8111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 164 loss 0.685913691022661 {'train_loss': 0.685913691022661, 'epoch': 1.8222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 165 loss 0.6858124553786383 {'train_loss': 0.6858124553786383, 'epoch': 1.8333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 166 loss 0.6859727437231276 {'train_loss': 0.6859727437231276, 'epoch': 1.8444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 167 loss 0.6858938147438897 {'train_loss': 0.6858938147438897, 'epoch': 1.8555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 168 loss 0.6860441174930997 {'train_loss': 0.6860441174930997, 'epoch': 1.8666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 169 loss 0.6864438720491197 {'train_loss': 0.6864438720491197, 'epoch': 1.8777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 170 loss 0.6864751656532287 {'train_loss': 0.6864751656532287, 'epoch': 1.8888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 171 loss 0.686003693930308 {'train_loss': 0.686003693930308, 'epoch': 1.9, 'lr': 0.1}\n",
      "uniform\n",
      "iter 172 loss 0.6859234370973375 {'train_loss': 0.6859234370973375, 'epoch': 1.9111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 173 loss 0.685542466322581 {'train_loss': 0.685542466322581, 'epoch': 1.9222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 174 loss 0.685674509578281 {'train_loss': 0.685674509578281, 'epoch': 1.9333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 175 loss 0.6855678842332628 {'train_loss': 0.6855678842332628, 'epoch': 1.9444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 176 loss 0.6856444146262275 {'train_loss': 0.6856444146262275, 'epoch': 1.9555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 177 loss 0.6859571820259094 {'train_loss': 0.6859571820259094, 'epoch': 1.9666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 178 loss 0.6855107051955329 {'train_loss': 0.6855107051955329, 'epoch': 1.9777777777777779, 'lr': 0.1}\n",
      "uniform\n",
      "iter 179 loss 0.6856048571904501 {'train_loss': 0.6856048571904501, 'epoch': 1.988888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 180 loss 0.6860823387145996 {'train_loss': 0.6860823387145996, 'epoch': 2.0, 'lr': 0.1}\n",
      "uniform\n",
      "iter 181 loss 0.6852885155995687 {'train_loss': 0.6852885155995687, 'epoch': 2.011111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 182 loss 0.6851934416664971 {'train_loss': 0.6851934416664971, 'epoch': 2.022222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 183 loss 0.6856828886773851 {'train_loss': 0.6856828886773851, 'epoch': 2.033333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 184 loss 0.686239267677731 {'train_loss': 0.686239267677731, 'epoch': 2.0444444444444443, 'lr': 0.1}\n",
      "uniform\n",
      "iter 185 loss 0.6858741967201233 {'train_loss': 0.6858741967201233, 'epoch': 2.0555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 186 loss 0.6857817947387695 {'train_loss': 0.6857817947387695, 'epoch': 2.066666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 187 loss 0.6854585596932306 {'train_loss': 0.6854585596932306, 'epoch': 2.077777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 188 loss 0.6847162138303121 {'train_loss': 0.6847162138303121, 'epoch': 2.088888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 189 loss 0.685534131834242 {'train_loss': 0.685534131834242, 'epoch': 2.1, 'lr': 0.1}\n",
      "uniform\n",
      "iter 190 loss 0.6856557375060187 {'train_loss': 0.6856557375060187, 'epoch': 2.111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 191 loss 0.6856547029495239 {'train_loss': 0.6856547029495239, 'epoch': 2.1222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 192 loss 0.6854925333234999 {'train_loss': 0.6854925333234999, 'epoch': 2.1333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 193 loss 0.6863555291811625 {'train_loss': 0.6863555291811625, 'epoch': 2.1444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 194 loss 0.6864170711729262 {'train_loss': 0.6864170711729262, 'epoch': 2.1555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 195 loss 0.6859668682310316 {'train_loss': 0.6859668682310316, 'epoch': 2.1666666666666665, 'lr': 0.1}\n",
      "uniform\n",
      "iter 196 loss 0.6854776320033603 {'train_loss': 0.6854776320033603, 'epoch': 2.1777777777777776, 'lr': 0.1}\n",
      "uniform\n",
      "iter 197 loss 0.6858269790331523 {'train_loss': 0.6858269790331523, 'epoch': 2.188888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 198 loss 0.6855954636043973 {'train_loss': 0.6855954636043973, 'epoch': 2.2, 'lr': 0.1}\n",
      "uniform\n",
      "iter 199 loss 0.6853358254008823 {'train_loss': 0.6853358254008823, 'epoch': 2.2111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 200 loss 0.6853982573827108 {'train_loss': 0.6853982573827108, 'epoch': 2.2222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 201 loss 0.6853909866227044 {'train_loss': 0.6853909866227044, 'epoch': 2.2333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 202 loss 0.6859867220242818 {'train_loss': 0.6859867220242818, 'epoch': 2.2444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 203 loss 0.6859799844847785 {'train_loss': 0.6859799844847785, 'epoch': 2.2555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 204 loss 0.6861160934872097 {'train_loss': 0.6861160934872097, 'epoch': 2.2666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 205 loss 0.686622519991133 {'train_loss': 0.686622519991133, 'epoch': 2.2777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 206 loss 0.6867469807094998 {'train_loss': 0.6867469807094998, 'epoch': 2.2888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 207 loss 0.6861380951139662 {'train_loss': 0.6861380951139662, 'epoch': 2.3, 'lr': 0.1}\n",
      "uniform\n",
      "iter 208 loss 0.6859590406947665 {'train_loss': 0.6859590406947665, 'epoch': 2.311111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 209 loss 0.6858167029168871 {'train_loss': 0.6858167029168871, 'epoch': 2.3222222222222224, 'lr': 0.1}\n",
      "uniform\n",
      "iter 210 loss 0.6860078561676873 {'train_loss': 0.6860078561676873, 'epoch': 2.3333333333333335, 'lr': 0.1}\n",
      "uniform\n",
      "iter 211 loss 0.6853414999326071 {'train_loss': 0.6853414999326071, 'epoch': 2.3444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 212 loss 0.6851899901919895 {'train_loss': 0.6851899901919895, 'epoch': 2.3555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 213 loss 0.6844947841220432 {'train_loss': 0.6844947841220432, 'epoch': 2.3666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 214 loss 0.6848589129765829 {'train_loss': 0.6848589129765829, 'epoch': 2.3777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 215 loss 0.6850144566641914 {'train_loss': 0.6850144566641914, 'epoch': 2.388888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 216 loss 0.6849627765655517 {'train_loss': 0.6849627765655517, 'epoch': 2.4, 'lr': 0.1}\n",
      "uniform\n",
      "iter 217 loss 0.6852433982425266 {'train_loss': 0.6852433982425266, 'epoch': 2.411111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 218 loss 0.68478265209198 {'train_loss': 0.68478265209198, 'epoch': 2.422222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 219 loss 0.6849407645755344 {'train_loss': 0.6849407645755344, 'epoch': 2.433333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 220 loss 0.6850157739215427 {'train_loss': 0.6850157739215427, 'epoch': 2.4444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 221 loss 0.6853575066990323 {'train_loss': 0.6853575066990323, 'epoch': 2.4555555555555557, 'lr': 0.1}\n",
      "uniform\n",
      "iter 222 loss 0.6850897689395481 {'train_loss': 0.6850897689395481, 'epoch': 2.466666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 223 loss 0.6849611586676704 {'train_loss': 0.6849611586676704, 'epoch': 2.477777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 224 loss 0.6847712157143487 {'train_loss': 0.6847712157143487, 'epoch': 2.488888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 225 loss 0.6849643152236938 {'train_loss': 0.6849643152236938, 'epoch': 2.5, 'lr': 0.1}\n",
      "uniform\n",
      "iter 226 loss 0.6850032946162754 {'train_loss': 0.6850032946162754, 'epoch': 2.511111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 227 loss 0.6853433490117391 {'train_loss': 0.6853433490117391, 'epoch': 2.522222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 228 loss 0.6852407328075832 {'train_loss': 0.6852407328075832, 'epoch': 2.533333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 229 loss 0.6849751832644144 {'train_loss': 0.6849751832644144, 'epoch': 2.5444444444444443, 'lr': 0.1}\n",
      "uniform\n",
      "iter 230 loss 0.6851476667086284 {'train_loss': 0.6851476667086284, 'epoch': 2.5555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 231 loss 0.6846148572180006 {'train_loss': 0.6846148572180006, 'epoch': 2.566666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 232 loss 0.6846597556644016 {'train_loss': 0.6846597556644016, 'epoch': 2.577777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 233 loss 0.6845663079367743 {'train_loss': 0.6845663079367743, 'epoch': 2.588888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 234 loss 0.685063275443183 {'train_loss': 0.685063275443183, 'epoch': 2.6, 'lr': 0.1}\n",
      "uniform\n",
      "iter 235 loss 0.6844621177567376 {'train_loss': 0.6844621177567376, 'epoch': 2.611111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 236 loss 0.684630539756351 {'train_loss': 0.684630539756351, 'epoch': 2.6222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 237 loss 0.6849396611849466 {'train_loss': 0.6849396611849466, 'epoch': 2.6333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 238 loss 0.6847714727189805 {'train_loss': 0.6847714727189805, 'epoch': 2.6444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 239 loss 0.6845115912331475 {'train_loss': 0.6845115912331475, 'epoch': 2.6555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 240 loss 0.6850585939937168 {'train_loss': 0.6850585939937168, 'epoch': 2.6666666666666665, 'lr': 0.1}\n",
      "uniform\n",
      "iter 241 loss 0.6853722945849101 {'train_loss': 0.6853722945849101, 'epoch': 2.6777777777777776, 'lr': 0.1}\n",
      "uniform\n",
      "iter 242 loss 0.6860012909889222 {'train_loss': 0.6860012909889222, 'epoch': 2.688888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 243 loss 0.684908488146464 {'train_loss': 0.684908488146464, 'epoch': 2.7, 'lr': 0.1}\n",
      "uniform\n",
      "iter 244 loss 0.6853030843840705 {'train_loss': 0.6853030843840705, 'epoch': 2.7111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 245 loss 0.6851472859276666 {'train_loss': 0.6851472859276666, 'epoch': 2.7222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 246 loss 0.6849168136384752 {'train_loss': 0.6849168136384752, 'epoch': 2.7333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 247 loss 0.6847322935740153 {'train_loss': 0.6847322935740153, 'epoch': 2.7444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 248 loss 0.6848417995664808 {'train_loss': 0.6848417995664808, 'epoch': 2.7555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 249 loss 0.6844392628033956 {'train_loss': 0.6844392628033956, 'epoch': 2.7666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 250 loss 0.6845090553495619 {'train_loss': 0.6845090553495619, 'epoch': 2.7777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 251 loss 0.6846520254453023 {'train_loss': 0.6846520254453023, 'epoch': 2.7888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 252 loss 0.6849218972629971 {'train_loss': 0.6849218972629971, 'epoch': 2.8, 'lr': 0.1}\n",
      "uniform\n",
      "iter 253 loss 0.6847544103622436 {'train_loss': 0.6847544103622436, 'epoch': 2.811111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 254 loss 0.684874503241645 {'train_loss': 0.684874503241645, 'epoch': 2.8222222222222224, 'lr': 0.1}\n",
      "uniform\n",
      "iter 255 loss 0.6850386997858683 {'train_loss': 0.6850386997858683, 'epoch': 2.8333333333333335, 'lr': 0.1}\n",
      "uniform\n",
      "iter 256 loss 0.6848048523479038 {'train_loss': 0.6848048523479038, 'epoch': 2.8444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 257 loss 0.684521652507782 {'train_loss': 0.684521652507782, 'epoch': 2.8555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 258 loss 0.6848400081210666 {'train_loss': 0.6848400081210666, 'epoch': 2.8666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 259 loss 0.6846802872021993 {'train_loss': 0.6846802872021993, 'epoch': 2.8777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 260 loss 0.6849999890115526 {'train_loss': 0.6849999890115526, 'epoch': 2.888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 261 loss 0.6851515182918972 {'train_loss': 0.6851515182918972, 'epoch': 2.9, 'lr': 0.1}\n",
      "uniform\n",
      "iter 262 loss 0.6848328006850348 {'train_loss': 0.6848328006850348, 'epoch': 2.911111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 263 loss 0.6844951921463013 {'train_loss': 0.6844951921463013, 'epoch': 2.922222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 264 loss 0.6846213040457831 {'train_loss': 0.6846213040457831, 'epoch': 2.933333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 265 loss 0.6848481092558967 {'train_loss': 0.6848481092558967, 'epoch': 2.9444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 266 loss 0.6849268817159865 {'train_loss': 0.6849268817159865, 'epoch': 2.9555555555555557, 'lr': 0.1}\n",
      "uniform\n",
      "iter 267 loss 0.6846250772264268 {'train_loss': 0.6846250772264268, 'epoch': 2.966666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 268 loss 0.684344720448388 {'train_loss': 0.684344720448388, 'epoch': 2.977777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 269 loss 0.6841879729059007 {'train_loss': 0.6841879729059007, 'epoch': 2.988888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 270 loss 0.6841481879340278 {'train_loss': 0.6841481879340278, 'epoch': 3.0, 'lr': 0.1}\n",
      "uniform\n",
      "iter 271 loss 0.6843954568545023 {'train_loss': 0.6843954568545023, 'epoch': 3.011111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 272 loss 0.6848643384933472 {'train_loss': 0.6848643384933472, 'epoch': 3.022222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 273 loss 0.6850700812021892 {'train_loss': 0.6850700812021892, 'epoch': 3.033333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 274 loss 0.6847086033185323 {'train_loss': 0.6847086033185323, 'epoch': 3.0444444444444443, 'lr': 0.1}\n",
      "uniform\n",
      "iter 275 loss 0.6844743826230367 {'train_loss': 0.6844743826230367, 'epoch': 3.0555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 276 loss 0.6844672022501628 {'train_loss': 0.6844672022501628, 'epoch': 3.066666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 277 loss 0.6845641871346367 {'train_loss': 0.6845641871346367, 'epoch': 3.077777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 278 loss 0.6849497030893962 {'train_loss': 0.6849497030893962, 'epoch': 3.088888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 279 loss 0.6846131975597806 {'train_loss': 0.6846131975597806, 'epoch': 3.1, 'lr': 0.1}\n",
      "uniform\n",
      "iter 280 loss 0.6844269922998216 {'train_loss': 0.6844269922998216, 'epoch': 3.111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 281 loss 0.6849763883378771 {'train_loss': 0.6849763883378771, 'epoch': 3.1222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 282 loss 0.6846893156051636 {'train_loss': 0.6846893156051636, 'epoch': 3.1333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 283 loss 0.6840562991989984 {'train_loss': 0.6840562991989984, 'epoch': 3.1444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 284 loss 0.6841464210192363 {'train_loss': 0.6841464210192363, 'epoch': 3.1555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 285 loss 0.6841996613396538 {'train_loss': 0.6841996613396538, 'epoch': 3.1666666666666665, 'lr': 0.1}\n",
      "uniform\n",
      "iter 286 loss 0.6846377905633715 {'train_loss': 0.6846377905633715, 'epoch': 3.1777777777777776, 'lr': 0.1}\n",
      "uniform\n",
      "iter 287 loss 0.6845780316034953 {'train_loss': 0.6845780316034953, 'epoch': 3.188888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 288 loss 0.6840990569644504 {'train_loss': 0.6840990569644504, 'epoch': 3.2, 'lr': 0.1}\n",
      "uniform\n",
      "iter 289 loss 0.683951118543413 {'train_loss': 0.683951118543413, 'epoch': 3.2111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 290 loss 0.6840166630850898 {'train_loss': 0.6840166630850898, 'epoch': 3.2222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 291 loss 0.684540756670634 {'train_loss': 0.684540756670634, 'epoch': 3.2333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 292 loss 0.6847144913567437 {'train_loss': 0.6847144913567437, 'epoch': 3.2444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 293 loss 0.6846522574424744 {'train_loss': 0.6846522574424744, 'epoch': 3.2555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 294 loss 0.6849932833035787 {'train_loss': 0.6849932833035787, 'epoch': 3.2666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 295 loss 0.6855155792342292 {'train_loss': 0.6855155792342292, 'epoch': 3.2777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 296 loss 0.6849311747656928 {'train_loss': 0.6849311747656928, 'epoch': 3.2888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 297 loss 0.6845065482457479 {'train_loss': 0.6845065482457479, 'epoch': 3.3, 'lr': 0.1}\n",
      "uniform\n",
      "iter 298 loss 0.6846759787877401 {'train_loss': 0.6846759787877401, 'epoch': 3.311111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 299 loss 0.684878439394633 {'train_loss': 0.684878439394633, 'epoch': 3.3222222222222224, 'lr': 0.1}\n",
      "uniform\n",
      "iter 300 loss 0.6850156112564935 {'train_loss': 0.6850156112564935, 'epoch': 3.3333333333333335, 'lr': 0.1}\n",
      "uniform\n",
      "iter 301 loss 0.6849001009199355 {'train_loss': 0.6849001009199355, 'epoch': 3.3444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 302 loss 0.6850025636990865 {'train_loss': 0.6850025636990865, 'epoch': 3.3555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 303 loss 0.6847461752679613 {'train_loss': 0.6847461752679613, 'epoch': 3.3666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 304 loss 0.6848406065305074 {'train_loss': 0.6848406065305074, 'epoch': 3.3777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 305 loss 0.6850329824341668 {'train_loss': 0.6850329824341668, 'epoch': 3.388888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 306 loss 0.685226837433709 {'train_loss': 0.685226837433709, 'epoch': 3.4, 'lr': 0.1}\n",
      "uniform\n",
      "iter 307 loss 0.6848365294244554 {'train_loss': 0.6848365294244554, 'epoch': 3.411111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 308 loss 0.6852449249691434 {'train_loss': 0.6852449249691434, 'epoch': 3.422222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 309 loss 0.6847074714448717 {'train_loss': 0.6847074714448717, 'epoch': 3.433333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 310 loss 0.6845308760854933 {'train_loss': 0.6845308760854933, 'epoch': 3.4444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 311 loss 0.6843497573958502 {'train_loss': 0.6843497573958502, 'epoch': 3.4555555555555557, 'lr': 0.1}\n",
      "uniform\n",
      "iter 312 loss 0.6846781082259285 {'train_loss': 0.6846781082259285, 'epoch': 3.466666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 313 loss 0.6848158414416843 {'train_loss': 0.6848158414416843, 'epoch': 3.477777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 314 loss 0.6850694352679783 {'train_loss': 0.6850694352679783, 'epoch': 3.488888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 315 loss 0.685160006067488 {'train_loss': 0.685160006067488, 'epoch': 3.5, 'lr': 0.1}\n",
      "uniform\n",
      "iter 316 loss 0.685415876642863 {'train_loss': 0.685415876642863, 'epoch': 3.511111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 317 loss 0.6852108190960354 {'train_loss': 0.6852108190960354, 'epoch': 3.522222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 318 loss 0.685027644803789 {'train_loss': 0.685027644803789, 'epoch': 3.533333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 319 loss 0.6851655444145203 {'train_loss': 0.6851655444145203, 'epoch': 3.5444444444444443, 'lr': 0.1}\n",
      "uniform\n",
      "iter 320 loss 0.6847722339312236 {'train_loss': 0.6847722339312236, 'epoch': 3.5555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 321 loss 0.6847899732801649 {'train_loss': 0.6847899732801649, 'epoch': 3.566666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 322 loss 0.6851208054860433 {'train_loss': 0.6851208054860433, 'epoch': 3.577777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 323 loss 0.6845326198365953 {'train_loss': 0.6845326198365953, 'epoch': 3.588888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 324 loss 0.684594325796763 {'train_loss': 0.684594325796763, 'epoch': 3.6, 'lr': 0.1}\n",
      "uniform\n",
      "iter 325 loss 0.6842401688363817 {'train_loss': 0.6842401688363817, 'epoch': 3.611111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 326 loss 0.6848635477277968 {'train_loss': 0.6848635477277968, 'epoch': 3.6222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 327 loss 0.6850604493988884 {'train_loss': 0.6850604493988884, 'epoch': 3.6333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 328 loss 0.6851163606113858 {'train_loss': 0.6851163606113858, 'epoch': 3.6444444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 329 loss 0.685098279232449 {'train_loss': 0.685098279232449, 'epoch': 3.6555555555555554, 'lr': 0.1}\n",
      "uniform\n",
      "iter 330 loss 0.684202878051334 {'train_loss': 0.684202878051334, 'epoch': 3.6666666666666665, 'lr': 0.1}\n",
      "uniform\n",
      "iter 331 loss 0.6840733565542433 {'train_loss': 0.6840733565542433, 'epoch': 3.6777777777777776, 'lr': 0.1}\n",
      "uniform\n",
      "iter 332 loss 0.684565173763699 {'train_loss': 0.684565173763699, 'epoch': 3.688888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 333 loss 0.6844245660675896 {'train_loss': 0.6844245660675896, 'epoch': 3.7, 'lr': 0.1}\n",
      "uniform\n",
      "iter 334 loss 0.6845986051771376 {'train_loss': 0.6845986051771376, 'epoch': 3.7111111111111112, 'lr': 0.1}\n",
      "uniform\n",
      "iter 335 loss 0.6847538774808248 {'train_loss': 0.6847538774808248, 'epoch': 3.7222222222222223, 'lr': 0.1}\n",
      "uniform\n",
      "iter 336 loss 0.684523229153951 {'train_loss': 0.684523229153951, 'epoch': 3.7333333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 337 loss 0.6845141549746195 {'train_loss': 0.6845141549746195, 'epoch': 3.7444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 338 loss 0.6854961358176337 {'train_loss': 0.6854961358176337, 'epoch': 3.7555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 339 loss 0.6848413576126099 {'train_loss': 0.6848413576126099, 'epoch': 3.7666666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 340 loss 0.6850903484980265 {'train_loss': 0.6850903484980265, 'epoch': 3.7777777777777777, 'lr': 0.1}\n",
      "uniform\n",
      "iter 341 loss 0.6859738970862495 {'train_loss': 0.6859738970862495, 'epoch': 3.7888888888888888, 'lr': 0.1}\n",
      "uniform\n",
      "iter 342 loss 0.6858864010598924 {'train_loss': 0.6858864010598924, 'epoch': 3.8, 'lr': 0.1}\n",
      "uniform\n",
      "iter 343 loss 0.6853617675251431 {'train_loss': 0.6853617675251431, 'epoch': 3.811111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 344 loss 0.6850346248838637 {'train_loss': 0.6850346248838637, 'epoch': 3.8222222222222224, 'lr': 0.1}\n",
      "uniform\n",
      "iter 345 loss 0.6848228187666999 {'train_loss': 0.6848228187666999, 'epoch': 3.8333333333333335, 'lr': 0.1}\n",
      "uniform\n",
      "iter 346 loss 0.6845982975429958 {'train_loss': 0.6845982975429958, 'epoch': 3.8444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 347 loss 0.6844731118096246 {'train_loss': 0.6844731118096246, 'epoch': 3.8555555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 348 loss 0.6844899202982585 {'train_loss': 0.6844899202982585, 'epoch': 3.8666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 349 loss 0.6846079410023159 {'train_loss': 0.6846079410023159, 'epoch': 3.8777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 350 loss 0.6847341519991557 {'train_loss': 0.6847341519991557, 'epoch': 3.888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 351 loss 0.6847172965155708 {'train_loss': 0.6847172965155708, 'epoch': 3.9, 'lr': 0.1}\n",
      "uniform\n",
      "iter 352 loss 0.6846491581704881 {'train_loss': 0.6846491581704881, 'epoch': 3.911111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 353 loss 0.6842496207873027 {'train_loss': 0.6842496207873027, 'epoch': 3.922222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 354 loss 0.6844251963191562 {'train_loss': 0.6844251963191562, 'epoch': 3.933333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 355 loss 0.6845774190690782 {'train_loss': 0.6845774190690782, 'epoch': 3.9444444444444446, 'lr': 0.1}\n",
      "uniform\n",
      "iter 356 loss 0.6845479281213549 {'train_loss': 0.6845479281213549, 'epoch': 3.9555555555555557, 'lr': 0.1}\n",
      "uniform\n",
      "iter 357 loss 0.6847632775942485 {'train_loss': 0.6847632775942485, 'epoch': 3.966666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 358 loss 0.6850611373265584 {'train_loss': 0.6850611373265584, 'epoch': 3.977777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 359 loss 0.6848411237080891 {'train_loss': 0.6848411237080891, 'epoch': 3.988888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 360 loss 0.6845727595117357 {'train_loss': 0.6845727595117357, 'epoch': 4.0, 'lr': 0.1}\n",
      "uniform\n",
      "iter 361 loss 0.6848774354404873 {'train_loss': 0.6848774354404873, 'epoch': 4.011111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 362 loss 0.684727890862359 {'train_loss': 0.684727890862359, 'epoch': 4.022222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 363 loss 0.6845726491398282 {'train_loss': 0.6845726491398282, 'epoch': 4.033333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 364 loss 0.6844433754709032 {'train_loss': 0.6844433754709032, 'epoch': 4.044444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 365 loss 0.6846409575992161 {'train_loss': 0.6846409575992161, 'epoch': 4.055555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 366 loss 0.6847208567725288 {'train_loss': 0.6847208567725288, 'epoch': 4.066666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 367 loss 0.6847116022957696 {'train_loss': 0.6847116022957696, 'epoch': 4.0777777777777775, 'lr': 0.1}\n",
      "uniform\n",
      "iter 368 loss 0.6849138894610934 {'train_loss': 0.6849138894610934, 'epoch': 4.088888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 369 loss 0.6854835982852512 {'train_loss': 0.6854835982852512, 'epoch': 4.1, 'lr': 0.1}\n",
      "uniform\n",
      "iter 370 loss 0.685647473080953 {'train_loss': 0.685647473080953, 'epoch': 4.111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 371 loss 0.6852885724279616 {'train_loss': 0.6852885724279616, 'epoch': 4.122222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 372 loss 0.6854032590124343 {'train_loss': 0.6854032590124343, 'epoch': 4.133333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 373 loss 0.6866139510154724 {'train_loss': 0.6866139510154724, 'epoch': 4.144444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 374 loss 0.6859405077404446 {'train_loss': 0.6859405077404446, 'epoch': 4.155555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 375 loss 0.6855807108561198 {'train_loss': 0.6855807108561198, 'epoch': 4.166666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 376 loss 0.6848886532783508 {'train_loss': 0.6848886532783508, 'epoch': 4.177777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 377 loss 0.6846425955772399 {'train_loss': 0.6846425955772399, 'epoch': 4.188888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 378 loss 0.6841937470330133 {'train_loss': 0.6841937470330133, 'epoch': 4.2, 'lr': 0.1}\n",
      "uniform\n",
      "iter 379 loss 0.6842735007180107 {'train_loss': 0.6842735007180107, 'epoch': 4.211111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 380 loss 0.6848145745489332 {'train_loss': 0.6848145745489332, 'epoch': 4.222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 381 loss 0.6848142003907097 {'train_loss': 0.6848142003907097, 'epoch': 4.233333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 382 loss 0.6849703811963399 {'train_loss': 0.6849703811963399, 'epoch': 4.2444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 383 loss 0.6848487389988369 {'train_loss': 0.6848487389988369, 'epoch': 4.2555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 384 loss 0.6845238164795769 {'train_loss': 0.6845238164795769, 'epoch': 4.266666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 385 loss 0.6847673227416144 {'train_loss': 0.6847673227416144, 'epoch': 4.277777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 386 loss 0.6857714465671115 {'train_loss': 0.6857714465671115, 'epoch': 4.288888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 387 loss 0.6856161319626702 {'train_loss': 0.6856161319626702, 'epoch': 4.3, 'lr': 0.1}\n",
      "uniform\n",
      "iter 388 loss 0.6845468976974487 {'train_loss': 0.6845468976974487, 'epoch': 4.311111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 389 loss 0.6842561933729384 {'train_loss': 0.6842561933729384, 'epoch': 4.322222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 390 loss 0.6846337118996514 {'train_loss': 0.6846337118996514, 'epoch': 4.333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 391 loss 0.6843252456665039 {'train_loss': 0.6843252456665039, 'epoch': 4.344444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 392 loss 0.6846234621259901 {'train_loss': 0.6846234621259901, 'epoch': 4.355555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 393 loss 0.6851798467212253 {'train_loss': 0.6851798467212253, 'epoch': 4.366666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 394 loss 0.6845545862091912 {'train_loss': 0.6845545862091912, 'epoch': 4.377777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 395 loss 0.6843348706033495 {'train_loss': 0.6843348706033495, 'epoch': 4.388888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 396 loss 0.6854641739951239 {'train_loss': 0.6854641739951239, 'epoch': 4.4, 'lr': 0.1}\n",
      "uniform\n",
      "iter 397 loss 0.6852178791364034 {'train_loss': 0.6852178791364034, 'epoch': 4.411111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 398 loss 0.6844172585381402 {'train_loss': 0.6844172585381402, 'epoch': 4.4222222222222225, 'lr': 0.1}\n",
      "uniform\n",
      "iter 399 loss 0.6849353634728326 {'train_loss': 0.6849353634728326, 'epoch': 4.433333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 400 loss 0.685441292338901 {'train_loss': 0.685441292338901, 'epoch': 4.444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 401 loss 0.6857480047755771 {'train_loss': 0.6857480047755771, 'epoch': 4.455555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 402 loss 0.6857184883965386 {'train_loss': 0.6857184883965386, 'epoch': 4.466666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 403 loss 0.6848665774451361 {'train_loss': 0.6848665774451361, 'epoch': 4.477777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 404 loss 0.6849231645796034 {'train_loss': 0.6849231645796034, 'epoch': 4.488888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 405 loss 0.6847483884069655 {'train_loss': 0.6847483884069655, 'epoch': 4.5, 'lr': 0.1}\n",
      "uniform\n",
      "iter 406 loss 0.6846334275139703 {'train_loss': 0.6846334275139703, 'epoch': 4.511111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 407 loss 0.6847155431111653 {'train_loss': 0.6847155431111653, 'epoch': 4.522222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 408 loss 0.6848625644259982 {'train_loss': 0.6848625644259982, 'epoch': 4.533333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 409 loss 0.6844906978819105 {'train_loss': 0.6844906978819105, 'epoch': 4.544444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 410 loss 0.6848411320580376 {'train_loss': 0.6848411320580376, 'epoch': 4.555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 411 loss 0.6848914112197029 {'train_loss': 0.6848914112197029, 'epoch': 4.566666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 412 loss 0.6843659371376037 {'train_loss': 0.6843659371376037, 'epoch': 4.5777777777777775, 'lr': 0.1}\n",
      "uniform\n",
      "iter 413 loss 0.6846295162306891 {'train_loss': 0.6846295162306891, 'epoch': 4.588888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 414 loss 0.6846607275221083 {'train_loss': 0.6846607275221083, 'epoch': 4.6, 'lr': 0.1}\n",
      "uniform\n",
      "iter 415 loss 0.684790782515208 {'train_loss': 0.684790782515208, 'epoch': 4.611111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 416 loss 0.6844910230000814 {'train_loss': 0.6844910230000814, 'epoch': 4.622222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 417 loss 0.6846406713803609 {'train_loss': 0.6846406713803609, 'epoch': 4.633333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 418 loss 0.6843886222203572 {'train_loss': 0.6843886222203572, 'epoch': 4.644444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 419 loss 0.684671261575487 {'train_loss': 0.684671261575487, 'epoch': 4.655555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 420 loss 0.6845950435214573 {'train_loss': 0.6845950435214573, 'epoch': 4.666666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 421 loss 0.6845149341159397 {'train_loss': 0.6845149341159397, 'epoch': 4.677777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 422 loss 0.6843561604287889 {'train_loss': 0.6843561604287889, 'epoch': 4.688888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 423 loss 0.6845323167059156 {'train_loss': 0.6845323167059156, 'epoch': 4.7, 'lr': 0.1}\n",
      "uniform\n",
      "iter 424 loss 0.6854071146647135 {'train_loss': 0.6854071146647135, 'epoch': 4.711111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 425 loss 0.6874959727605184 {'train_loss': 0.6874959727605184, 'epoch': 4.722222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 426 loss 0.686688902844323 {'train_loss': 0.686688902844323, 'epoch': 4.733333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 427 loss 0.6858142801496717 {'train_loss': 0.6858142801496717, 'epoch': 4.7444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 428 loss 0.6857793537139892 {'train_loss': 0.6857793537139892, 'epoch': 4.7555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 429 loss 0.6848482120301989 {'train_loss': 0.6848482120301989, 'epoch': 4.766666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 430 loss 0.6848294491767883 {'train_loss': 0.6848294491767883, 'epoch': 4.777777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 431 loss 0.6848239915953742 {'train_loss': 0.6848239915953742, 'epoch': 4.788888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 432 loss 0.6850839018503825 {'train_loss': 0.6850839018503825, 'epoch': 4.8, 'lr': 0.1}\n",
      "uniform\n",
      "iter 433 loss 0.6856116703033447 {'train_loss': 0.6856116703033447, 'epoch': 4.811111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 434 loss 0.6874144744237264 {'train_loss': 0.6874144744237264, 'epoch': 4.822222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 435 loss 0.6865308584319221 {'train_loss': 0.6865308584319221, 'epoch': 4.833333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 436 loss 0.6858841289626227 {'train_loss': 0.6858841289626227, 'epoch': 4.844444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 437 loss 0.685450424501631 {'train_loss': 0.685450424501631, 'epoch': 4.855555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 438 loss 0.6849272021611531 {'train_loss': 0.6849272021611531, 'epoch': 4.866666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 439 loss 0.6850184750450982 {'train_loss': 0.6850184750450982, 'epoch': 4.877777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 440 loss 0.6846376849810283 {'train_loss': 0.6846376849810283, 'epoch': 4.888888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 441 loss 0.6846404691166348 {'train_loss': 0.6846404691166348, 'epoch': 4.9, 'lr': 0.1}\n",
      "uniform\n",
      "iter 442 loss 0.6845990094290839 {'train_loss': 0.6845990094290839, 'epoch': 4.911111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 443 loss 0.6847582282172309 {'train_loss': 0.6847582282172309, 'epoch': 4.9222222222222225, 'lr': 0.1}\n",
      "uniform\n",
      "iter 444 loss 0.6851557121594747 {'train_loss': 0.6851557121594747, 'epoch': 4.933333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 445 loss 0.684447402466668 {'train_loss': 0.684447402466668, 'epoch': 4.944444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 446 loss 0.6849205845938788 {'train_loss': 0.6849205845938788, 'epoch': 4.955555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 447 loss 0.6850922276708815 {'train_loss': 0.6850922276708815, 'epoch': 4.966666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 448 loss 0.6854975541432698 {'train_loss': 0.6854975541432698, 'epoch': 4.977777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 449 loss 0.6856397348509895 {'train_loss': 0.6856397348509895, 'epoch': 4.988888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 450 loss 0.6852734003384908 {'train_loss': 0.6852734003384908, 'epoch': 5.0, 'lr': 0.1}\n",
      "uniform\n",
      "iter 451 loss 0.6855433189709982 {'train_loss': 0.6855433189709982, 'epoch': 5.011111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 452 loss 0.6852529123306275 {'train_loss': 0.6852529123306275, 'epoch': 5.022222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 453 loss 0.6848978443781535 {'train_loss': 0.6848978443781535, 'epoch': 5.033333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 454 loss 0.6844637176937527 {'train_loss': 0.6844637176937527, 'epoch': 5.044444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 455 loss 0.6842454682456123 {'train_loss': 0.6842454682456123, 'epoch': 5.055555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 456 loss 0.6841412432670593 {'train_loss': 0.6841412432670593, 'epoch': 5.066666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 457 loss 0.6843829361915589 {'train_loss': 0.6843829361915589, 'epoch': 5.0777777777777775, 'lr': 0.1}\n",
      "uniform\n",
      "iter 458 loss 0.6850752384927538 {'train_loss': 0.6850752384927538, 'epoch': 5.088888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 459 loss 0.6843791225857205 {'train_loss': 0.6843791225857205, 'epoch': 5.1, 'lr': 0.1}\n",
      "uniform\n",
      "iter 460 loss 0.6844232318242391 {'train_loss': 0.6844232318242391, 'epoch': 5.111111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 461 loss 0.6841916812472874 {'train_loss': 0.6841916812472874, 'epoch': 5.122222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 462 loss 0.6844284560097589 {'train_loss': 0.6844284560097589, 'epoch': 5.133333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 463 loss 0.684880492846171 {'train_loss': 0.684880492846171, 'epoch': 5.144444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 464 loss 0.6846827619446648 {'train_loss': 0.6846827619446648, 'epoch': 5.155555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 465 loss 0.6848216516388788 {'train_loss': 0.6848216516388788, 'epoch': 5.166666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 466 loss 0.6851885156419543 {'train_loss': 0.6851885156419543, 'epoch': 5.177777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 467 loss 0.6856038491037156 {'train_loss': 0.6856038491037156, 'epoch': 5.188888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 468 loss 0.6860648282898797 {'train_loss': 0.6860648282898797, 'epoch': 5.2, 'lr': 0.1}\n",
      "uniform\n",
      "iter 469 loss 0.686599612903595 {'train_loss': 0.686599612903595, 'epoch': 5.211111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 470 loss 0.6868337085300021 {'train_loss': 0.6868337085300021, 'epoch': 5.222222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 471 loss 0.6853799868477716 {'train_loss': 0.6853799868477716, 'epoch': 5.233333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 472 loss 0.685589365333981 {'train_loss': 0.685589365333981, 'epoch': 5.2444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 473 loss 0.6853338617430793 {'train_loss': 0.6853338617430793, 'epoch': 5.2555555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 474 loss 0.6855125625186497 {'train_loss': 0.6855125625186497, 'epoch': 5.266666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 475 loss 0.6858078876813253 {'train_loss': 0.6858078876813253, 'epoch': 5.277777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 476 loss 0.685592698902554 {'train_loss': 0.685592698902554, 'epoch': 5.288888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 477 loss 0.6851557889090644 {'train_loss': 0.6851557889090644, 'epoch': 5.3, 'lr': 0.1}\n",
      "uniform\n",
      "iter 478 loss 0.6847803418265449 {'train_loss': 0.6847803418265449, 'epoch': 5.311111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 479 loss 0.6848606774012248 {'train_loss': 0.6848606774012248, 'epoch': 5.322222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 480 loss 0.6849862834612529 {'train_loss': 0.6849862834612529, 'epoch': 5.333333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 481 loss 0.6857349514537388 {'train_loss': 0.6857349514537388, 'epoch': 5.344444444444444, 'lr': 0.1}\n",
      "uniform\n",
      "iter 482 loss 0.6856456675317553 {'train_loss': 0.6856456675317553, 'epoch': 5.355555555555555, 'lr': 0.1}\n",
      "uniform\n",
      "iter 483 loss 0.6854356588575575 {'train_loss': 0.6854356588575575, 'epoch': 5.366666666666666, 'lr': 0.1}\n",
      "uniform\n",
      "iter 484 loss 0.6856564850277371 {'train_loss': 0.6856564850277371, 'epoch': 5.377777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 485 loss 0.6858668243090311 {'train_loss': 0.6858668243090311, 'epoch': 5.388888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 486 loss 0.6863394005775452 {'train_loss': 0.6863394005775452, 'epoch': 5.4, 'lr': 0.1}\n",
      "uniform\n",
      "iter 487 loss 0.685586227830251 {'train_loss': 0.685586227830251, 'epoch': 5.411111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 488 loss 0.6856673725446065 {'train_loss': 0.6856673725446065, 'epoch': 5.4222222222222225, 'lr': 0.1}\n",
      "uniform\n",
      "iter 489 loss 0.6858713877571954 {'train_loss': 0.6858713877571954, 'epoch': 5.433333333333334, 'lr': 0.1}\n",
      "uniform\n",
      "iter 490 loss 0.6864461601999071 {'train_loss': 0.6864461601999071, 'epoch': 5.444444444444445, 'lr': 0.1}\n",
      "uniform\n",
      "iter 491 loss 0.686597723780738 {'train_loss': 0.686597723780738, 'epoch': 5.455555555555556, 'lr': 0.1}\n",
      "uniform\n",
      "iter 492 loss 0.6858479857550727 {'train_loss': 0.6858479857550727, 'epoch': 5.466666666666667, 'lr': 0.1}\n",
      "uniform\n",
      "iter 493 loss 0.6861365889655219 {'train_loss': 0.6861365889655219, 'epoch': 5.477777777777778, 'lr': 0.1}\n",
      "uniform\n",
      "iter 494 loss 0.6854854188919067 {'train_loss': 0.6854854188919067, 'epoch': 5.488888888888889, 'lr': 0.1}\n",
      "uniform\n",
      "iter 495 loss 0.6857476285616557 {'train_loss': 0.6857476285616557, 'epoch': 5.5, 'lr': 0.1}\n",
      "uniform\n",
      "iter 496 loss 0.6865114005724589 {'train_loss': 0.6865114005724589, 'epoch': 5.511111111111111, 'lr': 0.1}\n",
      "uniform\n",
      "iter 497 loss 0.686889682759179 {'train_loss': 0.686889682759179, 'epoch': 5.522222222222222, 'lr': 0.1}\n",
      "uniform\n",
      "iter 498 loss 0.6865562242296007 {'train_loss': 0.6865562242296007, 'epoch': 5.533333333333333, 'lr': 0.1}\n",
      "uniform\n",
      "iter 499 loss 0.6868776456832886 {'train_loss': 0.6868776456832886, 'epoch': 5.544444444444444, 'lr': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "# import matplotlib.pyplot as plt\n",
    "if os.path.exists(f\"./cache/{args.wandb_group_name}\") == False:\n",
    "    os.makedirs(f\"./cache/{args.wandb_group_name}\", exist_ok=True)\n",
    "fileprefix = f\"{args.wandb_group_name}_{args.fileprefix}_K_{args.K}_L_{args.len_context}_hidden_{args.num_hidden_features}_nheads_{args.num_heads}_nlayers_{args.num_layers}_{time.time()}\"\n",
    "dirprefix = f\"./cache/{args.wandb_group_name}/{fileprefix}\"\n",
    "if os.path.exists(dirprefix) == False:\n",
    "    os.makedirs(dirprefix, exist_ok=True)\n",
    "exp_name = f\"{dirprefix}/{fileprefix}.pkl\"\n",
    "print(\"Saving to\", exp_name)\n",
    "num_iters_per_epoch = 50\n",
    "num_apppearances = np.zeros(args.K)\n",
    "test_every = 1 # np.log10(args.K).astype(int) * 2\n",
    "all_sequences_across_switches = {\n",
    "    \"sequences\": [],\n",
    "    \"switch_start_iter\": [],\n",
    "}\n",
    "# save all sequences in pickle file\n",
    "all_sequences_across_switches[\"sequences\"].append(copy.deepcopy(train_dataset.sequences))\n",
    "all_sequences_across_switches[\"switch_start_iter\"].append([0] * len(train_dataset.sequences))\n",
    "with open(f\"{dirprefix}/{fileprefix}_all_sequences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_sequences_across_switches, f)\n",
    "for iter in range(args.num_iters // num_iters_per_epoch):\n",
    "    # Switch the sequences half way through the training\n",
    "    if iter == args.num_iters // num_iters_per_epoch / 2 and args.is_resample_tasks == \"True\": # resample the tasks\n",
    "        train_dataset.generate_sequences()\n",
    "        all_sequences_across_switches[\"sequences\"].append(copy.deepcopy(train_dataset.sequences))\n",
    "        all_sequences_across_switches[\"switch_start_iter\"].append([iter] * len(train_dataset.sequences))\n",
    "        with open(f\"{dirprefix}/{fileprefix}_all_sequences.pkl\", \"wb\") as f:\n",
    "            pickle.dump(all_sequences_across_switches, f)\n",
    "        \n",
    "        if args.sequence_sampling_distribution == \"zipf\":\n",
    "            iwl_dataset.sequences = train_dataset.sequences[::10] # take every 10th sequence from train_dataset\n",
    "            iwl_test_loader = torch.utils.data.DataLoader(iwl_dataset,\n",
    "                                            sampler=val_sampler,\n",
    "                                            **{'batch_size': args.batch_size, 'num_workers': args.workers,\n",
    "                                        \"shuffle\": False,\n",
    "                                        'pin_memory': True})\n",
    "        num_apppearances = np.zeros(args.K)\n",
    "        \n",
    "    # Switch the sequences several times throughout the training\n",
    "    # Save the sequences and the start iter of the switch, so that\n",
    "    # we can plot forgetting curves across different switches\n",
    "    elif args.is_resample_tasks == \"Forget\" and iter % 100 == 0:\n",
    "        train_dataset.generate_sequences()\n",
    "        all_sequences_across_switches[\"sequences\"].append(copy.deepcopy(train_dataset.sequences[::50]))\n",
    "        all_sequences_across_switches[\"switch_start_iter\"].append([iter] * len(train_dataset.sequences[::50]))\n",
    "        num_apppearances = np.zeros(args.K)\n",
    "        if args.sequence_sampling_distribution == \"zipf\":\n",
    "            iwl_dataset.sequences = torch.cat(all_sequences_across_switches[\"sequences\"], dim=0)\n",
    "            iwl_dataset.len_data = len(iwl_dataset.sequences)\n",
    "            # print(\"iter\", iter, \"len(iwl_dataset.sequences)\", iwl_dataset.sequences.shape)\n",
    "            iwl_test_loader = torch.utils.data.DataLoader(iwl_dataset,\n",
    "                                            sampler=val_sampler,\n",
    "                                            **{'batch_size': args.batch_size, 'num_workers': args.workers,\n",
    "                                        \"shuffle\": False,\n",
    "                                        'pin_memory': True})\n",
    "    logs = {\n",
    "        \"num_apppearances\": copy.deepcopy(num_apppearances),\n",
    "    }\n",
    "    if args.sequence_sampling_distribution == \"zipf\"  :\n",
    "        if (iter % test_every == 0):\n",
    "            test_metrics = validate_gradient_descent_zipf(iter, iwl_test_loader, model, args, criterion, device)\n",
    "        else:\n",
    "            test_metrics = {}\n",
    "    else:\n",
    "        test_losses, test_top1 = validate_gradient_descent(iter, train_loader, model, args, criterion, device)\n",
    "        \n",
    "    if args.is_probe == \"True\" and iter % 100 == 0:\n",
    "        probe_metrics = probe_gradient_descent_uniform(iter, probe_loader, model, args, criterion, device, get_model) \n",
    "        \n",
    "    losses = utils.AverageMeter('Loss', ':.4e')\n",
    "    ridge_losses = utils.AverageMeter('Ridge Loss', ':.4e')\n",
    "    top1 = utils.AverageMeter('Acc@1', ':6.2f')\n",
    "    \n",
    "    \n",
    "    model.train() # switch to train mode\n",
    "    # train iteration\n",
    "    appearances = []\n",
    "    loss_per_appearance = []\n",
    "    for i, (seq, task) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        seq, task = seq.to(device), task.to(device)\n",
    "        batch_num_appearances = torch.bincount(task, minlength=args.K).detach().cpu().numpy()\n",
    "        num_apppearances += batch_num_appearances # update the number of appearances of each task\n",
    "        \n",
    "        # print (\"seq\", seq.shape, \"task\", task.shape, batch_num_appearances[:20])\n",
    "        output = model(seq, task)\n",
    "        B, N, D = output.shape\n",
    "        preds = output # shape: (B, L, 2)\n",
    "        # loss function: cross-entropy loss\n",
    "        # output at position i should be the input at position i+1\n",
    "        \n",
    "        # Write a function to compute the binary cross-entropy for each position in the sequence\n",
    "        # But don't compute the mean, keep the vector dimension\n",
    "        # loss = criterion(preds[:,:-1,:].reshape(B * (N-1), D), seq[:,1:].reshape(B * (N-1)))\n",
    "        logsoftmax = F.log_softmax(preds[:,:-1,:], dim=-1).reshape(B * (N-1), D)\n",
    "        logsoftmaxloss = criterion(logsoftmax, seq[:,1:].reshape(B * (N-1)))\n",
    "        logsoftmaxloss = logsoftmaxloss.reshape(B, N-1).mean(dim=-1) # shape: (B,)\n",
    "        appearances.append(task.detach().cpu().numpy())\n",
    "        loss_per_appearance.append(logsoftmaxloss.detach().cpu().numpy())\n",
    "        loss = logsoftmaxloss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), seq.size(0)) \n",
    "        \n",
    "    # scheduler.step()\n",
    "    # print (\"loss\" , loss, \"preds\", preds.shape, \"target\", seq.shape)\n",
    "    \n",
    "    logs.update({\n",
    "            \"train_loss\": losses.avg,\n",
    "            \"epoch\": iter,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],  \n",
    "            \"loss_per_appearance\": (loss_per_appearance),\n",
    "            \"appearances\": (appearances),\n",
    "        })\n",
    "    print(\"iter\", iter, \"loss\", losses.avg)\n",
    "    if iter == args.num_iters - 1: \n",
    "        if args.sequence_sampling_distribution == \"zipf\":\n",
    "            test_metrics = validate_gradient_descent_zipf(iter, iwl_test_loader, model, args, criterion, device)\n",
    "        else:\n",
    "            test_losses, test_top1 = validate_gradient_descent(iter, train_loader, model, args, criterion, device)            \n",
    "    if args.sequence_sampling_distribution == \"zipf\":\n",
    "        logs[\"test_metrics\"] = test_metrics\n",
    "    elif args.sequence_sampling_distribution == \"uniform\":\n",
    "        for i in range(args.len_context):\n",
    "            logs[f\"test_loss_{i}\"] = test_losses[i].avg\n",
    "            logs[f\"test_top1_{i}\"] = test_top1[i].avg\n",
    "    \n",
    "    if args.wandb_log:\n",
    "        wandb.log(logs)\n",
    "    else:\n",
    "        # wandb.log(logs)\n",
    "        record[\"logs\"].append(logs)\n",
    "    \n",
    " \n",
    "    # save phi_xt_list_epoch \n",
    "\n",
    "    if iter % 50 == 0 and args.wandb_log != True:\n",
    "        record[\"model\"] = model.state_dict()\n",
    "        with open(exp_name, \"wb\") as f:\n",
    "            pickle.dump(record, f)\n",
    "            \n",
    "        # save model state_dict\n",
    "        # with open(f\"{dirprefix}/{fileprefix}_model_iter_{iter}.pkl\", \"wb\") as f:\n",
    "            # pickle.dump(model.state_dict(), f)\n",
    "        # use json\n",
    "        # with open(exp_name, \"w\") as f:\n",
    "            # json.dump(record, f)\n",
    "  \n",
    "if args.wandb_log != True:\n",
    "    with open(exp_name, \"wb\") as f:\n",
    "        pickle.dump(record, f)\n",
    "        \n",
    "    with open(f\"{dirprefix}/{fileprefix}_all_sequences.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_sequences_across_switches, f)\n",
    "        \n",
    "    # save model state_dict\n",
    "    # with open(f\"{dirprefix}/{fileprefix}_model_iter_{iter}.pkl\", \"wb\") as f:\n",
    "        # pickle.dump(model.state_dict(), f)\n",
    "print(\"Finished training\")\n",
    "sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fmri)",
   "language": "python",
   "name": "fmri"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
