{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class IsingModel:\n",
    "    def __init__(self, size, T = 1, J = 1, h = 0):\n",
    "        self.size = size # size of lattice\n",
    "        self.T = T # k_B * temperature (default 1)\n",
    "        self.J = J # strength of interaction \n",
    "        self.h = h # strength of magnetic field\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.state = np.random.choice([-1, 1], (self.size, self.size))\n",
    "        \n",
    "    \n",
    "    def update_mh(self, steps = 10000):\n",
    "        for _ in range(steps):\n",
    "            r_ind, c_ind = np.random.choice(self.size, 2)\n",
    "\n",
    "            energy = self.J * self.state[r_ind][c_ind]*(self.state[(r_ind-1)%self.size][c_ind] + \\\n",
    "                                                self.state[(r_ind+1)%self.size][c_ind] + \\\n",
    "                                                self.state[r_ind][(c_ind-1)%self.size] + \\\n",
    "                                                self.state[r_ind][(c_ind+1)%self.size])\n",
    "            energy += -self.h * (self.state[r_ind][c_ind]) # generally absent\n",
    "\n",
    "            prob = min(1, float(np.e**(-2*energy/self.T)))\n",
    "\n",
    "            if np.random.random() < prob:\n",
    "                self.state[r_ind][c_ind] *= -1\n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        cmap = colors.ListedColormap(['purple', 'yellow'])\n",
    "        bounds=[-1,0,1]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "        plt.imshow(self.state, interpolation='nearest', origin='lower',\n",
    "                    cmap=cmap, norm=norm)\n",
    "    def getNN(self, site_indices, site_ranges, num_NN):\n",
    "        '''\n",
    "            site_indices: [i,j], site to get NN of\n",
    "            site_ranges: [Nx,Ny], boundaries of the grid\n",
    "            num_NN: number of nearest neighbors, usually 1\n",
    "            function which gets NN on any d dimensional cubic grid\n",
    "            with a periodic boundary condition\n",
    "        '''\n",
    "\n",
    "        Nearest_Neighbors = list();\n",
    "        for i in range(len(site_indices)):\n",
    "            for j in range(-num_NN,num_NN+1): #of nearest neighbors to include\n",
    "                if(j == 0): continue;\n",
    "                NN = list(deepcopy(site_indices)); #don't want to overwite;\n",
    "                NN[i] = (NN[i] + j)%(site_ranges[i]);\n",
    "                Nearest_Neighbors.append(tuple(NN))\n",
    "        return Nearest_Neighbors;\n",
    "    \n",
    "    def SW_BFS(self, bonded, clusters, start, beta, nearest_neighbors = 1):\n",
    "        '''\n",
    "        function currently cannot generalize to dimensions higher than 2...\n",
    "        main idea is that we populate a lattice with clusters according to SW using a BFS from a root coord\n",
    "        :param lattice: lattice\n",
    "        :param bonded: 1 or 0, indicates whether a site has been assigned to a cluster\n",
    "               or not\n",
    "        :param clusters: dictionary containing all existing clusters, keys are an integer\n",
    "                denoting natural index of root of cluster\n",
    "        :param start: root node of graph (x,y)\n",
    "        :param beta: temperature\n",
    "        :param J: strength of lattice coupling\n",
    "        :param nearest_neighbors: number or NN to probe\n",
    "        :return:\n",
    "        '''\n",
    "        N = self.state.shape;\n",
    "        visited = np.zeros(N); #indexes whether we have visited nodes during\n",
    "                                     #this particular BFS search\n",
    "        if(bonded[tuple(start)] != 0): #cannot construct a cluster from this site\n",
    "            return bonded, clusters, visited;\n",
    "        \n",
    "        p = 1 - np.exp(-2 * beta * self.J); #bond forming probability\n",
    "        \n",
    "        queue = list();\n",
    "        \n",
    "\n",
    "        queue.append(start);\n",
    "        index = tuple(start)\n",
    "        clusters[index] = [index];\n",
    "        cluster_spin = self.state[index]\n",
    "        color = np.max(bonded) + 1;\n",
    "\n",
    "        ## need to make sub2ind work in arbitrary dimensions\n",
    "        \n",
    "        #whatever the input coordinates are\n",
    "        while(len(queue) > 0):\n",
    "            #print(queue)\n",
    "            r = tuple(queue.pop(0));\n",
    "            ##print(x,y)\n",
    "            if(visited[r] == 0): #if not visited\n",
    "                visited[r] = 1;\n",
    "                #to see clusters, always use different numbers\n",
    "                bonded[r] = color;\n",
    "                NN = self.getNN(r,N, nearest_neighbors);\n",
    "                for nn_coords in NN:\n",
    "                    rn = tuple(nn_coords);\n",
    "                    if(self.state[rn] == cluster_spin and bonded[rn] == 0\\\n",
    "                       and visited[rn] == 0): #require spins to be aligned\n",
    "                        random = np.random.rand();\n",
    "                        if (random < p):  # accept bond proposal\n",
    "                            queue.append(rn); #add coordinate to search\n",
    "                            clusters[index].append(rn) #add point to the cluster\n",
    "                            bonded[rn] = color; #indicate site is no longer available\n",
    "        \n",
    "        return bonded, clusters, visited;\n",
    "    \n",
    "    def run_cluster_epoch(self, nearest_neighbors = 1):\n",
    "        \"\"\"\n",
    "        Implements 1 step of the Swendsen Wang algorithm\n",
    "        \"\"\"\n",
    "        #simulation parameters\n",
    "        beta = 1.0 / self.T\n",
    "        Nx, Ny = self.state.shape\n",
    "        \n",
    "        #scan through every element of the lattice\n",
    "\n",
    "        #propose a random lattice site to generate a cluster\n",
    "        bonded = np.zeros((Nx,Ny));\n",
    "        clusters = dict();  # keep track of bonds\n",
    "        ## iterate through the entire lattice to assign bonds\n",
    "        ## and clusters\n",
    "        for i in range(Nx):\n",
    "            for j in range(Ny):\n",
    "                ## at this point, we do a BFS search to create the cluster\n",
    "                bonded, clusters, visited = self.SW_BFS(bonded, clusters, [i,j], beta, nearest_neighbors=1);\n",
    "        \n",
    "        \n",
    "        for cluster_index in clusters.keys():\n",
    "            [x0, y0] = np.unravel_index(cluster_index, (Nx,Ny));\n",
    "            r = np.random.rand();\n",
    "            if(r < 0.5):\n",
    "                for coords in clusters[cluster_index]:\n",
    "                    [x,y] = coords;\n",
    "                    #print(Lattice[x,y], end=', '); #check clusters\n",
    "                    self.state[x,y] = -1*self.state[x,y];\n",
    "\n",
    "        return self.state;\n",
    "    \n",
    "    def update_SW(self, steps = 30):\n",
    "        \"\"\"\n",
    "        Runs some steps of the Swendsen Wang algorithm\n",
    "        \"\"\"\n",
    "        for _ in range(steps):\n",
    "            self.run_cluster_epoch()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Correlated Data\n",
    "also in `./generate_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing\n",
    "data = []\n",
    "for _ in range(1):\n",
    "    i = IsingModel((81), T = 2.269) # change T to sample at different temperatures\n",
    "    i.initialize()\n",
    "    for _ in range(3):\n",
    "        i.update_SW(3) # use the Swendsen-Wang algorithm\n",
    "        # to use the Metropolis-Hastings algorithm do\n",
    "        # i.update_mh(steps=10000)\n",
    "    data.append(i.state)\n",
    "data = np.array(data)\n",
    "i.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for temp in [1, 2.269185, 5]:\\n    data = []\\n    for _ in range(10000):\\n        i = IsingModel((81), 2.269185)\\n        i.initialize()\\n        for _ in range(10):\\n            i.update_SW(1)\\n        data.append(i.state)\\n    data = np.array(data)\\n    np.save(\"ising81x81_temp_{}.npy\".format(temp), data)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating 10000 samples and save them\n",
    "\"\"\"for temp in [1, 2.269185, 5]:\n",
    "    data = []\n",
    "    for _ in range(10000):\n",
    "        i = IsingModel((81), 2.269185)\n",
    "        i.initialize()\n",
    "        for _ in range(10):\n",
    "            i.update_SW(1)\n",
    "        data.append(i.state)\n",
    "    data = np.array(data)\n",
    "    np.save(\"ising81x81_temp_{}.npy\".format(temp), data)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate uncorrelated samples\n",
    "also in `supervised_convnet/generate_uncorrelated_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncorrelated_data \n",
      " [[-1 -1 -1  1  1  1  1  1  1]\n",
      " [-1 -1 -1  1  1  1  1  1  1]\n",
      " [-1 -1 -1  1  1  1  1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1  1  1  1]\n",
      " [ 1  1 -1 -1 -1 -1  1  1 -1]\n",
      " [ 1  1  1 -1 -1 -1  1  1 -1]\n",
      " [-1  1 -1 -1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1  1  1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"supervised_convnet/t_2.269/ising81x81_temp2.269.npy\")[:, :9, :9]\n",
    "# print(\"data\", data[:10])\n",
    "# Create uncorrelated samples\n",
    "uncorrelated_data = []\n",
    "for _ in range(10000):\n",
    "    # Get random block from 10000 samples\n",
    "    sample = np.random.randint(0, 10000, (3, 3))\n",
    "    # Pick horizontal, vertical position of 3x3 block in 9x9 block\n",
    "    horizontal, vertical = np.random.randint(0, 3, (2, 3, 3))\n",
    "    # Concatenate blocks\n",
    "    uncorrelated = []\n",
    "    for i in range(3):\n",
    "        tile = []\n",
    "        for j in range(3):\n",
    "            tile.append(data[sample[i, j], 3*horizontal[i, j]:(3*horizontal[i, j] + 3), \\\n",
    "                    3*vertical[i, j]:(3*vertical[i, j] + 3)])\n",
    "        uncorrelated.append(np.hstack(tile))\n",
    "    uncorrelated_data.append(np.vstack(uncorrelated))\n",
    "\n",
    "uncorrelated_data = np.array(uncorrelated_data)\n",
    "print(\"uncorrelated_data \\n\", uncorrelated_data[0])\n",
    "# np.save(\"../ising81x81_temp5_uncorrelated9x9.npy\", uncorrelated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class SupervisedConvNet(nn.Module):\n",
    "    def __init__(self, filter_size, square_size):\n",
    "        super(SupervisedConvNet, self).__init__()\n",
    "        self.filter_size = filter_size\n",
    "        self.square_size = square_size\n",
    "        self.leakyrelu = torch.nn.LeakyReLU(0.1)\n",
    "        self.conv2d = nn.Conv2d(1, 1, filter_size, padding=0, stride = filter_size)  \n",
    "        self.linear1 = nn.Linear(filter_size ** 2, 1)\n",
    "        # self.linear2 = nn.Linear(100, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layers with relu activation function\n",
    "        layer1 = torch.tanh(self.conv2d(x))\n",
    "        reshape = layer1.view(-1, 1, self.square_size**2)\n",
    "        layer2 = torch.tanh(self.linear1(reshape))\n",
    "        # layer3 = torch.tanh(self.linear2(layer2))\n",
    "        # layer3 = torch.clamp(layer3, 0, 1)\n",
    "        # for row in x:\n",
    "        #     print(\"row\", row)\n",
    "        #     for el in row[0]:\n",
    "        #         print(\"el\", el)\n",
    "        # x = torch.tanh(self.decoder(x))\n",
    "        return reshape, layer2#, layer3\n",
    "\n",
    "class IsingDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.X = data\n",
    "        self.y = label\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr * (0.1 ** (epoch // 200))\n",
    "    print (\"learning rate\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network to distinguish between correlated/uncorrelated samples\n",
    "also in `train.py` files in each temperature subdirectory e.g. `supervised_convnet/t_2.269/train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sys; \n",
    "from supervised_convnet import supervised_convnet\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrelated [[-1 -1 -1 -1  1  1  1  1  1]\n",
      " [-1 -1 -1 -1 -1  1 -1 -1  1]\n",
      " [-1 -1 -1 -1 -1  1 -1 -1 -1]\n",
      " [-1 -1 -1  1  1  1  1  1  1]\n",
      " [-1 -1 -1  1  1  1  1  1  1]\n",
      " [-1 -1 -1  1  1  1  1  1  1]\n",
      " [ 1  1  1  1  1  1  1  1 -1]\n",
      " [ 1  1  1  1  1  1 -1 -1 -1]\n",
      " [-1  1  1  1  1  1 -1 -1 -1]]\n",
      "Correlated [[ 1  1 -1  1 -1 -1 -1 -1  1]\n",
      " [ 1  1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 1  1 -1 -1 -1  1  1  1  1]\n",
      " [-1 -1 -1 -1 -1 -1  1  1  1]\n",
      " [ 1 -1 -1 -1 -1  1  1  1  1]\n",
      " [-1 -1 -1 -1 -1  1  1  1  1]\n",
      " [-1 -1 -1 -1 -1  1  1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1  1  1  1]\n",
      " [-1 -1  1 -1 -1 -1 -1 -1  1]]\n"
     ]
    }
   ],
   "source": [
    "uncorrelated_data = np.load(\"supervised_convnet/t_2.269/ising81x81_temp2.269_uncorrelated9x9.npy\")\n",
    "correlated_data = np.load(\"supervised_convnet/t_2.269/ising81x81_temp2.269.npy\")[:10000,:9,:9]\n",
    "print(\"Uncorrelated\", uncorrelated_data[0])\n",
    "print(\"Correlated\", correlated_data[0])\n",
    "\n",
    "data = np.vstack((uncorrelated_data, correlated_data))\n",
    "label = np.hstack((-np.ones(10000), np.ones(10000)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)\n",
    "# raise ValueError\n",
    "\n",
    "# Create training and test dataloaders\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1000\n",
    "# number of epochs to train the model\n",
    "n_epochs = 500\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "# adjust learning rate?\n",
    "adjust_learning_rate = False\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# build model\n",
    "model = supervised_convnet.SupervisedConvNet(filter_size = 3, square_size = 3)\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_isingdataset = supervised_convnet.IsingDataset(X_train[:3000], y_train[:3000])\n",
    "train_loader = torch.utils.data.DataLoader(train_isingdataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "validate_isingdataset = supervised_convnet.IsingDataset(X_train[-1000:], y_train[-1000:])\n",
    "validate_loader = torch.utils.data.DataLoader(validate_isingdataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Accuracy: 0.47400000000000003 \t Validate_Accuracy: 0.473\n",
      "Epoch: 20 \t Accuracy: 0.49866666666666665 \t Validate_Accuracy: 0.491\n",
      "Epoch: 30 \t Accuracy: 0.5133333333333333 \t Validate_Accuracy: 0.507\n",
      "Epoch: 40 \t Accuracy: 0.512 \t Validate_Accuracy: 0.519\n",
      "Epoch: 50 \t Accuracy: 0.5076666666666666 \t Validate_Accuracy: 0.518\n",
      "Epoch: 60 \t Accuracy: 0.5060000000000001 \t Validate_Accuracy: 0.525\n",
      "Epoch: 70 \t Accuracy: 0.5100000000000001 \t Validate_Accuracy: 0.529\n",
      "Epoch: 80 \t Accuracy: 0.514 \t Validate_Accuracy: 0.53\n",
      "Epoch: 90 \t Accuracy: 0.5133333333333333 \t Validate_Accuracy: 0.535\n",
      "Epoch: 100 \t Accuracy: 0.515 \t Validate_Accuracy: 0.534\n",
      "Epoch: 110 \t Accuracy: 0.5156666666666667 \t Validate_Accuracy: 0.529\n",
      "Epoch: 120 \t Accuracy: 0.5143333333333334 \t Validate_Accuracy: 0.527\n",
      "Epoch: 130 \t Accuracy: 0.5126666666666666 \t Validate_Accuracy: 0.522\n",
      "Epoch: 140 \t Accuracy: 0.512 \t Validate_Accuracy: 0.518\n",
      "Epoch: 150 \t Accuracy: 0.5113333333333333 \t Validate_Accuracy: 0.511\n",
      "Epoch: 160 \t Accuracy: 0.5133333333333333 \t Validate_Accuracy: 0.51\n",
      "Epoch: 170 \t Accuracy: 0.5153333333333333 \t Validate_Accuracy: 0.504\n",
      "Epoch: 180 \t Accuracy: 0.518 \t Validate_Accuracy: 0.506\n",
      "Epoch: 190 \t Accuracy: 0.517 \t Validate_Accuracy: 0.503\n",
      "Epoch: 200 \t Accuracy: 0.516 \t Validate_Accuracy: 0.506\n",
      "Epoch: 210 \t Accuracy: 0.516 \t Validate_Accuracy: 0.501\n",
      "Epoch: 220 \t Accuracy: 0.515 \t Validate_Accuracy: 0.509\n",
      "Epoch: 230 \t Accuracy: 0.52 \t Validate_Accuracy: 0.506\n",
      "Epoch: 240 \t Accuracy: 0.5193333333333333 \t Validate_Accuracy: 0.51\n",
      "Epoch: 250 \t Accuracy: 0.5216666666666666 \t Validate_Accuracy: 0.515\n",
      "Epoch: 260 \t Accuracy: 0.5246666666666667 \t Validate_Accuracy: 0.523\n",
      "Epoch: 270 \t Accuracy: 0.522 \t Validate_Accuracy: 0.526\n",
      "Epoch: 280 \t Accuracy: 0.5196666666666667 \t Validate_Accuracy: 0.521\n",
      "Epoch: 290 \t Accuracy: 0.517 \t Validate_Accuracy: 0.521\n",
      "Epoch: 300 \t Accuracy: 0.5173333333333333 \t Validate_Accuracy: 0.517\n",
      "Epoch: 310 \t Accuracy: 0.516 \t Validate_Accuracy: 0.522\n",
      "Epoch: 320 \t Accuracy: 0.516 \t Validate_Accuracy: 0.522\n",
      "Epoch: 330 \t Accuracy: 0.5183333333333334 \t Validate_Accuracy: 0.515\n",
      "Epoch: 340 \t Accuracy: 0.5156666666666667 \t Validate_Accuracy: 0.519\n",
      "Epoch: 350 \t Accuracy: 0.514 \t Validate_Accuracy: 0.517\n",
      "Epoch: 360 \t Accuracy: 0.5106666666666667 \t Validate_Accuracy: 0.519\n",
      "Epoch: 370 \t Accuracy: 0.5106666666666667 \t Validate_Accuracy: 0.522\n",
      "Epoch: 380 \t Accuracy: 0.51 \t Validate_Accuracy: 0.524\n",
      "Epoch: 390 \t Accuracy: 0.5113333333333333 \t Validate_Accuracy: 0.523\n",
      "Epoch: 400 \t Accuracy: 0.5126666666666667 \t Validate_Accuracy: 0.526\n",
      "Epoch: 410 \t Accuracy: 0.5156666666666667 \t Validate_Accuracy: 0.529\n",
      "Epoch: 420 \t Accuracy: 0.516 \t Validate_Accuracy: 0.529\n",
      "Epoch: 430 \t Accuracy: 0.517 \t Validate_Accuracy: 0.524\n",
      "Epoch: 440 \t Accuracy: 0.5203333333333334 \t Validate_Accuracy: 0.518\n",
      "Epoch: 450 \t Accuracy: 0.5223333333333334 \t Validate_Accuracy: 0.52\n",
      "Epoch: 460 \t Accuracy: 0.5219999999999999 \t Validate_Accuracy: 0.516\n",
      "Epoch: 470 \t Accuracy: 0.5213333333333333 \t Validate_Accuracy: 0.513\n",
      "Epoch: 480 \t Accuracy: 0.5216666666666667 \t Validate_Accuracy: 0.512\n",
      "Epoch: 490 \t Accuracy: 0.522 \t Validate_Accuracy: 0.512\n",
      "Epoch: 500 \t Accuracy: 0.523 \t Validate_Accuracy: 0.515\n",
      "data tensor([[[[-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1., -1., -1.,  1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1.,  1.,  1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1.,  1.,  1.,  1., -1.],\n",
      "          [-1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1.,  1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1.,  1.,  1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1.,  1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1., -1.,  1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.],\n",
      "          [-1.,  1., -1.,  1., -1.,  1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1.,  1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1., -1.,  1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "          [ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1.,  1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]]])\n",
      "output [tensor([-0.0502,  0.1607,  0.1607,  ..., -0.0485,  0.1607, -0.0502],\n",
      "       grad_fn=<ViewBackward>), tensor([-5.1602e-02, -2.3472e-02,  4.6384e-02, -5.8343e-02,  6.8486e-03,\n",
      "        -3.0039e-02,  1.3527e-01,  2.9041e-02, -4.4843e-02, -5.5259e-02,\n",
      "        -4.0755e-03,  2.1281e-02,  5.6229e-02,  4.0157e-02, -8.4067e-03,\n",
      "        -1.9254e-02, -3.2927e-02, -4.1509e-02,  4.5572e-02,  4.5869e-02,\n",
      "         4.3766e-02,  5.6537e-02, -3.4328e-02,  2.9692e-02, -3.6401e-02,\n",
      "        -6.0523e-02,  2.4679e-02, -1.2795e-03,  4.7922e-02, -3.7718e-02,\n",
      "        -1.0553e-02,  1.5410e-02,  6.4084e-02, -2.9001e-02,  4.7755e-02,\n",
      "        -1.9260e-02,  6.9667e-02,  3.3045e-02, -7.3431e-02, -4.3216e-02,\n",
      "        -1.2119e-02,  4.3597e-02,  5.9702e-02,  8.1359e-02,  6.2609e-02,\n",
      "         7.0619e-02, -4.4101e-02, -4.7108e-02,  2.9667e-02,  3.5771e-02,\n",
      "         1.0122e-01,  2.1238e-03,  7.0846e-02, -8.9732e-02,  1.8076e-02,\n",
      "         4.7513e-02,  6.5367e-03,  3.8532e-02, -1.3550e-02, -8.6155e-02,\n",
      "        -5.4784e-03, -6.3118e-02, -3.6331e-02,  3.5694e-02,  3.5569e-02,\n",
      "         1.8000e-02,  2.4796e-02,  3.2027e-02, -9.5245e-03, -6.8793e-02,\n",
      "         4.4471e-03, -7.9377e-02,  2.3972e-02,  6.8375e-02, -1.1140e-01,\n",
      "        -3.4936e-02,  1.0486e-01, -1.0650e-01,  2.9873e-02, -5.9078e-02,\n",
      "        -1.1121e-02, -1.1164e-02, -6.3263e-02,  3.4498e-02, -3.9334e-02,\n",
      "         1.1341e-01,  2.1801e-02,  9.4745e-03, -2.3528e-02,  2.4793e-04,\n",
      "        -8.2801e-02,  3.2023e-03,  4.0152e-02, -3.1945e-02, -4.6116e-02,\n",
      "        -5.2170e-03,  6.5765e-02, -3.6241e-02,  2.4218e-02, -3.0161e-02,\n",
      "        -5.4598e-02,  2.4007e-02,  2.4283e-02, -8.4876e-02,  9.5450e-03,\n",
      "         6.2191e-02, -8.7019e-02, -7.9148e-02, -1.2613e-02, -6.3963e-02,\n",
      "        -2.6156e-02, -6.4601e-04, -5.9695e-03,  8.8508e-02,  2.6762e-02,\n",
      "         3.9289e-02,  5.4779e-02, -9.0678e-02, -5.5835e-02, -6.2460e-02,\n",
      "        -6.9223e-02, -3.2579e-02,  1.2489e-02,  3.2875e-02, -1.4621e-02,\n",
      "        -2.4511e-02, -6.2096e-02, -7.6027e-02,  7.8605e-02, -9.1392e-02,\n",
      "        -2.3522e-02,  1.6298e-02,  6.1685e-02,  1.9321e-02,  1.2101e-02,\n",
      "         5.3209e-02, -5.2058e-03, -1.1226e-01, -4.7812e-02, -4.8626e-02,\n",
      "        -1.9787e-02, -1.3601e-02,  3.3633e-02,  5.6028e-02,  7.8072e-03,\n",
      "        -8.6497e-02, -4.0936e-02, -1.0423e-03,  5.3293e-02,  3.1835e-02,\n",
      "        -1.1401e-02,  4.1678e-04,  1.2085e-02, -1.2789e-02, -3.5841e-02,\n",
      "        -7.5348e-02,  6.7106e-02,  2.7967e-03,  2.2502e-02, -3.0557e-02,\n",
      "        -4.1971e-02,  4.6558e-02, -2.0067e-02,  6.7764e-02,  5.3222e-02,\n",
      "        -5.5987e-02, -2.9383e-02, -2.4757e-03, -5.2638e-02,  1.1753e-02,\n",
      "         6.8965e-02, -1.8475e-02,  9.7961e-03,  5.6983e-02, -2.0952e-02,\n",
      "         7.3628e-02,  8.5727e-02,  3.7677e-02,  1.9767e-02,  4.4278e-02,\n",
      "        -2.4395e-03,  2.1969e-02,  5.9939e-03,  1.0962e-02, -4.8041e-02,\n",
      "        -2.9395e-02, -1.1343e-01,  9.4525e-03,  3.7770e-03,  3.0714e-02,\n",
      "         3.1671e-02, -3.8189e-02, -1.2420e-02, -7.9300e-02, -6.1672e-03,\n",
      "        -6.6047e-02,  3.9087e-02,  5.1740e-02,  5.8551e-03, -1.6399e-02,\n",
      "        -4.6037e-02,  3.4876e-03, -4.1456e-02, -4.9800e-02, -1.3886e-01,\n",
      "         3.5818e-02,  1.0677e-02, -4.3340e-02, -3.8925e-02, -3.8299e-02,\n",
      "        -1.1240e-01,  3.0766e-03,  1.6065e-02,  6.3902e-02,  3.7985e-03,\n",
      "         3.0590e-02, -1.1584e-02, -9.7363e-02,  1.4337e-01,  7.5164e-02,\n",
      "         8.2965e-02,  1.9359e-02,  4.5905e-02, -5.7097e-02, -1.2965e-02,\n",
      "        -3.8730e-02,  4.6439e-02,  2.4045e-02, -2.1167e-02,  2.0853e-02,\n",
      "         6.7936e-02, -2.4985e-02,  4.1272e-02,  7.9189e-02, -1.3409e-02,\n",
      "         4.3486e-02,  9.3942e-03,  1.3647e-02, -1.1870e-04,  9.2063e-02,\n",
      "         1.1704e-02, -1.1225e-03,  4.7883e-02,  6.2739e-02, -6.6755e-02,\n",
      "         6.6615e-03,  8.7756e-04, -1.4765e-01, -1.0607e-01, -4.8075e-02,\n",
      "        -3.7954e-03, -5.1176e-02,  5.3888e-02, -5.2463e-02, -2.5804e-02,\n",
      "        -4.9191e-02,  5.7588e-03,  2.7901e-02, -3.4314e-02,  9.8806e-02,\n",
      "         2.9551e-02, -2.6993e-02,  3.8993e-02,  5.7063e-02, -1.1848e-02,\n",
      "         1.7269e-02, -1.0915e-01, -7.9970e-03,  4.2878e-02, -2.0388e-02,\n",
      "         2.8356e-02,  3.2285e-02, -3.0738e-02,  2.4263e-02, -4.4782e-02,\n",
      "         9.2497e-02,  6.2679e-03, -3.2611e-02, -9.0241e-03, -3.0154e-02,\n",
      "         3.2164e-02, -1.0367e-02, -5.5976e-04,  1.3623e-02,  3.8815e-02,\n",
      "        -1.0448e-01,  4.8967e-02,  5.6191e-02, -2.9585e-02, -1.1967e-01,\n",
      "         7.8329e-02,  2.9364e-02,  7.0965e-02, -1.4039e-01, -1.0739e-01,\n",
      "         6.5203e-02, -5.4431e-03, -4.1127e-02, -7.0438e-02,  2.3708e-02,\n",
      "         2.9733e-02, -2.7163e-02,  3.5827e-02,  3.2539e-02, -2.3494e-02,\n",
      "         4.6399e-02, -2.2988e-02,  7.9220e-02, -4.2761e-02, -7.3891e-06,\n",
      "        -7.9567e-02,  1.4179e-02,  7.4805e-02, -9.8242e-02,  8.3393e-02,\n",
      "         2.7477e-02, -2.8346e-02,  1.9564e-02, -2.9022e-03,  2.0099e-02,\n",
      "        -1.7397e-02, -1.1402e-01,  1.4480e-01, -4.5049e-02, -5.8910e-03,\n",
      "         3.0019e-02,  2.4724e-02, -7.3251e-02, -1.2102e-01,  6.1742e-02,\n",
      "         2.8929e-02, -8.5868e-02,  7.8013e-02, -1.6097e-02,  3.7876e-02,\n",
      "        -2.0358e-02,  6.0222e-02,  2.6274e-02,  6.6334e-02,  2.2575e-02,\n",
      "         2.9365e-02, -1.2401e-01,  3.5276e-03,  2.7492e-02,  1.2148e-02,\n",
      "        -8.6363e-02, -4.4941e-02,  7.0032e-02, -6.6346e-03, -2.9033e-02,\n",
      "         2.3658e-02, -2.5545e-02, -3.5985e-02,  7.7074e-02, -3.5350e-02,\n",
      "         9.8914e-02, -2.0228e-02,  3.0851e-02,  1.7641e-02, -2.7370e-02,\n",
      "         7.3526e-03, -1.0721e-02, -7.0190e-03, -1.9013e-03, -1.3491e-02,\n",
      "         8.2266e-02, -3.6117e-02,  6.1082e-02,  5.1400e-02, -3.0042e-02,\n",
      "         1.7283e-02, -2.2335e-02, -1.0365e-01, -5.1254e-02,  2.4409e-02,\n",
      "         6.1883e-02,  5.1216e-02, -5.2484e-02, -2.7687e-03,  7.0126e-02,\n",
      "         3.5217e-02, -3.4084e-02,  6.4179e-03, -1.1657e-03,  6.6409e-02,\n",
      "         4.2626e-02,  8.1349e-02, -1.7354e-02, -3.3567e-02, -3.3682e-02,\n",
      "         7.7912e-03,  8.8367e-03,  9.0149e-02,  2.2225e-02,  6.0827e-02,\n",
      "        -3.3910e-02, -1.1835e-01,  1.8323e-02,  1.1869e-02, -1.1569e-01,\n",
      "         3.1200e-02,  1.4361e-01,  2.0435e-02, -3.7671e-03,  1.1573e-01,\n",
      "        -2.8323e-02,  1.4952e-02, -7.4684e-02, -1.6474e-02, -3.5442e-02,\n",
      "        -1.0373e-02, -4.9742e-02, -3.6777e-02,  2.5536e-02,  8.2388e-02,\n",
      "        -9.7197e-02,  5.2765e-02, -7.6663e-02,  2.3692e-02,  2.5904e-02,\n",
      "        -2.5557e-02, -6.6328e-02,  9.8233e-02, -3.2960e-02,  2.9515e-03,\n",
      "        -1.6878e-03, -2.2982e-02, -3.2461e-02, -5.0393e-02, -3.5637e-02,\n",
      "        -3.1475e-02, -3.4268e-02, -5.6778e-02,  4.9968e-02,  8.3994e-02,\n",
      "         5.9356e-02,  4.0316e-02,  6.0433e-02,  7.1818e-02,  2.5766e-02,\n",
      "         2.6942e-02,  8.8455e-02,  7.1830e-02, -1.8538e-02, -7.4566e-03,\n",
      "        -7.8941e-02,  1.3614e-02, -2.4977e-02,  2.5869e-02,  1.4886e-02,\n",
      "         1.7512e-03,  6.1240e-02, -3.6676e-03,  1.0820e-01,  3.3133e-02,\n",
      "        -6.8427e-02,  8.5813e-02,  1.4371e-02,  2.6395e-02,  1.0628e-02,\n",
      "        -2.2842e-02, -5.1516e-02, -1.7305e-02, -1.1332e-01,  9.1347e-03,\n",
      "         2.9102e-02,  3.3557e-03, -1.6627e-01, -8.8277e-02,  5.7249e-02,\n",
      "        -7.9267e-02, -6.8287e-02, -2.0054e-02, -3.7173e-02, -4.7328e-02,\n",
      "         7.9511e-02, -2.8978e-02, -5.1873e-02,  1.9225e-02,  1.0354e-01,\n",
      "        -4.6102e-02, -1.1408e-02,  5.2984e-02, -9.5662e-02,  3.0442e-02,\n",
      "        -4.0044e-02,  1.7211e-03, -5.5230e-02,  1.0079e-01, -8.6562e-02,\n",
      "         4.6337e-02,  2.3171e-02,  6.9114e-02,  1.1283e-02,  8.9688e-02,\n",
      "         1.3769e-01,  1.1133e-01,  2.5316e-02,  2.3370e-02,  3.0547e-02,\n",
      "         1.7626e-02, -6.3254e-02, -9.1194e-03, -7.0404e-02,  3.0555e-02,\n",
      "        -2.9676e-02, -1.2886e-02,  1.8765e-03, -3.3560e-02, -4.9190e-02,\n",
      "        -1.4889e-02, -7.7679e-02,  6.7720e-02, -6.6124e-02, -6.7681e-02,\n",
      "         3.1075e-02, -2.1132e-02,  3.8780e-02,  1.0594e-02,  9.5962e-03,\n",
      "        -5.6015e-02, -1.7086e-02, -2.6512e-03, -4.4988e-03, -8.1914e-02,\n",
      "         5.0465e-02, -6.3725e-02, -2.2117e-02, -3.9906e-02, -5.9284e-03,\n",
      "        -1.9989e-02, -6.8836e-02,  2.6914e-02,  1.5876e-02,  1.0837e-01,\n",
      "         4.0860e-02, -1.1059e-01,  1.2190e-02, -7.8206e-02,  6.7463e-03,\n",
      "        -2.0013e-02, -2.0743e-03,  5.4106e-02, -8.2282e-02,  1.0658e-01,\n",
      "        -6.3049e-02,  9.7433e-02, -5.6657e-02,  2.1731e-02,  4.1193e-02,\n",
      "        -5.2751e-02,  2.6700e-02,  1.0297e-01,  3.5840e-02,  1.8049e-02,\n",
      "        -4.3489e-03,  1.5496e-02,  1.0848e-02,  6.3190e-02,  1.0008e-01,\n",
      "        -9.0623e-03, -7.3763e-02,  9.1951e-02,  1.4205e-03, -5.3563e-02,\n",
      "        -1.9260e-03, -4.7554e-02, -6.8917e-03, -6.1420e-02, -5.9349e-02,\n",
      "         3.4944e-02,  7.0943e-02,  3.5618e-02,  2.9613e-02, -5.3735e-02,\n",
      "         2.1940e-02,  7.1589e-02,  1.7239e-02,  2.8303e-02,  1.2540e-02,\n",
      "        -4.0461e-02, -1.0045e-01, -3.1866e-02, -3.2314e-02, -1.1176e-01,\n",
      "        -6.5634e-02,  3.4871e-02,  2.6934e-02,  6.1240e-02, -2.3430e-02,\n",
      "         1.1073e-02,  5.6218e-02, -6.4137e-03,  5.3600e-02, -4.3870e-02,\n",
      "         1.6291e-02,  3.4908e-02, -4.9073e-02, -1.2130e-02,  6.3486e-02,\n",
      "        -6.4596e-02,  4.3964e-02, -3.1576e-02, -6.4237e-02,  8.2533e-02,\n",
      "        -6.7269e-02, -3.6419e-02,  4.1980e-02, -2.1514e-02,  3.3949e-02,\n",
      "        -3.1930e-02,  4.1905e-02, -1.0038e-01,  3.2068e-02,  5.7953e-02,\n",
      "         4.3218e-02,  7.6459e-02, -6.1852e-03,  4.4524e-02, -1.4034e-02,\n",
      "         1.9740e-02,  4.4936e-02,  5.4099e-02, -4.1855e-02, -2.0938e-02,\n",
      "         3.5816e-03,  1.0995e-01, -8.0361e-02, -2.6058e-02, -9.3282e-03,\n",
      "         1.5463e-02,  1.7869e-02,  9.3783e-03,  5.4295e-02, -1.5775e-02,\n",
      "         9.6395e-03,  5.5583e-02,  1.4738e-02,  2.2400e-02, -8.1731e-02,\n",
      "         9.0745e-02,  2.4754e-03, -6.1501e-02, -1.7617e-02,  6.0371e-02,\n",
      "        -4.3348e-02, -3.8071e-02, -3.5236e-03,  2.3570e-02, -7.3567e-02,\n",
      "        -7.8579e-02, -7.1673e-02,  1.4176e-02, -2.5271e-03, -1.0141e-01,\n",
      "         3.0745e-02, -1.1499e-02,  6.6414e-02,  6.0995e-02,  1.1865e-01,\n",
      "        -8.8534e-02,  3.4368e-02,  5.2455e-02,  3.3130e-03, -3.9156e-04,\n",
      "        -1.8515e-02,  2.0381e-02,  5.7153e-02,  5.5956e-03, -1.9781e-02,\n",
      "         4.2702e-02, -8.6664e-02, -1.6920e-02, -6.0059e-02, -3.1262e-03,\n",
      "        -2.7014e-02,  3.5608e-02, -2.8233e-03, -5.7555e-02, -6.0086e-02,\n",
      "        -5.4874e-02,  3.3547e-02, -1.0183e-02,  6.4392e-02, -8.0474e-03,\n",
      "        -2.2254e-02,  4.2389e-02, -3.1911e-02, -7.3300e-02, -8.5214e-02,\n",
      "         9.3258e-02, -1.0827e-02,  1.5094e-02,  4.1691e-03,  5.6491e-02,\n",
      "         1.7604e-02, -9.6421e-02,  3.4904e-02, -5.5630e-02, -3.7462e-02,\n",
      "         4.5679e-02, -3.3534e-03,  4.0146e-02,  2.9490e-02,  2.8217e-02,\n",
      "        -1.9613e-02, -3.3675e-02,  5.1231e-02, -5.1801e-02, -6.0156e-02,\n",
      "        -4.5533e-02, -1.0285e-02, -3.7548e-02,  1.0114e-01, -2.0176e-02,\n",
      "         1.2117e-03,  8.1361e-02, -7.8278e-02, -7.1745e-02,  1.3271e-02,\n",
      "        -4.0388e-02,  2.6976e-02,  2.9005e-02,  2.3138e-02, -1.8423e-02,\n",
      "        -5.4165e-02, -4.5496e-02,  3.7474e-02, -2.6733e-02, -6.6146e-02,\n",
      "         1.2154e-02,  7.8755e-03,  6.4307e-02,  3.7890e-03,  1.0985e-01,\n",
      "         3.3108e-02, -2.0876e-02,  6.6177e-03,  3.2607e-02,  2.7582e-02,\n",
      "         8.1110e-02, -1.4484e-02,  2.6822e-02,  8.9957e-04,  4.7074e-04,\n",
      "        -6.0737e-02, -4.4633e-02,  8.2229e-02, -7.2723e-02,  6.1185e-03,\n",
      "        -1.9875e-02,  2.3532e-02, -7.0372e-02, -3.8602e-02,  2.6334e-02,\n",
      "         1.1001e-02,  1.8080e-02, -1.5090e-02,  4.1920e-02,  3.3998e-03,\n",
      "        -1.5307e-02, -5.9163e-02,  1.2460e-03, -3.8289e-02,  1.2808e-02,\n",
      "         1.7829e-02,  5.8083e-02, -2.3146e-02,  7.9424e-02,  5.1627e-02,\n",
      "        -8.8765e-02, -1.5167e-02,  8.4686e-02, -3.2581e-02,  6.9562e-03,\n",
      "         1.9349e-02, -8.4843e-02, -3.8680e-02, -4.3472e-02,  1.0523e-01,\n",
      "         7.4699e-02,  4.4469e-02, -3.1463e-02, -6.3614e-02,  5.1920e-02,\n",
      "         1.1309e-01, -3.2347e-02, -4.5140e-02, -2.8787e-02,  6.8629e-02,\n",
      "        -6.9468e-02, -1.0368e-02, -8.6157e-02,  2.8543e-02,  1.0033e-01,\n",
      "        -7.1318e-02,  6.9211e-02,  1.9068e-02,  8.4958e-02, -2.1034e-02,\n",
      "         9.4081e-02, -8.3846e-02, -2.7268e-02,  8.6853e-02,  3.9644e-02,\n",
      "        -5.5618e-02,  6.7979e-02, -6.5317e-02, -4.8611e-04,  5.7821e-02,\n",
      "         7.3509e-03,  5.0898e-02,  9.2735e-02, -1.4588e-02, -1.0321e-02,\n",
      "        -9.9120e-03,  9.2583e-02,  5.0384e-02,  1.2598e-02, -7.3537e-02,\n",
      "        -5.1370e-02,  8.4306e-02,  3.0870e-02, -1.3953e-02,  1.1794e-02,\n",
      "         4.7852e-02,  2.9298e-02,  5.2514e-02,  4.0197e-02, -6.4536e-02,\n",
      "         1.1366e-01,  3.7157e-02,  7.3367e-02,  4.7820e-02,  5.0849e-02,\n",
      "        -8.0261e-03,  1.5700e-02,  4.2516e-02, -2.3254e-02, -1.0914e-01,\n",
      "        -4.5214e-02,  5.5454e-03,  3.7810e-02, -1.0658e-01, -1.8840e-02,\n",
      "         4.4124e-02, -1.9465e-02, -2.6240e-02,  2.3440e-02, -1.1190e-03,\n",
      "         5.5392e-02, -3.9770e-03, -1.3961e-02,  1.7524e-02, -6.3003e-02,\n",
      "         2.7161e-02,  8.9684e-02, -2.1840e-02, -3.0296e-02,  1.5307e-02,\n",
      "         1.0757e-03,  4.3916e-02, -3.6143e-02,  2.8838e-02, -1.0338e-01,\n",
      "         4.1496e-03, -6.2689e-02,  2.0373e-03, -6.1288e-02, -3.8449e-02,\n",
      "         2.9438e-02, -4.3081e-02, -2.2638e-03, -5.1772e-02, -7.8672e-02,\n",
      "        -6.7019e-02, -4.4224e-02, -1.2067e-01, -6.3485e-02,  1.4113e-02,\n",
      "        -1.1279e-01,  8.9602e-02,  2.7583e-02, -2.0147e-03,  5.1637e-03,\n",
      "         1.2914e-02, -3.0388e-02, -3.3078e-02,  7.2908e-03, -2.5129e-02,\n",
      "         3.1871e-02, -5.9315e-02, -4.1648e-02, -7.6370e-02, -5.1248e-02,\n",
      "         4.2740e-02,  2.6538e-02, -2.8531e-02, -2.6085e-02, -4.1891e-04,\n",
      "         2.8667e-02,  7.8572e-02, -7.9433e-03, -6.4122e-02,  1.9688e-02,\n",
      "         1.2029e-03, -9.8728e-02, -5.8200e-03, -2.9984e-02, -1.2553e-01,\n",
      "        -3.4404e-02,  3.1177e-04,  4.3559e-02, -1.6416e-01,  4.3898e-02,\n",
      "         7.3823e-02,  8.8430e-03, -9.2716e-03,  2.0404e-02, -1.5283e-02,\n",
      "        -8.2821e-03, -3.5543e-02, -5.6676e-02,  2.8638e-02,  5.9094e-02,\n",
      "         4.2158e-02, -2.6200e-02,  6.1964e-02, -1.8891e-02,  9.1691e-02,\n",
      "         2.5320e-02, -8.9163e-03,  1.0630e-01,  2.8775e-02,  3.0331e-02,\n",
      "         4.9602e-03,  2.7824e-04, -7.3384e-02, -5.7555e-02, -4.0156e-02,\n",
      "        -3.1520e-02,  3.9240e-03,  3.4710e-02, -5.1506e-02, -2.8200e-02,\n",
      "        -8.7375e-02,  1.2281e-02,  5.1692e-02, -9.0332e-02, -4.1058e-02,\n",
      "         5.2291e-03, -7.4597e-02, -3.9305e-02, -5.5065e-03,  1.3437e-02,\n",
      "        -1.6055e-02, -3.5038e-02,  7.3103e-02,  1.9163e-02, -4.9154e-02,\n",
      "         4.9913e-02,  1.4605e-02,  1.0974e-02,  9.7872e-03,  7.7294e-02,\n",
      "         4.9305e-02, -5.4443e-02,  2.4554e-02,  5.4694e-02,  1.2955e-02,\n",
      "         4.6426e-02, -4.3787e-02, -2.0927e-02, -4.3155e-02,  5.8108e-02,\n",
      "         6.6302e-03, -9.8281e-03, -1.4037e-02,  6.0241e-02, -5.7152e-02,\n",
      "         5.1581e-02,  6.2615e-02,  7.0372e-02, -6.3897e-02,  6.3624e-03,\n",
      "        -1.2878e-03, -4.9189e-02,  8.5128e-02, -8.0329e-03, -7.9170e-02,\n",
      "         4.6428e-02, -8.7959e-02, -6.1163e-02,  3.1649e-02, -1.0148e-01,\n",
      "        -5.0423e-02,  5.2680e-02, -3.7768e-02, -2.8732e-02, -2.6877e-02],\n",
      "       grad_fn=<ViewBackward>)]\n",
      "target tensor([-1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.])\n",
      "correlated model(v) (tensor([[[-0.0502, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502,\n",
      "          -0.0502, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[-0.0321]]], grad_fn=<TanhBackward>))\n",
      "uncorrelated model(v) (tensor([[[ 0.1607,  0.1607, -0.0502,  0.1607,  0.1607, -0.0502, -0.0502,\n",
      "           0.1607, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[0.0084]]], grad_fn=<TanhBackward>))\n",
      "data tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.],\n",
      "          [ 1., -1.,  1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "          [-1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1., -1., -1.],\n",
      "          [-1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [-1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1.,  1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1., -1.,  1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [-1., -1.,  1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
      "          [-1.,  1.,  1., -1., -1., -1., -1.,  1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1.,  1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1.,  1., -1., -1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]]])\n",
      "output [tensor([-0.0502, -0.0502, -0.2569,  ...,  0.1090, -0.1585,  0.1071],\n",
      "       grad_fn=<ViewBackward>), tensor([-3.4357e-02,  2.4263e-04, -1.9304e-02,  2.6836e-03,  5.0409e-02,\n",
      "         2.0606e-02, -6.4375e-02, -8.2314e-03,  1.0917e-01, -8.8613e-02,\n",
      "        -7.3600e-02,  6.6194e-02,  3.5828e-02, -3.9056e-02, -1.0159e-01,\n",
      "        -2.6615e-02, -6.6424e-02, -5.1023e-02, -4.4181e-03,  2.6229e-02,\n",
      "         6.0099e-03, -8.5807e-02, -3.2114e-02, -1.3142e-01,  1.6554e-02,\n",
      "         8.5824e-03, -1.6894e-02,  5.2402e-02, -7.6924e-02,  9.3824e-03,\n",
      "         5.9888e-02,  2.6543e-02,  1.3701e-02, -4.4502e-02, -3.0849e-02,\n",
      "         5.8652e-02, -3.5310e-02, -5.3356e-02,  1.6710e-03,  1.2158e-01,\n",
      "        -6.4166e-02,  1.0261e-01,  3.2092e-02,  3.6409e-02, -1.1453e-03,\n",
      "        -5.0528e-02, -3.9574e-02, -2.4935e-03,  6.5439e-02,  3.5204e-02,\n",
      "         7.5534e-02, -2.2586e-02,  7.4921e-05, -3.4798e-02, -1.9102e-02,\n",
      "        -2.8970e-02, -9.9812e-02, -5.2512e-02, -1.3548e-02,  6.1174e-03,\n",
      "        -2.2139e-02,  3.3512e-02,  1.3861e-01, -4.4362e-03,  3.4588e-02,\n",
      "        -1.3081e-02,  1.3807e-02, -7.5425e-02,  6.3055e-02,  1.4098e-02,\n",
      "         9.4719e-03,  9.3190e-02, -2.1447e-02,  7.8199e-02, -1.6572e-01,\n",
      "        -2.7938e-02,  9.7419e-03,  5.4344e-02, -9.1447e-03, -5.5315e-02,\n",
      "        -3.8324e-02, -2.8402e-02, -2.8892e-02, -9.1234e-02,  5.0109e-02,\n",
      "        -1.5361e-02,  3.0345e-04, -3.0494e-03, -3.0387e-02, -4.4899e-02,\n",
      "        -2.7038e-02,  2.6463e-02, -6.2481e-02,  4.9937e-02, -1.5214e-01,\n",
      "         8.7793e-03, -4.6374e-02, -1.1166e-02,  3.7909e-02, -1.2388e-01,\n",
      "        -9.4855e-05, -6.6055e-02, -9.2652e-02,  1.6713e-02,  3.9341e-02,\n",
      "        -5.2482e-03,  3.6641e-02, -8.2333e-02,  2.9154e-02,  1.9678e-02,\n",
      "        -3.0218e-02, -3.9968e-03, -3.8329e-02,  1.1445e-02, -1.8095e-02,\n",
      "         2.6184e-02, -2.2203e-02, -5.6140e-02, -7.0677e-02,  7.3493e-02,\n",
      "         3.5737e-02, -4.5469e-02, -4.2997e-02, -4.7540e-02,  2.9297e-02,\n",
      "         1.4052e-02,  5.7729e-03, -5.5921e-02, -1.2018e-02,  7.3077e-03,\n",
      "        -7.9624e-02,  3.5931e-02,  5.4077e-02,  5.7456e-02,  5.6578e-02,\n",
      "         4.1524e-02, -4.9543e-02, -2.6807e-03, -5.4728e-02,  2.8522e-02,\n",
      "         9.9731e-02,  1.5575e-02,  9.2300e-03, -2.2620e-03, -2.2764e-02,\n",
      "         6.1272e-02, -3.6588e-02, -3.1016e-02,  2.8667e-02,  3.2850e-03,\n",
      "        -2.2461e-02, -3.8620e-02, -2.6109e-03,  1.0778e-01, -2.2629e-02,\n",
      "        -3.5858e-02, -1.2553e-01,  6.3530e-02,  7.1028e-02, -2.3249e-02,\n",
      "         7.5806e-02,  2.0471e-02, -7.8265e-02,  4.6643e-02, -3.8060e-02,\n",
      "        -2.6986e-02,  1.3716e-01,  3.6160e-02,  7.0031e-02, -4.0956e-02,\n",
      "         5.4570e-02,  1.3921e-02, -4.0513e-02,  3.1521e-03,  1.4136e-02,\n",
      "        -9.8081e-03,  1.8325e-02,  1.1218e-02, -3.7796e-02,  5.3676e-02,\n",
      "         1.4420e-03,  1.9236e-02,  2.2062e-03, -8.2946e-03,  2.8526e-02,\n",
      "         1.2249e-01, -8.0852e-04,  5.5085e-02,  3.4875e-02, -4.7192e-02,\n",
      "         2.7700e-02, -1.9714e-02,  1.3763e-02, -1.8486e-02,  7.2124e-02,\n",
      "        -6.5986e-02, -1.9615e-02,  4.8595e-02,  9.2978e-02, -6.2818e-03,\n",
      "         2.1697e-02, -2.5158e-02,  8.7742e-02, -3.6655e-02,  1.9939e-02,\n",
      "        -4.1390e-02, -4.8784e-02, -4.2068e-03, -4.8684e-02,  1.6396e-02,\n",
      "        -5.1018e-02,  5.2149e-03,  1.0889e-02, -4.0306e-02, -1.3253e-02,\n",
      "         6.1736e-02, -2.4648e-02,  4.2897e-02,  2.7846e-02, -1.0851e-02,\n",
      "         1.8117e-02,  6.9813e-02, -1.7116e-02,  3.6035e-02, -2.5052e-02,\n",
      "         4.8606e-02,  5.9169e-03, -9.3281e-03, -2.2112e-02,  1.6149e-02,\n",
      "         3.8595e-02,  3.5898e-02,  1.8676e-02, -1.3459e-02,  8.3886e-03,\n",
      "        -1.6567e-02, -1.6359e-02,  8.9632e-02, -5.7823e-02, -1.4178e-02,\n",
      "         3.1554e-02,  2.5928e-02, -2.3020e-02, -6.0737e-02, -2.8929e-02,\n",
      "         9.2361e-02,  6.5355e-03, -1.0153e-02, -3.5418e-02, -1.9189e-02,\n",
      "        -1.5710e-02, -9.6074e-02,  6.6300e-02,  6.9613e-02, -1.1271e-02,\n",
      "         2.0203e-02, -1.5778e-02,  4.6024e-02, -2.7964e-02, -4.6986e-02,\n",
      "         7.1457e-02, -1.0101e-02,  2.5827e-02, -7.7026e-02, -4.2813e-02,\n",
      "        -2.9404e-02, -2.2875e-02,  1.9008e-03,  5.6426e-02, -3.7334e-02,\n",
      "        -2.7280e-02,  1.9159e-03, -3.9086e-02, -5.8507e-02, -2.2496e-02,\n",
      "        -4.2482e-02,  1.7298e-02, -6.3443e-02,  1.1777e-01, -2.8626e-02,\n",
      "        -6.9393e-02, -4.3418e-02, -8.6873e-02, -1.2727e-02,  5.4160e-02,\n",
      "         1.6747e-02, -4.9655e-02, -1.1934e-02,  7.7618e-02,  1.8377e-02,\n",
      "         3.1530e-02,  1.0281e-02, -7.8996e-02,  4.9174e-02,  5.6885e-02,\n",
      "         6.1590e-02,  8.8819e-02, -7.3861e-02,  5.1125e-02,  1.7978e-02,\n",
      "         5.7972e-02, -5.4561e-02, -5.2427e-02, -5.2197e-02, -5.3636e-02,\n",
      "         6.0865e-02,  1.1614e-03,  3.2500e-02,  3.3319e-02, -7.6267e-03,\n",
      "        -1.2455e-02, -1.8819e-03, -5.1676e-02, -7.8813e-02, -2.0145e-02,\n",
      "         5.0323e-02, -9.5349e-03, -4.0278e-02,  4.3510e-02,  7.0093e-02,\n",
      "         4.9152e-02,  3.8270e-02, -7.3292e-02, -6.5243e-02, -1.4536e-02,\n",
      "         6.5791e-02, -9.2452e-02,  7.0274e-03,  4.6297e-02, -2.4731e-02,\n",
      "        -2.3027e-02, -6.6786e-02,  8.0024e-02,  3.2907e-02,  5.7817e-02,\n",
      "         1.8456e-02,  3.4911e-03, -8.0788e-02,  4.1844e-03, -1.9459e-02,\n",
      "        -1.0642e-01, -5.8142e-02, -8.5754e-02,  9.4288e-02, -9.8949e-02,\n",
      "         9.7250e-02,  2.9575e-02,  1.0607e-01, -6.5372e-02, -4.5835e-02,\n",
      "         1.7901e-02, -2.4000e-02, -8.6072e-03, -3.7330e-03,  1.1372e-02,\n",
      "         3.1199e-02, -6.1926e-02, -1.5075e-01, -7.1224e-03,  2.3883e-02,\n",
      "         4.8057e-02, -1.2189e-02, -2.9336e-02,  1.1623e-01,  1.8023e-02,\n",
      "        -2.0701e-02,  3.3783e-02, -1.1620e-02,  1.1505e-01, -2.9303e-02,\n",
      "        -8.6753e-02,  3.4042e-02, -8.7430e-02, -5.1039e-02, -2.1567e-02,\n",
      "         7.1555e-02, -3.3511e-02,  2.0945e-03,  4.9348e-02, -4.3360e-03,\n",
      "         8.4904e-02, -7.0846e-02,  1.0677e-02, -2.7494e-02,  4.9307e-02,\n",
      "         6.5279e-02,  8.1138e-02, -4.0727e-02,  4.2335e-04, -6.6851e-02,\n",
      "        -3.0792e-02, -9.0393e-03, -2.9904e-02,  8.1190e-02,  6.0275e-02,\n",
      "         7.7381e-02, -3.9519e-02, -2.7160e-02, -2.4761e-02,  2.5359e-02,\n",
      "         6.6163e-02,  3.9978e-02,  1.9958e-03,  7.8120e-02, -1.1751e-02,\n",
      "        -7.1103e-02,  2.2857e-02,  1.1480e-02, -2.6101e-02, -2.4529e-02,\n",
      "        -5.0576e-02, -6.1287e-02,  7.7143e-02, -1.7400e-02,  6.6282e-02,\n",
      "        -3.5769e-02, -7.2665e-02, -1.1773e-02,  1.8786e-02,  7.1099e-02,\n",
      "        -1.0020e-01, -4.8797e-02,  4.9520e-02, -1.3163e-03, -3.2685e-02,\n",
      "        -6.5374e-02,  2.2430e-02, -4.2483e-02,  1.0546e-02,  3.1356e-02,\n",
      "        -8.5592e-03,  6.8557e-02,  3.5387e-04, -1.1750e-02, -3.5609e-02,\n",
      "        -4.1601e-02, -1.5604e-01, -8.6944e-02,  2.1241e-02, -5.5225e-02,\n",
      "         7.7322e-04, -2.2027e-02, -7.2078e-02, -2.5196e-02, -3.8174e-03,\n",
      "         9.8103e-03, -1.0619e-01,  8.8379e-02,  8.0232e-02,  3.9970e-02,\n",
      "        -3.5587e-02, -5.7518e-02, -5.7391e-02, -3.0383e-02,  3.6210e-02,\n",
      "         5.4744e-02,  6.3418e-02, -2.8104e-03,  1.6384e-02,  1.2159e-02,\n",
      "         3.2110e-02,  1.7024e-02, -3.2751e-02, -4.7196e-02, -5.0462e-03,\n",
      "         1.5706e-02, -5.2447e-02, -6.4596e-02, -1.8440e-02, -7.4205e-02,\n",
      "         8.2061e-02,  1.4927e-01, -8.7641e-03,  4.0346e-02,  4.7602e-02,\n",
      "         1.4393e-02,  5.7976e-02,  2.6013e-02, -1.5071e-02, -1.1766e-03,\n",
      "        -1.7070e-02, -4.8071e-03, -7.7303e-02, -4.8671e-02, -3.7900e-02,\n",
      "         1.3451e-02,  6.7563e-02, -8.6737e-02,  2.5434e-02, -5.7086e-02,\n",
      "        -1.0197e-02,  2.5360e-02, -4.8181e-02, -4.0133e-02,  3.7719e-02,\n",
      "         4.2672e-02,  9.7170e-02, -1.2633e-02, -2.2981e-02,  2.7850e-02,\n",
      "        -3.8084e-03,  7.8059e-02,  3.1446e-02,  1.7881e-03,  1.4715e-02,\n",
      "         3.0482e-02,  1.2654e-02, -6.5878e-03,  1.0231e-02, -8.9710e-03,\n",
      "         4.2751e-02, -7.4830e-02,  8.1804e-02,  3.1473e-02, -3.6812e-02,\n",
      "        -6.1236e-02,  5.0350e-02, -3.3032e-02,  1.5674e-02, -1.7014e-01,\n",
      "        -6.1056e-02, -4.8309e-02, -1.7908e-02, -3.7888e-02, -2.5940e-02,\n",
      "        -2.0897e-02, -5.4669e-02, -9.8456e-02, -6.8027e-02, -5.2727e-02,\n",
      "         5.6497e-02,  3.3852e-02, -8.5017e-03,  1.8251e-02, -1.1085e-01,\n",
      "        -4.6817e-03,  5.8629e-03,  1.1756e-02, -3.5803e-02,  9.6346e-03,\n",
      "         1.9681e-02,  2.7234e-02,  3.9650e-02, -6.3268e-02, -3.3534e-02,\n",
      "        -5.4075e-02,  1.1318e-01, -2.3310e-02,  6.3788e-02,  6.4055e-02,\n",
      "        -1.0671e-02,  2.3278e-02, -2.5212e-02, -2.2767e-02, -6.0865e-02,\n",
      "         3.5426e-02,  5.8038e-02, -4.3588e-02,  6.2876e-02,  9.2234e-02,\n",
      "         5.0699e-02, -9.1728e-02,  3.0323e-02,  2.7542e-02,  2.7721e-02,\n",
      "        -7.0999e-02, -8.0821e-02, -2.0131e-02, -2.3877e-02, -1.7808e-02,\n",
      "         4.0562e-02,  8.0953e-02, -1.0209e-01, -5.9207e-03,  1.7682e-02,\n",
      "        -6.8793e-03,  8.2325e-02, -7.4820e-03, -1.6822e-02,  9.8217e-02,\n",
      "         7.6004e-03,  2.6860e-03, -9.7709e-02,  3.5367e-02,  3.0687e-04,\n",
      "         2.0246e-03, -5.1803e-03, -2.3907e-02,  7.4411e-02,  3.8976e-02,\n",
      "         3.8080e-02,  8.5966e-03, -7.7932e-02, -4.2097e-02, -1.2985e-02,\n",
      "         5.9445e-02, -6.5480e-02,  7.4644e-03, -3.2551e-02,  8.6069e-02,\n",
      "        -7.5778e-02, -6.3627e-02,  3.2478e-02,  1.5107e-02, -8.5916e-03,\n",
      "        -4.3627e-02, -5.2811e-02, -1.4477e-02,  3.8516e-02,  2.1088e-02,\n",
      "        -4.3141e-02,  8.7329e-02,  4.0818e-02, -3.1347e-04, -4.5963e-02,\n",
      "         9.6282e-02,  5.3585e-03,  5.7506e-03, -1.4908e-01, -8.6681e-02,\n",
      "         3.9047e-02, -8.1297e-02, -4.8234e-02, -6.0777e-02,  5.8577e-03,\n",
      "        -7.4572e-03, -1.4822e-02,  2.2711e-02,  9.1142e-03, -2.3120e-02,\n",
      "         4.7291e-02, -3.8868e-02,  3.3197e-02, -1.1024e-02,  4.9090e-02,\n",
      "        -9.8191e-03, -7.5624e-03, -2.0821e-02, -1.2739e-02,  2.7601e-02,\n",
      "        -1.9421e-02, -2.0415e-02, -1.3596e-02,  6.9858e-02, -6.0276e-02,\n",
      "        -7.6523e-02, -1.1978e-01,  5.5650e-02, -5.2445e-02, -7.6851e-02,\n",
      "         5.4759e-02,  7.0314e-02, -1.9711e-02, -1.9588e-03, -2.3852e-03,\n",
      "        -1.1652e-02, -1.8124e-02, -2.4643e-02,  5.4581e-03, -2.1402e-04,\n",
      "        -9.3967e-03, -5.3192e-02, -2.5956e-02,  4.5336e-02, -6.2329e-02,\n",
      "        -1.1640e-01, -3.0452e-02,  8.9828e-03, -2.8135e-02, -1.3705e-02,\n",
      "        -6.3537e-02,  3.0768e-02,  9.9245e-02,  1.1340e-02, -3.2176e-02,\n",
      "         5.0348e-02,  9.8320e-02,  2.4204e-02, -5.2936e-02, -2.9736e-03,\n",
      "        -3.8028e-02,  6.0298e-02, -1.8153e-02,  3.3803e-02, -1.2208e-01,\n",
      "        -7.7807e-03,  1.7209e-02, -1.8118e-02,  7.0817e-02, -1.3744e-03,\n",
      "        -4.7583e-03, -1.8209e-02,  2.1317e-02, -1.3205e-02,  2.2087e-02,\n",
      "         9.5411e-03,  2.6980e-02,  1.0582e-01,  6.6936e-02,  8.6582e-03,\n",
      "        -4.2450e-02, -5.6574e-02, -1.7277e-02,  3.6158e-02, -1.0705e-02,\n",
      "        -2.7000e-02,  3.4371e-03,  7.5160e-03,  5.3734e-02, -6.1263e-02,\n",
      "         4.2127e-02, -2.9979e-02, -3.2129e-02, -9.0064e-02, -2.6494e-02,\n",
      "        -1.1182e-01, -3.1729e-02,  1.1660e-02,  1.9204e-02, -3.0125e-02,\n",
      "        -4.9238e-03,  5.0859e-02, -4.4106e-02,  5.1285e-02,  4.5689e-02,\n",
      "        -4.6262e-02,  1.2369e-02,  6.6566e-02, -8.2128e-02,  3.2274e-03,\n",
      "        -5.3324e-02, -2.6309e-02,  6.4387e-02,  2.9044e-02, -3.5064e-02,\n",
      "         8.0521e-03, -2.3497e-02,  8.4132e-02, -4.4854e-02, -2.3250e-02,\n",
      "         8.1650e-03, -1.6931e-02, -1.3467e-02,  5.4474e-02,  8.9946e-02,\n",
      "         3.1940e-02,  3.8613e-02, -1.3011e-02,  7.0965e-02, -5.0153e-02,\n",
      "         6.2104e-02, -1.7794e-02,  2.9889e-03,  2.8309e-02, -4.5754e-02,\n",
      "        -8.8405e-03, -3.6188e-02, -9.8971e-03, -9.8113e-02, -3.4738e-02,\n",
      "         1.5838e-02,  5.7312e-03, -4.7265e-02,  4.8754e-02,  2.4589e-02,\n",
      "        -2.8289e-02, -8.9685e-02, -2.2136e-02, -5.5934e-02,  5.6831e-02,\n",
      "        -3.6466e-02, -7.0986e-02,  8.1882e-03, -1.1050e-02, -4.2237e-03,\n",
      "         2.6455e-02, -3.1131e-02,  1.3809e-02,  9.8180e-02, -6.7561e-02,\n",
      "        -6.9613e-03,  4.7655e-02,  8.5269e-02, -3.9637e-03, -7.6830e-02,\n",
      "         6.3732e-02, -3.5884e-02,  1.3926e-02,  4.8347e-03,  8.3719e-02,\n",
      "         7.5872e-02,  4.7322e-02,  3.5692e-03, -3.2358e-02,  5.1481e-02,\n",
      "        -3.7712e-02, -2.0537e-03,  3.9704e-04, -6.9504e-02, -2.8840e-02,\n",
      "         3.1747e-02, -6.9234e-02,  2.0547e-02, -6.5831e-03,  3.0676e-02,\n",
      "         2.1569e-02,  4.2823e-02, -1.7668e-02,  7.6772e-02,  1.3544e-03,\n",
      "         2.7585e-03,  2.1489e-02,  9.6274e-03, -1.5723e-05,  3.6773e-02,\n",
      "         1.4596e-02,  3.9308e-02, -9.3329e-02,  2.2307e-02,  1.9144e-02,\n",
      "        -3.9534e-02,  3.6225e-02, -6.2374e-03,  4.8350e-02, -6.0506e-02,\n",
      "        -1.6955e-02, -3.5904e-02, -2.3223e-02,  9.8909e-03,  8.1483e-02,\n",
      "         2.8930e-02,  7.7604e-02, -7.2170e-02,  7.8285e-02,  7.9986e-02,\n",
      "         2.3214e-02,  1.9854e-02,  3.9743e-02,  7.0248e-03,  6.4697e-02,\n",
      "         9.0826e-02,  3.9726e-02,  6.5878e-02,  3.3821e-02,  1.1178e-02,\n",
      "         4.0639e-02,  8.2774e-02, -9.4847e-02,  1.9955e-03,  8.0912e-02,\n",
      "        -3.7274e-02,  4.4650e-02,  1.3312e-02, -1.1398e-02, -8.4911e-03,\n",
      "        -5.3292e-02,  3.9194e-02,  2.9705e-02,  5.2591e-02,  2.3951e-02,\n",
      "        -2.0175e-02, -1.7757e-02, -9.3108e-02, -1.8622e-02, -8.6373e-02,\n",
      "        -2.4188e-02, -5.6345e-02, -1.2761e-02, -1.0851e-01, -7.5382e-02,\n",
      "         3.1815e-02,  5.7262e-03, -5.3658e-02,  4.0368e-02, -4.0356e-02,\n",
      "         1.9611e-02, -2.7564e-04, -9.4917e-02, -1.7916e-03,  9.0847e-02,\n",
      "        -7.4714e-03,  8.1739e-02, -9.9614e-03,  1.2541e-02, -7.0002e-02,\n",
      "         6.5005e-02, -9.3902e-04,  3.7608e-02, -4.4476e-02, -1.7352e-02,\n",
      "        -7.9024e-02, -1.4668e-01, -3.9281e-02, -2.6128e-03,  1.2776e-02,\n",
      "         4.0658e-02,  7.7062e-02,  3.8716e-02, -6.1752e-02, -5.2781e-02,\n",
      "         1.3940e-02, -1.2595e-02, -4.9038e-02,  1.8949e-02, -3.0886e-02,\n",
      "        -1.1009e-02,  1.1939e-01,  5.0946e-02,  8.8414e-03,  1.1823e-02,\n",
      "         2.0166e-02,  6.5746e-02, -3.3068e-02, -1.6340e-02,  1.1260e-02,\n",
      "         3.1376e-02,  5.9389e-03, -8.5261e-02, -6.7225e-02, -9.1181e-02,\n",
      "         1.1286e-02,  1.2424e-02, -5.0941e-03, -9.0255e-02, -2.8315e-02,\n",
      "         6.2498e-02, -9.0387e-02,  9.8351e-03,  3.9148e-02,  1.0207e-03,\n",
      "        -4.3292e-02, -2.5541e-02,  5.9964e-02, -8.3212e-02,  4.6373e-02,\n",
      "        -8.5085e-02,  5.6198e-02, -7.4791e-03, -2.5594e-02,  5.4570e-02,\n",
      "         1.4782e-02,  2.6835e-02, -1.1320e-01, -5.0685e-02,  6.9759e-02,\n",
      "         3.1934e-02,  3.8178e-02, -5.4517e-02,  9.2114e-03,  9.4188e-03,\n",
      "         1.8160e-02,  2.3291e-02,  5.2194e-02,  7.2041e-03,  2.3979e-03,\n",
      "        -4.3724e-02,  1.0393e-02,  9.0367e-03,  7.8837e-02, -1.2560e-02,\n",
      "        -5.4081e-02, -1.1210e-02,  3.5160e-02,  9.5335e-02,  1.3039e-02,\n",
      "        -3.8194e-02,  1.5503e-02, -7.2973e-02,  8.9994e-02,  6.0482e-02,\n",
      "         8.7802e-03,  5.1153e-02,  5.7845e-02,  4.4121e-02,  2.9331e-02,\n",
      "        -1.6722e-03,  1.3924e-02, -7.5937e-02,  6.8686e-02,  5.3860e-02,\n",
      "         1.2857e-01, -2.9583e-02,  7.3459e-02,  6.0980e-02, -7.1148e-02,\n",
      "         4.9157e-02,  7.4101e-02, -8.3088e-02, -2.7627e-02, -2.8949e-02,\n",
      "        -2.7268e-02,  7.8420e-02,  8.4615e-02, -4.3208e-03, -8.5176e-02,\n",
      "         8.2494e-02, -3.6547e-02,  4.8354e-03, -2.0249e-02,  4.2887e-02],\n",
      "       grad_fn=<ViewBackward>)]\n",
      "target tensor([-1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.])\n",
      "correlated model(v) (tensor([[[-0.0502, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502,\n",
      "          -0.0502, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[-0.0321]]], grad_fn=<TanhBackward>))\n",
      "uncorrelated model(v) (tensor([[[ 0.1607,  0.1607, -0.0502,  0.1607,  0.1607, -0.0502, -0.0502,\n",
      "           0.1607, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[0.0084]]], grad_fn=<TanhBackward>))\n",
      "data tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1.,  1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1.,  1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1.,  1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1.,  1.,  1., -1., -1., -1.,  1., -1., -1.],\n",
      "          [-1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [-1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1., -1., -1., -1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1., -1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1., -1.,  1., -1., -1.],\n",
      "          [ 1., -1.,  1., -1., -1., -1.,  1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.],\n",
      "          [ 1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.],\n",
      "          [ 1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "          [ 1., -1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.],\n",
      "          [ 1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.],\n",
      "          [ 1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1., -1., -1., -1.,  1., -1.,  1.],\n",
      "          [-1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1., -1.,  1., -1., -1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "          [ 1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1., -1., -1., -1., -1., -1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.]]]])\n",
      "output [tensor([-0.0502, -0.0502, -0.1364,  ..., -0.0721,  0.1338,  0.1607],\n",
      "       grad_fn=<ViewBackward>), tensor([-4.3109e-02, -4.6726e-02,  1.7989e-02, -1.1996e-01,  4.1090e-02,\n",
      "        -2.9716e-02,  3.6880e-02,  8.7336e-02,  5.8638e-03, -6.0333e-02,\n",
      "        -7.6900e-03,  8.0576e-03, -7.0616e-02, -2.4435e-02, -1.3751e-02,\n",
      "         3.8876e-02,  4.4166e-02, -3.5158e-02,  5.4488e-03,  3.6177e-02,\n",
      "        -1.8059e-02,  1.6234e-02, -7.5243e-02, -1.2882e-01, -4.5830e-02,\n",
      "        -4.8790e-02, -2.2898e-02, -9.8640e-03, -2.3521e-02, -3.2498e-02,\n",
      "        -3.7231e-04,  1.1808e-02, -3.2568e-02,  3.5800e-02,  3.1989e-02,\n",
      "        -5.0709e-02, -2.1672e-02, -2.5107e-02,  8.6387e-02,  7.3679e-02,\n",
      "         8.6842e-02,  7.1057e-02, -1.3956e-01,  4.3512e-02,  2.3602e-02,\n",
      "         4.2035e-02,  6.0107e-02,  1.6336e-02,  8.6939e-03, -3.5103e-03,\n",
      "        -2.8216e-02,  2.8201e-02,  2.9712e-02,  4.5840e-02, -1.9366e-01,\n",
      "        -1.6921e-03, -1.0654e-01, -1.6546e-02,  1.1803e-02, -5.3484e-02,\n",
      "         3.9425e-02, -4.3074e-02,  4.8707e-02,  4.1297e-02,  3.0294e-02,\n",
      "        -3.4716e-02,  3.6347e-02, -2.9068e-02,  1.6791e-02,  1.1421e-01,\n",
      "         1.5643e-03, -4.4582e-02,  6.1958e-02,  9.3023e-02,  1.9863e-02,\n",
      "        -4.7444e-02,  6.6007e-03, -2.4637e-03, -7.5507e-02, -3.5570e-02,\n",
      "         2.8452e-02,  7.9120e-02, -9.5284e-02,  6.7020e-02, -6.9911e-02,\n",
      "         1.0873e-01,  8.4287e-02, -2.0783e-02,  4.7757e-02,  1.9530e-02,\n",
      "         6.1842e-02, -5.8553e-04, -1.3085e-01,  9.8209e-02,  2.8683e-02,\n",
      "         7.5464e-02, -1.6688e-02, -1.3747e-02,  4.9503e-02, -1.1336e-02,\n",
      "        -3.1769e-02, -5.9767e-04, -1.8992e-02, -1.3789e-01, -4.5945e-02,\n",
      "        -8.4986e-02, -4.9278e-03, -4.6858e-03,  8.2542e-02,  3.0066e-02,\n",
      "        -2.3393e-02,  8.7602e-03, -1.1025e-02,  9.6375e-03,  2.5488e-02,\n",
      "        -4.1136e-02, -4.1668e-02,  2.9048e-02,  1.4883e-02, -6.2806e-02,\n",
      "        -1.8918e-03, -5.2359e-02, -6.6113e-02, -6.3780e-02, -3.1011e-02,\n",
      "        -2.0211e-02, -5.6570e-02,  7.1642e-02, -1.0891e-01, -3.8958e-02,\n",
      "        -3.8039e-02, -1.1690e-02, -9.5756e-02,  2.9486e-02, -7.6916e-04,\n",
      "        -7.0774e-02,  3.3360e-02, -8.4898e-02,  2.7978e-02, -2.9417e-02,\n",
      "        -8.7795e-02, -8.6680e-02, -9.4991e-02, -7.7228e-03, -1.0749e-01,\n",
      "        -1.9089e-02, -2.3053e-02, -3.8115e-03, -2.1837e-02, -2.7764e-02,\n",
      "         2.2729e-03,  4.0745e-02,  6.8972e-02, -4.6489e-02,  6.1117e-02,\n",
      "        -3.8262e-02, -2.0870e-02, -5.1565e-02,  6.0433e-02,  7.5952e-02,\n",
      "        -3.9887e-03, -3.0936e-02,  1.6149e-02,  1.8283e-02, -3.0645e-02,\n",
      "         4.7860e-02, -8.4303e-02,  1.4000e-02, -2.3857e-02,  6.0691e-03,\n",
      "         2.5391e-02,  2.7667e-02,  1.5615e-02, -1.7698e-02, -3.8140e-02,\n",
      "        -4.0648e-02,  4.9582e-02, -9.1526e-02,  3.4240e-02, -1.0184e-02,\n",
      "         2.2971e-02, -1.9315e-02, -8.4234e-02,  3.4077e-02,  1.6866e-02,\n",
      "        -1.4785e-02,  1.6845e-02,  2.9443e-02,  9.0285e-02,  9.0944e-03,\n",
      "        -4.9694e-03, -6.9527e-02, -2.0869e-02,  2.8989e-02,  4.2720e-02,\n",
      "         2.4434e-02, -4.0280e-02,  3.5459e-02,  2.4877e-02, -2.4644e-02,\n",
      "        -2.5424e-02, -1.9109e-02,  2.1821e-02,  4.9558e-02,  1.7838e-02,\n",
      "         4.2955e-02, -1.7489e-02, -2.4215e-02, -2.7330e-02,  2.2070e-02,\n",
      "        -1.0760e-02, -6.9996e-02,  3.6028e-02,  5.4307e-03, -1.4500e-02,\n",
      "        -4.7358e-02, -6.0230e-02, -6.0929e-02, -1.4350e-02,  6.8692e-02,\n",
      "         7.0514e-02,  6.8671e-02,  3.2541e-02, -4.6486e-02, -4.2454e-02,\n",
      "         6.1266e-02, -1.4297e-01,  6.4328e-02, -8.0956e-02, -1.2395e-02,\n",
      "         2.0306e-02,  4.5145e-02,  7.3146e-02,  8.6402e-03,  5.7367e-02,\n",
      "        -3.7021e-02,  2.9437e-02, -4.1982e-02,  4.7846e-02, -7.6203e-02,\n",
      "        -1.0919e-01,  8.2677e-02,  7.0771e-02, -4.7849e-02, -3.6108e-02,\n",
      "         6.1985e-02, -2.2956e-02, -6.0650e-03, -1.9198e-02,  3.8670e-03,\n",
      "        -6.4991e-02, -8.6205e-03, -1.4406e-02, -6.1381e-02, -7.8200e-02,\n",
      "        -3.8653e-02, -2.9558e-02,  3.3400e-02,  8.9191e-03, -3.2339e-02,\n",
      "        -7.1439e-02,  7.6657e-02,  2.6756e-02, -4.4346e-02, -1.1484e-02,\n",
      "         2.3555e-02, -7.7944e-02, -3.5100e-02,  1.1596e-03,  2.7315e-03,\n",
      "         2.5286e-02, -8.0894e-02,  1.2344e-02, -5.3875e-02,  5.7643e-02,\n",
      "         7.1390e-02,  4.8052e-02,  1.8060e-02, -8.3233e-02,  2.1820e-02,\n",
      "         4.5791e-02, -2.5045e-02, -8.6356e-02,  4.9968e-03,  2.7389e-02,\n",
      "        -2.7641e-02, -1.9539e-02, -1.5793e-02, -4.7890e-02, -4.1217e-03,\n",
      "         2.2472e-02,  4.4510e-03, -5.7882e-02,  3.5659e-02, -6.4439e-02,\n",
      "         6.2937e-02,  4.4439e-03,  5.5726e-04,  2.4571e-02, -4.0826e-04,\n",
      "        -9.5583e-03, -7.3198e-02,  9.2559e-03, -9.6453e-03, -2.4422e-02,\n",
      "         1.1436e-01,  2.1529e-02, -1.1577e-02, -2.3054e-02,  8.2483e-02,\n",
      "        -6.2005e-02,  1.2553e-02,  1.1008e-01,  1.0746e-01, -4.0265e-03,\n",
      "         1.4057e-03, -5.5434e-03,  6.2337e-02,  7.0217e-02,  9.8403e-03,\n",
      "         2.1866e-02,  1.0089e-01,  3.8677e-02, -4.4296e-02, -5.7767e-02,\n",
      "         3.9887e-02, -2.9895e-02,  6.2111e-03,  1.5925e-02, -5.1878e-03,\n",
      "         6.1146e-02,  1.9988e-02,  3.5669e-03, -1.5690e-02,  3.3328e-02,\n",
      "         2.9759e-02, -1.2286e-01,  4.5656e-03, -7.4594e-02, -8.9107e-02,\n",
      "         5.1912e-02, -4.4238e-02,  3.4795e-03,  3.4961e-02,  6.3548e-02,\n",
      "        -3.6204e-02, -6.0675e-02,  3.0988e-02,  1.0276e-02,  2.8245e-02,\n",
      "        -1.6822e-02,  1.4110e-02, -8.1474e-02, -9.6312e-03, -9.2212e-02,\n",
      "        -4.3537e-04, -5.4090e-02,  4.4397e-03,  7.5255e-02,  1.6380e-01,\n",
      "        -2.4837e-02, -4.1520e-02, -1.0020e-01,  4.0048e-02,  7.6439e-02,\n",
      "        -3.9594e-02,  3.6334e-02,  3.0066e-02, -6.4418e-02, -4.3562e-02,\n",
      "        -1.3969e-01, -9.5826e-03,  5.9560e-02,  1.2190e-02, -6.9798e-03,\n",
      "         3.3599e-02, -3.5503e-02,  6.1294e-02, -5.9170e-02,  1.5073e-02,\n",
      "        -3.3531e-02, -1.2343e-02, -2.0183e-02,  4.6101e-02,  1.7705e-02,\n",
      "        -1.1246e-01,  5.0912e-04,  1.4157e-01,  5.9243e-03, -4.4335e-02,\n",
      "         2.2773e-02,  3.8597e-02,  2.3283e-02, -3.2187e-02,  4.2094e-02,\n",
      "         1.0769e-01,  3.2593e-02, -4.8735e-02,  2.0374e-02, -9.6113e-04,\n",
      "         5.3493e-02, -4.1311e-02,  1.0916e-02, -3.6528e-02,  8.0127e-02,\n",
      "         3.4181e-02,  9.0580e-02,  2.7307e-02,  5.6907e-02,  7.8440e-02,\n",
      "         2.1157e-02, -5.8625e-02, -3.5348e-02,  7.5423e-02,  2.8446e-02,\n",
      "        -4.3710e-02,  1.3984e-02, -9.1724e-03, -4.9641e-02,  7.9352e-02,\n",
      "         4.2937e-02,  4.8391e-02, -7.6696e-03, -1.1088e-02, -2.1821e-02,\n",
      "         1.6615e-02,  1.0055e-01, -2.0965e-02, -4.5630e-02,  9.7000e-03,\n",
      "        -3.1211e-03,  3.6930e-02, -1.9369e-02, -4.1837e-02,  5.9708e-02,\n",
      "        -4.8294e-02,  2.4752e-02, -3.5792e-02, -2.1377e-02, -1.3379e-01,\n",
      "         9.3071e-03, -1.5979e-02, -1.6052e-02,  2.4510e-02, -6.3334e-02,\n",
      "        -4.9556e-02, -4.2315e-02,  1.1531e-01, -2.5761e-02,  7.5453e-02,\n",
      "        -4.0641e-02,  1.1895e-02,  2.1086e-02,  4.3297e-02, -2.1699e-02,\n",
      "        -1.0030e-02, -1.6277e-02, -7.2929e-03,  2.7936e-02,  1.1146e-01,\n",
      "        -6.7219e-02, -5.5953e-02, -1.9102e-01,  2.4286e-02,  1.0710e-02,\n",
      "         5.8207e-02,  9.1452e-02, -6.3720e-02,  5.5484e-02, -1.8705e-02,\n",
      "         5.7503e-02,  3.5912e-02, -6.3960e-02, -2.4281e-02,  1.9026e-03,\n",
      "        -8.2456e-02, -2.2775e-02,  2.7646e-02, -5.8677e-02,  4.1334e-02,\n",
      "        -4.1548e-02, -1.9321e-02, -2.8181e-02,  5.6177e-02,  3.6484e-02,\n",
      "        -4.4459e-02, -3.8078e-02,  5.3251e-02,  2.2609e-02,  9.4320e-02,\n",
      "         5.9017e-02, -1.2148e-01, -8.3608e-02,  1.4869e-02,  6.1242e-03,\n",
      "         2.7417e-02, -5.4028e-02, -6.3744e-02, -9.3660e-02,  6.7032e-03,\n",
      "         2.8413e-02, -1.0794e-01, -7.4234e-02, -6.0407e-02, -4.6053e-02,\n",
      "        -1.1798e-02, -7.1014e-02,  6.1678e-03, -1.1274e-01,  2.1047e-02,\n",
      "         5.1184e-02, -4.4059e-02, -1.8884e-03,  6.8878e-02, -8.8390e-02,\n",
      "        -7.3545e-03,  1.0697e-01, -4.0062e-02,  2.9532e-03,  1.7475e-03,\n",
      "        -3.7353e-03,  1.0844e-02,  7.6999e-02, -6.1946e-02,  7.7708e-02,\n",
      "         3.3028e-02, -2.8698e-02, -3.4846e-02, -6.9730e-02, -3.2713e-02,\n",
      "         2.2600e-02, -7.8812e-02, -4.7929e-02, -2.1218e-02, -2.4503e-02,\n",
      "         5.2681e-02, -7.2624e-02, -8.9749e-02, -3.7424e-03,  1.3156e-02,\n",
      "         4.0345e-02, -6.1767e-02, -1.2612e-01, -2.3192e-02, -1.3606e-02,\n",
      "        -3.3307e-02,  1.9728e-02,  8.8257e-02, -3.2784e-02,  2.4404e-02,\n",
      "         4.5003e-02, -3.9114e-02, -1.0686e-02,  4.9304e-02,  5.3773e-02,\n",
      "        -2.3875e-02, -3.5602e-02,  4.3045e-03,  8.4584e-03, -2.9986e-02,\n",
      "         5.0386e-02,  2.1143e-02,  7.3225e-03, -3.6067e-02, -7.6484e-02,\n",
      "        -3.1488e-02, -1.5340e-03, -9.1480e-03, -3.4490e-02,  8.8097e-02,\n",
      "        -5.4905e-02, -1.5144e-02, -3.8944e-02,  4.0994e-02,  1.2037e-01,\n",
      "         3.1272e-02, -8.0880e-02, -8.5412e-02,  4.7222e-02, -4.6666e-02,\n",
      "         9.5875e-03, -3.5189e-02,  5.6126e-03,  2.0831e-02, -3.3459e-02,\n",
      "         4.8299e-02,  1.5566e-02, -2.8489e-02,  9.1406e-02, -1.0523e-02,\n",
      "        -1.5222e-03, -4.6430e-02,  3.2918e-02,  2.2706e-02,  4.6368e-03,\n",
      "         2.7659e-02,  2.0227e-02,  4.9097e-02,  5.1583e-03,  2.4138e-02,\n",
      "         9.8080e-02,  2.4603e-02,  5.5490e-02, -3.0268e-02, -1.2086e-02,\n",
      "        -1.5660e-03, -2.6131e-02,  3.0830e-02, -2.2046e-03,  6.2495e-02,\n",
      "        -5.8882e-02,  5.5600e-02, -4.4436e-02,  2.8031e-02,  1.9347e-02,\n",
      "        -8.4160e-02,  4.1963e-02,  2.6820e-04, -5.6034e-02, -5.3867e-03,\n",
      "        -1.4880e-04, -1.7159e-02,  1.9786e-02,  1.5490e-02, -5.4845e-02,\n",
      "        -4.8836e-02, -5.0230e-02,  4.8601e-02, -1.5873e-02,  4.9607e-03,\n",
      "        -8.6253e-03,  5.5473e-02,  5.1469e-02, -3.8513e-02, -3.5624e-02,\n",
      "        -1.3370e-01,  1.9989e-03,  1.4307e-02, -8.6326e-02, -3.1933e-02,\n",
      "        -5.8446e-02,  1.8513e-03, -1.7497e-02, -7.8665e-02, -1.9582e-03,\n",
      "        -6.5552e-02, -2.4602e-02, -4.6682e-02, -6.4047e-02, -1.6765e-02,\n",
      "         6.8977e-03, -6.0028e-02,  9.2866e-02, -6.0716e-02,  2.2528e-02,\n",
      "         9.3773e-02,  4.8104e-02,  5.3905e-02,  7.1853e-02,  8.9737e-03,\n",
      "        -5.5326e-02,  2.9541e-02,  8.2003e-03, -3.1143e-02, -7.2407e-03,\n",
      "         4.8483e-02, -2.1149e-02, -2.8613e-02,  3.3473e-02, -3.5173e-03,\n",
      "        -5.7896e-02, -5.2089e-02, -4.9224e-02,  2.6640e-02, -8.4778e-03,\n",
      "        -5.1619e-02, -2.5282e-02,  1.8951e-04,  4.1976e-02, -1.2154e-02,\n",
      "         5.1504e-02, -5.3737e-04, -5.8339e-02,  3.9274e-02, -3.7866e-02,\n",
      "        -7.6738e-04, -1.5474e-02, -9.7350e-02,  1.6608e-02, -1.9588e-02,\n",
      "         7.2844e-02, -1.4502e-02, -2.3074e-02, -3.2286e-02,  6.0114e-02,\n",
      "         2.7838e-02,  5.3926e-02, -4.5507e-02,  9.8745e-02,  2.5014e-02,\n",
      "        -3.6271e-02,  2.8968e-02,  5.5214e-02,  2.1917e-02,  2.2263e-02,\n",
      "         1.8134e-02,  2.5802e-02,  4.3861e-03, -3.4265e-02, -3.5779e-02,\n",
      "         5.7789e-02,  5.9495e-03,  8.7367e-02,  1.6310e-02,  5.5378e-03,\n",
      "         8.7057e-02, -4.9444e-03, -2.1841e-02,  3.9620e-02, -4.0917e-02,\n",
      "        -1.1503e-01, -1.9678e-02, -6.4524e-02, -2.0902e-02,  2.3528e-02,\n",
      "        -8.4364e-02,  2.6008e-03,  4.3444e-02,  1.2590e-02,  2.0499e-02,\n",
      "        -1.9456e-04,  4.3803e-02, -3.5485e-02, -4.5111e-02, -2.8570e-02,\n",
      "        -1.0644e-01, -4.4657e-02,  6.0886e-02, -4.5304e-02,  9.6520e-02,\n",
      "         5.6700e-02, -8.6617e-02, -2.9176e-02, -8.1547e-02, -3.9127e-02,\n",
      "         3.8982e-03, -2.4301e-02,  1.5328e-02, -6.5383e-02, -2.3320e-02,\n",
      "        -8.8680e-02, -8.2918e-03, -7.6180e-03,  4.6878e-02, -8.2007e-03,\n",
      "         8.3432e-02, -6.2902e-02,  2.9497e-02, -3.2715e-02,  6.8818e-02,\n",
      "        -3.9168e-02,  2.5119e-02,  4.2596e-02, -7.4362e-03,  6.0565e-02,\n",
      "        -5.0132e-02,  4.5952e-02,  4.4013e-02,  1.1501e-02, -1.0828e-01,\n",
      "         6.5261e-02,  2.8215e-02,  4.9618e-02,  8.0455e-03,  2.5951e-02,\n",
      "        -7.1238e-02, -1.9142e-02, -3.1914e-02, -7.6380e-02,  1.2985e-01,\n",
      "         4.8923e-02,  4.6653e-02, -7.9900e-03,  6.7150e-02,  1.4517e-02,\n",
      "        -4.0464e-02,  7.9021e-03,  7.5277e-02,  4.7823e-03,  6.8594e-02,\n",
      "        -2.3632e-03,  1.4283e-02, -5.2457e-02,  2.6902e-02,  5.2304e-02,\n",
      "        -3.3668e-02,  3.7738e-03, -6.3313e-02, -1.4994e-02,  8.9074e-02,\n",
      "        -2.9943e-02,  5.6377e-02, -2.0250e-02,  3.2309e-03, -2.5706e-02,\n",
      "        -3.1076e-02,  1.9803e-03, -2.6636e-02,  5.1977e-02, -5.8395e-02,\n",
      "         3.8148e-02, -1.7109e-02, -2.5769e-03, -2.3247e-02,  6.0261e-02,\n",
      "         3.9023e-02, -5.7162e-03, -2.4845e-03,  1.1304e-01, -2.2917e-02,\n",
      "         4.2390e-03, -6.4623e-02, -2.7465e-02, -5.0172e-02, -6.5712e-04,\n",
      "        -1.3369e-02, -5.0370e-03,  5.1437e-02,  6.1007e-02, -9.2842e-02,\n",
      "        -5.0897e-02,  1.0124e-02, -5.5004e-02, -5.6310e-02,  7.6768e-02,\n",
      "         1.6043e-01, -8.8513e-02, -1.1380e-01, -1.5639e-02, -1.8646e-02,\n",
      "        -6.3616e-02, -1.5313e-02, -6.8742e-02, -1.1669e-02,  4.8287e-02,\n",
      "        -5.6313e-02,  4.4197e-02, -3.1014e-02,  5.4517e-02,  5.6171e-02,\n",
      "         5.5206e-03, -3.2178e-02, -1.9921e-03,  2.6478e-02, -5.0876e-02,\n",
      "        -2.2576e-02, -2.4724e-02,  8.2182e-03, -7.0358e-03,  4.5301e-02,\n",
      "        -2.4239e-02,  4.2009e-02, -1.0946e-01, -1.8060e-02, -1.4094e-02,\n",
      "        -4.0234e-02,  6.5017e-02,  7.7618e-02, -2.4599e-02, -1.2894e-03,\n",
      "         3.7688e-02,  1.2544e-01, -1.1111e-01, -4.2821e-02, -6.9333e-03,\n",
      "        -1.0887e-02, -1.3291e-02,  1.5008e-02,  4.1105e-02,  2.1445e-02,\n",
      "        -3.4209e-02, -6.0466e-02, -4.3890e-02,  2.2020e-02,  2.1886e-02,\n",
      "        -1.7112e-02,  4.4279e-02,  1.1194e-02, -2.1610e-02, -3.5030e-02,\n",
      "         7.8323e-02,  1.3156e-02,  1.9264e-02, -3.2224e-02,  9.3562e-02,\n",
      "         5.0559e-03, -9.5261e-02,  3.2795e-02, -9.3674e-02,  2.4408e-02,\n",
      "         1.9289e-02, -1.6423e-02,  2.8581e-02, -2.6920e-02,  5.4639e-02,\n",
      "        -3.1313e-02,  4.6124e-02,  2.3299e-02,  2.5441e-02,  2.5310e-03,\n",
      "         5.7878e-03,  5.4014e-02,  6.7774e-03,  2.1668e-02, -5.2697e-03,\n",
      "         1.5426e-03,  1.1278e-01, -3.9804e-02,  4.1868e-02,  8.5093e-02,\n",
      "        -1.7547e-03,  3.8689e-02,  3.0452e-02, -2.7951e-02,  4.7022e-02,\n",
      "         2.9130e-02, -9.2860e-03,  1.6620e-02, -6.3398e-02,  1.1991e-02,\n",
      "         6.3133e-02, -7.4316e-02,  3.5562e-02,  6.2814e-02, -9.6287e-02,\n",
      "         7.4738e-02, -1.7385e-02,  2.5969e-02,  1.7024e-02,  2.9708e-02,\n",
      "         4.3990e-02,  1.5937e-02,  3.9828e-02,  6.3124e-02,  2.1543e-02,\n",
      "        -8.2545e-02, -6.9278e-03,  8.3090e-03, -2.7721e-02,  6.7461e-02,\n",
      "         2.0871e-02, -6.3358e-02, -3.3588e-02, -4.4534e-02, -4.6774e-02,\n",
      "         8.9171e-03, -2.7352e-02, -2.7902e-02,  1.6860e-03, -2.5565e-03,\n",
      "         2.3918e-02, -8.1423e-02,  4.4813e-03,  1.4282e-02, -5.5140e-02,\n",
      "         1.6900e-02, -1.6337e-02,  5.7733e-03, -1.1331e-02, -3.2100e-02,\n",
      "        -6.1226e-03, -3.9100e-02, -2.6703e-02, -7.7068e-02, -4.0642e-03,\n",
      "         1.7949e-02, -1.4960e-02,  2.5093e-03, -2.2166e-02, -3.9730e-02,\n",
      "         6.0453e-02,  1.1021e-02,  1.1545e-01,  2.2547e-02,  6.0087e-02,\n",
      "         3.5768e-02, -4.0481e-02,  1.1999e-01, -8.9688e-03, -4.1332e-02,\n",
      "         1.4041e-02,  2.7068e-02,  1.7108e-02,  9.3065e-02,  4.5610e-02,\n",
      "         6.8860e-02,  2.5245e-02, -4.9841e-02, -1.5508e-02,  1.6391e-02],\n",
      "       grad_fn=<ViewBackward>)]\n",
      "target tensor([ 1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.])\n",
      "correlated model(v) (tensor([[[-0.0502, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502,\n",
      "          -0.0502, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[-0.0321]]], grad_fn=<TanhBackward>))\n",
      "uncorrelated model(v) (tensor([[[ 0.1607,  0.1607, -0.0502,  0.1607,  0.1607, -0.0502, -0.0502,\n",
      "           0.1607, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[0.0084]]], grad_fn=<TanhBackward>))\n",
      "negative (tensor([[[-0.0502, -0.0003, -0.0502, -0.0502, -0.0502, -0.0502, -0.0502,\n",
      "          -0.0502, -0.0502]]], grad_fn=<ViewBackward>), tensor([[[-0.0323]]], grad_fn=<TanhBackward>))\n",
      "positive (tensor([[[0.1607, 0.1116, 0.1607, 0.1607, 0.1607, 0.1607, 0.1607, 0.1607,\n",
      "          0.1607]]], grad_fn=<ViewBackward>), tensor([[[0.0280]]], grad_fn=<TanhBackward>))\n",
      "conv2d.weight tensor([[[[ 0.1171, -0.0751,  0.0273],\n",
      "          [ 0.0137,  0.0241,  0.0396],\n",
      "          [ 0.0803,  0.0250, -0.1459]]]])\n",
      "conv2d.bias tensor([0.0559])\n",
      "linear1.weight tensor([[ 0.1147, -0.0029, -0.1231, -0.0679,  0.2118,  0.1888, -0.0083, -0.0635,\n",
      "          0.0349]])\n",
      "linear1.bias tensor([-0.0179])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    # adjust learning rate\n",
    "    if adjust_learning_rate == True:\n",
    "        supervised_convnet.adjust_learning_rate(optimizer, epoch, lr)\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.unsqueeze(1).type('torch.FloatTensor')\n",
    "        target = target.type('torch.FloatTensor')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[-1].view(-1)\n",
    "        loss = criterion(output, target) \n",
    "        # add regularization\n",
    "        for param in model.parameters():\n",
    "            loss += ((param)**2).sum()/300\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running training loss\n",
    "        accuracy += (torch.sign(output) == target).sum().item() / batch_size\n",
    "        train_loss += loss.item() * batch_size\n",
    "    \n",
    "    \n",
    "    # print avg training statistics \n",
    "    # train_loss = train_loss/len(train_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        validate_accuracy = 0\n",
    "        for batch_idx, (data, target) in enumerate(validate_loader):\n",
    "            data = data.unsqueeze(1).type('torch.FloatTensor')\n",
    "            target = target.type('torch.FloatTensor')\n",
    "            output = model(data)[-1].view(-1)\n",
    "            validate_accuracy += (torch.sign(output) == target).sum().item() / batch_size\n",
    "\n",
    "        print('Epoch: {} \\t Accuracy: {} \\t Validate_Accuracy: {}'.format(\n",
    "            epoch, \n",
    "            accuracy/len(train_loader),\n",
    "            validate_accuracy/len(validate_loader),\n",
    "            ))\n",
    "#         print(\"data\", data[:10])\n",
    "        # print(\"output\", (output)[:10])\n",
    "        # print(\"target\", (target)[:10])\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print (name, param.data)\n",
    "\n",
    "patience = 0\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data = data.unsqueeze(1).type('torch.FloatTensor')#[0].unsqueeze(1)\n",
    "    # print(\"data\", data)\n",
    "    target = target.type('torch.FloatTensor')\n",
    "    optimizer.zero_grad()\n",
    "    output = [i.view(-1) for i in model(data)]\n",
    "    print(\"data\", data[:10])\n",
    "    print(\"output\", (output[:10]))\n",
    "    print(\"target\", target[:10])\n",
    "    v = torch.tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]]])\n",
    "    print(\"correlated model(v)\", model(v))\n",
    "    v = torch.tensor([[[[ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
    "        [-1., -1., -1.,  1.,  1.,  1., -1., -1., -1.]]]])\n",
    "    print(\"uncorrelated model(v)\", model(v))\n",
    "    # loss = criterion(output, target[0])\n",
    "    # print(\"loss.data\", loss.data)\n",
    "    # loss.backward()\n",
    "    patience += 1\n",
    "    if patience > 100:\n",
    "        break\n",
    "    \n",
    "\n",
    "v = torch.tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1.,  1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
    "          [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]]])\n",
    "print(\"negative\", model(v))\n",
    "print(\"positive\", model(-v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)\n",
    "\n",
    "# torch.save(model.state_dict(), \"9x9->3x3.pt\")\n",
    "    # optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Renormalization",
   "language": "python",
   "name": "renormalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
